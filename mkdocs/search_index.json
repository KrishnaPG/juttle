{
    "docs": [
        {
            "location": "/", 
            "text": "Juttle Documentation\n\n\nWelcome to the documentation for Juttle, a dataflow programming language!\n\n\nAbout Juttle\n\n\nJuttle is an analytics system and language for developers built upon\na stream-processing core and targeted for \npresentation-layer\n scale.\nJuttle gives you an agile way to query,\nanalyze, and visualize live and historical data from many different\nbig data backends or other web services.\nUsing the Juttle \ndataflow\n language,\nyou can specify your presentation analytics in a single place with a\nsyntax modeled after the classic unix shell pipeline.  There is no need\nto program against data query and visualization libraries.  Juttle scripts,\nor \njuttles\n, tie everything together and abstract away the details.\n\n\nWhile the Juttle syntax embraces the simplicity of the\nunix pipeline design pattern,\nit also includes a number of more powerful language concepts including\nfunctions, dataflow subgraph notation, native expressions, modules, scoping,\nand special aggregation functions called \nreducers\n.  The details of\nthe language and the dataflow model are desribed in the\n\nJuttle Language Reference\n.\n\n\nIn Juttle, you read data from a backend service, analyze it using\n\ndataflow processors\n, and send derived data or synthesized events to\nsome output, e.g., streaming results to a browser view,\nwriting data to a storage backend, posting http events to\nslack, hipchat, pagerduty, etc.\n\n\nJuttle presently includes a number of adapters for various big-data backends\nand we are continually adding more.\nThis means that you can interoperate with\nyour existing infrastructure, whether it is a cassandra cluster, elastic\nsearch, a SQL database, something from the hadoop ecosystem, and so forth.\nIf a particular backend is not yet supported, an adapter for it can be\nadded in a relatively straightforward manner using\nJuttle's backend adapter API.\n\n\nUnder the hood, the Juttle compiler generates javascript\noutput that implements the Juttle dataflow computation by\nexecuting alongside the Juttle runtime, either in node or the\nbrowser.  The Juttle optimizer figures out the pattern of queries\nneeded to run on the various big-data backends to perform\nthe analytics specified in your Juttle programs.  This greatly\nsimplifies the development of presentation-layer analytics since you\ncan simply specify a high-level Juttle program and don't have to worry about\nall the details involved in querying the big-data backends.\n\n\nUse Cases\n\n\nHere are some ideas of what you can do with Juttle:\n\n\n\n\nembedding analytics-driven visualization in your application,\n\n\nproviding users of your software with highly customizable views by\n  tapping into data residing in your system's storage backends,\n\n\ncomposing custom dashboards or \"wallboards\" that run continuously for\n  your internal users (ops teams, dev teams, business users, etc),\n\n\nbuilding custom dataflow microservices around a specific\n  analytics workflow like smart alerting, anomaly detection, or\n  custom application logic,\n\n\nexploring your data interactively using Juttle's rapid prototyping model,\n\n\nintrospecting and debugging your big-data pipeline, and\n\n\nprototyping and experimenting with dataflow concepts applied to\n  your data before implementing them in detail and in production using other\n  stream-processing systems like Spark or Storm.\n\n\n\n\nStream-processing Systems\n\n\nJuttle was inspired by modern, stream-processing systems like\n\nStorm\n\nand\n\nSpark Streaming\n.\nWhile Juttle\ncan do a lot of what these other systems do, it is not intended\nas an outright replacement for them nor does it have their\nproperties of parallelizability, durability, or checkpointing.\nRather, Juttle's strengths are centered around interactive visualizations,\ni.e., performing analytics at a scale\nwhere data is interactively viewed and complements systems like Spark where\nthe output of Spark can feed the input of Juttle (the Juttle Spark adapter\nis currently under development).\nAn interesting future extension for Juttle would be to adapt\nthe Juttle compiler and runtime to generate scala in addition to\njavascript as a target output langauge and leverage the Spark infrastructure\nfor Juttle programs.\n\n\nHow To Navigate\n\n\nUse the left sidebar menu for navigating the sections. Getting started with the \nJuttle Overview\n should orient you reasonably well; the rest of the Concepts section has background information on the language design.\n\n\nThe search feature (powered by lunr.js in mkdocs) can be helpful too, see the search box in upper left corner.\n\n\nJuttle examples are provided as code snippets; copy them to run in your own environment.\n\n\n Embedded Juttle examples are coming soon.\n\n\nMaintaining Docs\n\n\n These docs are just coming together.\n\n\nIf you find a problem in the documentation, members of juttle GitHub project can edit the articles inline by following \"Edit on GitHub\" link. Small obvious fixes may be committed directly to master. If discussion is needed, please put up a PR on a branch.\n\n\n This documentation was produced with \nmkdocs\n, see details in \nREADME\n.", 
            "title": "About"
        }, 
        {
            "location": "/#juttle-documentation", 
            "text": "Welcome to the documentation for Juttle, a dataflow programming language!", 
            "title": "Juttle Documentation"
        }, 
        {
            "location": "/#about-juttle", 
            "text": "Juttle is an analytics system and language for developers built upon\na stream-processing core and targeted for  presentation-layer  scale.\nJuttle gives you an agile way to query,\nanalyze, and visualize live and historical data from many different\nbig data backends or other web services.\nUsing the Juttle  dataflow  language,\nyou can specify your presentation analytics in a single place with a\nsyntax modeled after the classic unix shell pipeline.  There is no need\nto program against data query and visualization libraries.  Juttle scripts,\nor  juttles , tie everything together and abstract away the details.  While the Juttle syntax embraces the simplicity of the\nunix pipeline design pattern,\nit also includes a number of more powerful language concepts including\nfunctions, dataflow subgraph notation, native expressions, modules, scoping,\nand special aggregation functions called  reducers .  The details of\nthe language and the dataflow model are desribed in the Juttle Language Reference .  In Juttle, you read data from a backend service, analyze it using dataflow processors , and send derived data or synthesized events to\nsome output, e.g., streaming results to a browser view,\nwriting data to a storage backend, posting http events to\nslack, hipchat, pagerduty, etc.  Juttle presently includes a number of adapters for various big-data backends\nand we are continually adding more.\nThis means that you can interoperate with\nyour existing infrastructure, whether it is a cassandra cluster, elastic\nsearch, a SQL database, something from the hadoop ecosystem, and so forth.\nIf a particular backend is not yet supported, an adapter for it can be\nadded in a relatively straightforward manner using\nJuttle's backend adapter API.  Under the hood, the Juttle compiler generates javascript\noutput that implements the Juttle dataflow computation by\nexecuting alongside the Juttle runtime, either in node or the\nbrowser.  The Juttle optimizer figures out the pattern of queries\nneeded to run on the various big-data backends to perform\nthe analytics specified in your Juttle programs.  This greatly\nsimplifies the development of presentation-layer analytics since you\ncan simply specify a high-level Juttle program and don't have to worry about\nall the details involved in querying the big-data backends.", 
            "title": "About Juttle"
        }, 
        {
            "location": "/#use-cases", 
            "text": "Here are some ideas of what you can do with Juttle:   embedding analytics-driven visualization in your application,  providing users of your software with highly customizable views by\n  tapping into data residing in your system's storage backends,  composing custom dashboards or \"wallboards\" that run continuously for\n  your internal users (ops teams, dev teams, business users, etc),  building custom dataflow microservices around a specific\n  analytics workflow like smart alerting, anomaly detection, or\n  custom application logic,  exploring your data interactively using Juttle's rapid prototyping model,  introspecting and debugging your big-data pipeline, and  prototyping and experimenting with dataflow concepts applied to\n  your data before implementing them in detail and in production using other\n  stream-processing systems like Spark or Storm.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/#stream-processing-systems", 
            "text": "Juttle was inspired by modern, stream-processing systems like Storm \nand Spark Streaming .\nWhile Juttle\ncan do a lot of what these other systems do, it is not intended\nas an outright replacement for them nor does it have their\nproperties of parallelizability, durability, or checkpointing.\nRather, Juttle's strengths are centered around interactive visualizations,\ni.e., performing analytics at a scale\nwhere data is interactively viewed and complements systems like Spark where\nthe output of Spark can feed the input of Juttle (the Juttle Spark adapter\nis currently under development).\nAn interesting future extension for Juttle would be to adapt\nthe Juttle compiler and runtime to generate scala in addition to\njavascript as a target output langauge and leverage the Spark infrastructure\nfor Juttle programs.", 
            "title": "Stream-processing Systems"
        }, 
        {
            "location": "/#how-to-navigate", 
            "text": "Use the left sidebar menu for navigating the sections. Getting started with the  Juttle Overview  should orient you reasonably well; the rest of the Concepts section has background information on the language design.  The search feature (powered by lunr.js in mkdocs) can be helpful too, see the search box in upper left corner.  Juttle examples are provided as code snippets; copy them to run in your own environment.   Embedded Juttle examples are coming soon.", 
            "title": "How To Navigate"
        }, 
        {
            "location": "/#maintaining-docs", 
            "text": "These docs are just coming together.  If you find a problem in the documentation, members of juttle GitHub project can edit the articles inline by following \"Edit on GitHub\" link. Small obvious fixes may be committed directly to master. If discussion is needed, please put up a PR on a branch.   This documentation was produced with  mkdocs , see details in  README .", 
            "title": "Maintaining Docs"
        }, 
        {
            "location": "/README/", 
            "text": "Juttle Documentation README\n\n\nThis explains how the docs are produced. \n\n\nThe documentation articles are written in GitHub flavored markdown. \n\n\nWe use \nmkdocs\n for static site generation, that is, to produce html files out of markdown articles. \n\n\nThe theme is ReadTheDocs theme supplied by mkdocs package, with a small \ncustom theme override\n for the left sidebar, to have larger fonts, and to hide some articles from the sidebar menu. If further theme changes are desired, make them under \ncustom_theme\n, our \nmkdocs.yml\n uses it.\n\n\n Support for GFM emoji is handled in a hackish way with \nunimoji\n Python markdown extension and custom images, see \nmkdocs.yml\n. \n\n\nThe left sidebar layout, along with other configuration, is encoded in \nmkdocs.yml\n. Hidden articles have names starting with underscore, such as \n'____ Not For Left Sidebar Menu'\n. This allows us to have a reasonable sized left menu, as collapsible menu sections are not yet supported by mkdocs.\n\n\n We intend to publish the docs to \nRead The Docs\n site, using their \nmkdocs integration\n and a \nGitHub webhook\n to auto-rebuild docs when updates are pushed. \n\n\n While the juttle repo is private, instead of Read The Docs we are publishing to \nGitHub Pages\n. There is no auto-updating right now, changes are published via a manual process with\n\n\nmkdocs gh-deploy -c\n\n\n\n\nTo edit the docs, follow \"Edit on GitHub\" links from the published site, or clone the juttle repo and edit the markdown files. Feel free to edit inline on master for small changes, put up a PR on a branch for larger changes that need discussion.\n\n\nIf you add a new docs article, be sure to add it to \nmkdocs.yml\n to not break the build. Every referenced article must be listed under \npages:\n ('____ Hidden' if you wish). Refer to the \ndocs template\n for our doc standard on using emoji, section headers etc.\n\n\nTo test out the site locally, run:\n\n\nmkdocs serve\n\n\n\n\nThen access the docs site at \n127.0.0.1:8000\n. \n\n\nYou need to have mkdocs, markdown-include and unimoji installed. We are currently on a fork with \nPR pending\n, in order to run with our fix to have proper \"Edit on GitHub links\", install mkdocs like this:\n\n\npip install markdown-include\npip install --upgrade git+git://github.com/kernc/mdx_unimoji.git\ngit clone https://github.com/dmehra/mkdocs.git\ncd mkdocs\npython setup.py install\n\n\n\n\n The client-side search feature in mkdocs (powered by lunr.js) appears to be broken on master, so it will not work when you do \nmkdocs serve\n from our fork. Waiting to hear on this \nissue\n.", 
            "title": "____ Docs README"
        }, 
        {
            "location": "/README/#juttle-documentation-readme", 
            "text": "This explains how the docs are produced.   The documentation articles are written in GitHub flavored markdown.   We use  mkdocs  for static site generation, that is, to produce html files out of markdown articles.   The theme is ReadTheDocs theme supplied by mkdocs package, with a small  custom theme override  for the left sidebar, to have larger fonts, and to hide some articles from the sidebar menu. If further theme changes are desired, make them under  custom_theme , our  mkdocs.yml  uses it.   Support for GFM emoji is handled in a hackish way with  unimoji  Python markdown extension and custom images, see  mkdocs.yml .   The left sidebar layout, along with other configuration, is encoded in  mkdocs.yml . Hidden articles have names starting with underscore, such as  '____ Not For Left Sidebar Menu' . This allows us to have a reasonable sized left menu, as collapsible menu sections are not yet supported by mkdocs.   We intend to publish the docs to  Read The Docs  site, using their  mkdocs integration  and a  GitHub webhook  to auto-rebuild docs when updates are pushed.    While the juttle repo is private, instead of Read The Docs we are publishing to  GitHub Pages . There is no auto-updating right now, changes are published via a manual process with  mkdocs gh-deploy -c  To edit the docs, follow \"Edit on GitHub\" links from the published site, or clone the juttle repo and edit the markdown files. Feel free to edit inline on master for small changes, put up a PR on a branch for larger changes that need discussion.  If you add a new docs article, be sure to add it to  mkdocs.yml  to not break the build. Every referenced article must be listed under  pages:  ('____ Hidden' if you wish). Refer to the  docs template  for our doc standard on using emoji, section headers etc.  To test out the site locally, run:  mkdocs serve  Then access the docs site at  127.0.0.1:8000 .   You need to have mkdocs, markdown-include and unimoji installed. We are currently on a fork with  PR pending , in order to run with our fix to have proper \"Edit on GitHub links\", install mkdocs like this:  pip install markdown-include\npip install --upgrade git+git://github.com/kernc/mdx_unimoji.git\ngit clone https://github.com/dmehra/mkdocs.git\ncd mkdocs\npython setup.py install   The client-side search feature in mkdocs (powered by lunr.js) appears to be broken on master, so it will not work when you do  mkdocs serve  from our fork. Waiting to hear on this  issue .", 
            "title": "Juttle Documentation README"
        }, 
        {
            "location": "/doc_templates/template/", 
            "text": "thisDocArticleTitle\n\n\nPlain text description goes here. Code \n-parameters\n should be inside backticks. End the line with two spaces to make it into end of paragraph.  \n\n\nSee \njuttle template\n for a template specifically for juttle reference articles. This template is more extensive.\n\n\nAbout title header\n\n\nThe title header table at the top of this document displays nicely in GitHub as a little box; their own documentation uses that style.\n\n\nHeading 3\n\n\nHeading 4\n\n\nHeading 5\n\n\nHeading 6\n\n\nMarkdown syntax\n\n\nIf you need a list, it's like this:\n  * 1\nst\n item\n  * 2\nnd\n item\n  - Use html tags for little things like superscript/subscript, there's no native markdown for that.\n  - both \n*\n and \n-\n work as bullet points.\n\n\n Note: if you need to make a note, do it with \n:information_source:\n.\n\n\n Tip: useful tips are marked with the \n:bulb:\n.\n\n\n Experimental: this is how we mark up experimental sections. We used to have a beaker symbol in the old docs, the GFM emoji list has nothing like that, but there's that crawling \n:baby_symbol:\n.\n\n\n In the works: this feature is in development or being revised, mark it with \n:information_source:\n.\n\n\nTo use another emoji from the \nlist of GFM supported emojis\n, add it to \nmkdocs.yml\n and place a PNG image into \ndocs/images/emoji\n dir.\n\n\n[link title](http://link/address)\n is how you make a URL in markdown.\n\n\nLinking to sections inside markdown docs is easy if your sections have headings; for example, I'm linking to \n\"About title header\" section\n. Easy. To link to a non-title section, you could put html anchor in there, but why bother.\n\n\nSyntax of a juttle proc/etc is presented below in a code block, with no section header, just right there. Long form first, then short form after \nor\n .  \n\n\nfunction makeFencedCodeBlock() {\n  // here is a code sample in a fenced code block\n  // don't use four-space-indenting approach, we prefer fencing with triple backticks\n  // this uses javascript syntax highlighting\n}\n\n\n\n\nor\n\n\n// and this doesn't have any syntax highlighting.\n\n\n\n\nThen we have a table with each parameter explained.  \n\n\nTables\n\n\nNotice that you're not allowed to put blank lines between table rows, so these tables tend to run dense. The only other option would be to do HTML tables, and we are trying to avoid that, for reasons of rendering difficulties with different markdown engines, and also because GFM tables are a bit more friendly. But dense.  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-optionOne\n\n\nWhat this option does for you\n\n\nNo; defaults to \nbananas\n\n\n\n\n\n\n-optionTwo\n\n\nHas suboptions, use unordered list: \naaa\nbbb\nAnd so much text we had to put it into two paragraphs. \nYou have to use HTML tags for lists and paragraphs inside table cells. \n\n\nYes\n\n\n\n\n\n\n-optionThree\n\n\nHas suboptions with details, use definition list: \none\nThe first suboption\ntwo\nThe second suboption\n\n\nNo\n\n\n\n\n\n\n\n\nExample: Title of usage example that follows, italicize this line\n  \n\n\nrun my code!\n\n\n\n\nExample: This example has inputs and outputs\n\n\nGiven this data point:\n\n\n\n\n{here: is, my: data, point: 0}  \n\n\n\n\nThis code:\n\n\nrun_my_code()\n\n\n\n\nYields the following output:\n\n\n\n\n{new: data, point: one}\n\n{new: data, point: two}  \n\n\n\n\nMind the two trailing spaces so the quoted lines don't get merged together. This style of quoting the inputs/outputs is better displayed in Github than making everything a code block. Let's keep code blocks for actual code.", 
            "title": "____ Doc Template"
        }, 
        {
            "location": "/doc_templates/template/#thisdocarticletitle", 
            "text": "Plain text description goes here. Code  -parameters  should be inside backticks. End the line with two spaces to make it into end of paragraph.    See  juttle template  for a template specifically for juttle reference articles. This template is more extensive.", 
            "title": "thisDocArticleTitle"
        }, 
        {
            "location": "/doc_templates/template/#about-title-header", 
            "text": "The title header table at the top of this document displays nicely in GitHub as a little box; their own documentation uses that style.", 
            "title": "About title header"
        }, 
        {
            "location": "/doc_templates/template/#heading-3", 
            "text": "", 
            "title": "Heading 3"
        }, 
        {
            "location": "/doc_templates/template/#heading-4", 
            "text": "", 
            "title": "Heading 4"
        }, 
        {
            "location": "/doc_templates/template/#heading-5", 
            "text": "", 
            "title": "Heading 5"
        }, 
        {
            "location": "/doc_templates/template/#heading-6", 
            "text": "", 
            "title": "Heading 6"
        }, 
        {
            "location": "/doc_templates/template/#markdown-syntax", 
            "text": "If you need a list, it's like this:\n  * 1 st  item\n  * 2 nd  item\n  - Use html tags for little things like superscript/subscript, there's no native markdown for that.\n  - both  *  and  -  work as bullet points.   Note: if you need to make a note, do it with  :information_source: .   Tip: useful tips are marked with the  :bulb: .   Experimental: this is how we mark up experimental sections. We used to have a beaker symbol in the old docs, the GFM emoji list has nothing like that, but there's that crawling  :baby_symbol: .   In the works: this feature is in development or being revised, mark it with  :information_source: .  To use another emoji from the  list of GFM supported emojis , add it to  mkdocs.yml  and place a PNG image into  docs/images/emoji  dir.  [link title](http://link/address)  is how you make a URL in markdown.  Linking to sections inside markdown docs is easy if your sections have headings; for example, I'm linking to  \"About title header\" section . Easy. To link to a non-title section, you could put html anchor in there, but why bother.  Syntax of a juttle proc/etc is presented below in a code block, with no section header, just right there. Long form first, then short form after  or  .    function makeFencedCodeBlock() {\n  // here is a code sample in a fenced code block\n  // don't use four-space-indenting approach, we prefer fencing with triple backticks\n  // this uses javascript syntax highlighting\n}  or  // and this doesn't have any syntax highlighting.  Then we have a table with each parameter explained.", 
            "title": "Markdown syntax"
        }, 
        {
            "location": "/doc_templates/template/#tables", 
            "text": "Notice that you're not allowed to put blank lines between table rows, so these tables tend to run dense. The only other option would be to do HTML tables, and we are trying to avoid that, for reasons of rendering difficulties with different markdown engines, and also because GFM tables are a bit more friendly. But dense.       Parameter  Description  Required?      -optionOne  What this option does for you  No; defaults to  bananas    -optionTwo  Has suboptions, use unordered list:  aaa bbb And so much text we had to put it into two paragraphs.  You have to use HTML tags for lists and paragraphs inside table cells.   Yes    -optionThree  Has suboptions with details, use definition list:  one The first suboption two The second suboption  No     Example: Title of usage example that follows, italicize this line     run my code!  Example: This example has inputs and outputs  Given this data point:   {here: is, my: data, point: 0}     This code:  run_my_code()  Yields the following output:   {new: data, point: one} \n{new: data, point: two}     Mind the two trailing spaces so the quoted lines don't get merged together. This style of quoting the inputs/outputs is better displayed in Github than making everything a code block. Let's keep code blocks for actual code.", 
            "title": "Tables"
        }, 
        {
            "location": "/doc_templates/juttle_TEMPLATE/", 
            "text": "juttle_element_name_goes_here\n\n\nDescribe this Juttle element in one or two sentences, using the\nimperative form (\"Return the value of the specified variable\" or\n\"Truncate a string to the specified number of characters\", for example).\n\n\nusage summary goes here\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-parameter1\n\n\nDescription of parameter1\n\n\nYes/No\n\n\n\n\n\n\n\n\nExample\n \n\n\nExplain what the example below does.\n\n\n// code snippet", 
            "title": "____ Juttle Doc Template"
        }, 
        {
            "location": "/doc_templates/juttle_TEMPLATE/#juttle_element_name_goes_here", 
            "text": "Describe this Juttle element in one or two sentences, using the\nimperative form (\"Return the value of the specified variable\" or\n\"Truncate a string to the specified number of characters\", for example).  usage summary goes here     Parameter  Description  Required?      -parameter1  Description of parameter1  Yes/No     Example    Explain what the example below does.  // code snippet", 
            "title": "juttle_element_name_goes_here"
        }, 
        {
            "location": "/examples/datasets/README/", 
            "text": "Sample Datasets\n\n\nA lot of juttle examples in this documentation use \nemit\n as the data source.\n\n\nExamples that call for more complex data sets rely on \nread file\n source with \n-file 'filename.json\n option, that allows to read data points from a local file in JSON array format, like this:\n\n\nread file -file 'docs/examples/datasets/input1.json'\n| reduce count() by hostname\n| view table\n\n\n\n\nThe dataset files are presented below. \n\n\nDataset 1\n\n\nTiny fake data set:\n\n\n[ \n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n },\n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nwarn\n },\n{ \ntime\n: \n2015-11-06T04:28:42.405Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n },\n{ \ntime\n: \n2015-11-06T04:28:42.502Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nok\n }\n]\n\n\n\n\nDataset 2\n\n\nLog messages, generated from the synthetic data source\n\n\nstochastic -source 'logs'\n\n\n\n\nSee file \nstochastic_logs.json\n with points like this:\n\n\n  {\n    \ntime\n: \n2015-11-23T01:25:16.000Z\n,\n    \nhost\n: \nsea.0\n,\n    \npop\n: \nsea\n,\n    \nsource_type\n: \nevent\n,\n    \nname\n: \nsyslog\n,\n    \nmessage\n: \n[ 62875 ] cfg80211: World regulatory domain updated:\n\n  }\n\n\n\n\nDataset 3\n\n\nHost metrics, generated from the synthetic data source\n\n\nstochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'\n\n\n\n\nSee file \nstochastic_cdn_nhosts3_dos05_pts100.json\n with points like this:\n\n\n  {\n    \nhost\n: \nsea.0\n,\n    \npop\n: \nsea\n,\n    \nservice\n: \nsearch\n,\n    \nsource_type\n: \nmetric\n,\n    \nname\n: \nrequests\n,\n    \ntime\n: \n2015-11-23T01:47:10.000Z\n,\n    \nvalue\n: 28.096945568298626\n  }", 
            "title": "____ Sample Datasets"
        }, 
        {
            "location": "/examples/datasets/README/#sample-datasets", 
            "text": "A lot of juttle examples in this documentation use  emit  as the data source.  Examples that call for more complex data sets rely on  read file  source with  -file 'filename.json  option, that allows to read data points from a local file in JSON array format, like this:  read file -file 'docs/examples/datasets/input1.json'\n| reduce count() by hostname\n| view table  The dataset files are presented below.", 
            "title": "Sample Datasets"
        }, 
        {
            "location": "/examples/datasets/README/#dataset-1", 
            "text": "Tiny fake data set:  [ \n{  time :  2015-11-06T04:28:32.304Z ,  hostname :  lemoncake ,  state :  ok  },\n{  time :  2015-11-06T04:28:32.304Z ,  hostname :  applepie ,  state :  warn  },\n{  time :  2015-11-06T04:28:42.405Z ,  hostname :  lemoncake ,  state :  ok  },\n{  time :  2015-11-06T04:28:42.502Z ,  hostname :  applepie ,  state :  ok  }\n]", 
            "title": "Dataset 1"
        }, 
        {
            "location": "/examples/datasets/README/#dataset-2", 
            "text": "Log messages, generated from the synthetic data source  stochastic -source 'logs'  See file  stochastic_logs.json  with points like this:    {\n     time :  2015-11-23T01:25:16.000Z ,\n     host :  sea.0 ,\n     pop :  sea ,\n     source_type :  event ,\n     name :  syslog ,\n     message :  [ 62875 ] cfg80211: World regulatory domain updated: \n  }", 
            "title": "Dataset 2"
        }, 
        {
            "location": "/examples/datasets/README/#dataset-3", 
            "text": "Host metrics, generated from the synthetic data source  stochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'  See file  stochastic_cdn_nhosts3_dos05_pts100.json  with points like this:    {\n     host :  sea.0 ,\n     pop :  sea ,\n     service :  search ,\n     source_type :  metric ,\n     name :  requests ,\n     time :  2015-11-23T01:47:10.000Z ,\n     value : 28.096945568298626\n  }", 
            "title": "Dataset 3"
        }, 
        {
            "location": "/concepts/overview/", 
            "text": "Juttle Overview\n\n\nThis section walks through a few simple Juttle examples to demonstrate a bit of the language and how it works.\n\n\nTo begin with, here is the canonical hello world example in juttle:\n\n\nemit | put message = \nHello World!\n | view table\n\n\n\n\nWhich outputs:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 message          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-12-12T00:24:46.222Z           \u2502 Hello World!     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThis example is the simplest possible Juttle flowgraph with the form:\n\n\nsource | processor | sink\n\n\n\n\nIn Juttle, \nthe basic unit of data is a point\n. A point consists of a number of key/value pairs, where the keys are strings and the values are numbers, strings, times, booleans, etc. Points flow through flowgraphs and can be transformed, aggregated, or joined at each processing step.\n\n\nIn the simple example above, the following steps occurred:\n\n\n\n\n\n\nThe source \nemit\n, generates a single synthetic point with a timestamp of the current time.\n\n\n\n\n\n\nThe point is fed into the processor \nput\n, which adds a field called message.\n\n\n\n\n\n\nThe point is then sent to a \ntable\n \nview\n which renders the result as the given table. Note that the specific views are actually not part of the Juttle language -- they are passed through to the calling environment, either the Juttle CLI (shown above) or an application environment like \noutrigger\n.\n\n\n\n\n\n\n\n\nLet's make this example a bit more interesting:\n\n\nemit -from :2015-01-01: -to :2015-02-01: -every :1 day:\n| reduce days = count()\n| view table -title 'Days in January'\n\n\n\n\nResults in:\n\n\nDays in January\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 days     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 31       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nHere we add a few more concepts. First, the \nemit\n source is configured with time options that will generate a data point for each day of January 2015. Each point is sent to the \nreduce\n processor using the \ncount\n reducer that produces an aggregate sum of the number of points, and then emits a single value that shows the number of days in the month. Finally the \ntable\n view is parameterized to include a title before displaying the results.\n\n\n\n\nJuttle also supports basic programming language constructs like constants and functions:\n\n\nconst days = [ 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat' ];\n\nfunction getDay(i) {\n    const offset = 3; // January 1, 2015 was a Thursday\n    return days[(i + offset) % Array.length(days)];\n}\n\nemit -from :2015-01-01: -to :2015-02-01: -every :1 day:\n| put i = count()\n| put day = getDay(i)\n| reduce count() by day\n\n\n\n\nResults in:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count    \u2502 day      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Thu      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Fri      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Sat      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Sun      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Mon      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Tue      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Wed      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn this case we've defined a constant array listing the days of the week, and a function called \ngetDay\n that returns the weekday corresponding to the the given day of the month. Then we use the \nday\n field as a grouping field for reduce, and thereby count the number of occurrences of the given day of the week for the month of January. The result is implicitly put into a field named \ncount\n, matching the name of the reducer that we used, and even though there is no explicit sink in the program, the runtime added an implicit \nview table\n to show the results in a table.\n\n\n\n\nFinally, Juttle's dataflow model allows for more complicated flowgraphs than simple pipelines, and various operations can divide time into intervals and operate on batches of points within that interval instead of treating the full stream in its entirety:\n\n\nconst fruits = [ 'apple', 'orange', 'banana' ];\n\nemit -from :2015-01-01: -to :2015-02-02: -every :1d:\n| put fruit = fruits[Math.floor(Math.random() * Array.length(fruits))]\n| (\n    reduce total = count() by fruit\n    | view table -title 'Fruit popularity';\n\n    batch :7 days:\n    | reduce count() by fruit\n    | sort count -desc\n    | head 1\n    | put week = (time - :2015-01-01:) / :7d:\n    | keep week, fruit\n    | view table -title 'Most popular fruit of the week';\n\n  )\n\n\n\n\nResults in something like the following:\n\n\nMost popular fruit of the week\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruit    \u2502 week     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 3        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 5        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nFruit popularity\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruit    \u2502 total    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 apple    \u2502 8        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 13       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 11       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThis example pulls together several additional concepts of Juttle.\n\n\nFirst, the source emits a point for every day of the week and adds a field with a random name of a fruit. Then the flowgraph is forked using the \na | (b ; c)\n syntax, which sends all points coming out of \na\n to both \nb\n and \nc\n.\n\n\nThe first branch performs a simple count of the number of times each fruit was picked and sorts the output before sending to a table view.\n\n\nThe second branch creates a time window of 7 days, and for each batch, counts the number of occurrences of each fruit within the given batch, uses \nsort\n to rank by the count, uses \nhead\n to pick first point, uses a \nput\n statement to add the week number within the month, \nkeep\n to remove all fields but the week number and the fruit, and finally outputs a table which prints which was most popular fruit for the given week.\n\n\nDig Deeper\n\n\nSee the in-depth \nTutorial\n to learn more about the juttle language and explore a richer data set.\n\n\nYou can also learn more about the conceptual underpinnings of the \nJuttle dataflow language\n including how to work with time and batching, and how to string together, merge, split, and join flowgraphs for data processing.\n\n\nThe language supports various \nprogramming constructs\n such as variables, constants, functions, subgraphs, and modules to compose flowgraphs with less repetition and more clarity.\n\n\nJuttle can interact with storage systems and other external data sources using \nadapters\n, some of which are both built into the distribution while others can be installed as external plugins.\n\n\nFinally, the language contains the declarative framework for specifying client-side \nviews\n to control data visualization. The Juttle CLI includes simple terminal-based outputs for viewing data as a table or in raw encoding formats, but Juttle can also be used in conjunction with a visualization library like \njuttle-viz\n for other charting.", 
            "title": "Overview"
        }, 
        {
            "location": "/concepts/overview/#juttle-overview", 
            "text": "This section walks through a few simple Juttle examples to demonstrate a bit of the language and how it works.  To begin with, here is the canonical hello world example in juttle:  emit | put message =  Hello World!  | view table  Which outputs:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 message          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-12-12T00:24:46.222Z           \u2502 Hello World!     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  This example is the simplest possible Juttle flowgraph with the form:  source | processor | sink  In Juttle,  the basic unit of data is a point . A point consists of a number of key/value pairs, where the keys are strings and the values are numbers, strings, times, booleans, etc. Points flow through flowgraphs and can be transformed, aggregated, or joined at each processing step.  In the simple example above, the following steps occurred:    The source  emit , generates a single synthetic point with a timestamp of the current time.    The point is fed into the processor  put , which adds a field called message.    The point is then sent to a  table   view  which renders the result as the given table. Note that the specific views are actually not part of the Juttle language -- they are passed through to the calling environment, either the Juttle CLI (shown above) or an application environment like  outrigger .     Let's make this example a bit more interesting:  emit -from :2015-01-01: -to :2015-02-01: -every :1 day:\n| reduce days = count()\n| view table -title 'Days in January'  Results in:  Days in January\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 days     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 31       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  Here we add a few more concepts. First, the  emit  source is configured with time options that will generate a data point for each day of January 2015. Each point is sent to the  reduce  processor using the  count  reducer that produces an aggregate sum of the number of points, and then emits a single value that shows the number of days in the month. Finally the  table  view is parameterized to include a title before displaying the results.   Juttle also supports basic programming language constructs like constants and functions:  const days = [ 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat' ];\n\nfunction getDay(i) {\n    const offset = 3; // January 1, 2015 was a Thursday\n    return days[(i + offset) % Array.length(days)];\n}\n\nemit -from :2015-01-01: -to :2015-02-01: -every :1 day:\n| put i = count()\n| put day = getDay(i)\n| reduce count() by day  Results in:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count    \u2502 day      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Thu      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Fri      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5        \u2502 Sat      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Sun      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Mon      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Tue      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4        \u2502 Wed      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In this case we've defined a constant array listing the days of the week, and a function called  getDay  that returns the weekday corresponding to the the given day of the month. Then we use the  day  field as a grouping field for reduce, and thereby count the number of occurrences of the given day of the week for the month of January. The result is implicitly put into a field named  count , matching the name of the reducer that we used, and even though there is no explicit sink in the program, the runtime added an implicit  view table  to show the results in a table.   Finally, Juttle's dataflow model allows for more complicated flowgraphs than simple pipelines, and various operations can divide time into intervals and operate on batches of points within that interval instead of treating the full stream in its entirety:  const fruits = [ 'apple', 'orange', 'banana' ];\n\nemit -from :2015-01-01: -to :2015-02-02: -every :1d:\n| put fruit = fruits[Math.floor(Math.random() * Array.length(fruits))]\n| (\n    reduce total = count() by fruit\n    | view table -title 'Fruit popularity';\n\n    batch :7 days:\n    | reduce count() by fruit\n    | sort count -desc\n    | head 1\n    | put week = (time - :2015-01-01:) / :7d:\n    | keep week, fruit\n    | view table -title 'Most popular fruit of the week';\n\n  )  Results in something like the following:  Most popular fruit of the week\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruit    \u2502 week     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 3        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 5        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nFruit popularity\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruit    \u2502 total    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 apple    \u2502 8        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 orange   \u2502 13       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 banana   \u2502 11       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  This example pulls together several additional concepts of Juttle.  First, the source emits a point for every day of the week and adds a field with a random name of a fruit. Then the flowgraph is forked using the  a | (b ; c)  syntax, which sends all points coming out of  a  to both  b  and  c .  The first branch performs a simple count of the number of times each fruit was picked and sorts the output before sending to a table view.  The second branch creates a time window of 7 days, and for each batch, counts the number of occurrences of each fruit within the given batch, uses  sort  to rank by the count, uses  head  to pick first point, uses a  put  statement to add the week number within the month,  keep  to remove all fields but the week number and the fruit, and finally outputs a table which prints which was most popular fruit for the given week.", 
            "title": "Juttle Overview"
        }, 
        {
            "location": "/concepts/overview/#dig-deeper", 
            "text": "See the in-depth  Tutorial  to learn more about the juttle language and explore a richer data set.  You can also learn more about the conceptual underpinnings of the  Juttle dataflow language  including how to work with time and batching, and how to string together, merge, split, and join flowgraphs for data processing.  The language supports various  programming constructs  such as variables, constants, functions, subgraphs, and modules to compose flowgraphs with less repetition and more clarity.  Juttle can interact with storage systems and other external data sources using  adapters , some of which are both built into the distribution while others can be installed as external plugins.  Finally, the language contains the declarative framework for specifying client-side  views  to control data visualization. The Juttle CLI includes simple terminal-based outputs for viewing data as a table or in raw encoding formats, but Juttle can also be used in conjunction with a visualization library like  juttle-viz  for other charting.", 
            "title": "Dig Deeper"
        }, 
        {
            "location": "/concepts/dataflow/", 
            "text": "Dataflow Concepts\n\n\nThis section lays out some of the conceptual framework for the Juttle dataflow language.\n\n\n\n\n\n\nDataflow Concepts\n\n\nDataflow Language\n\n\nData Streams\n\n\nTimeliness\n\n\nBatching\n\n\nGrouping\n\n\nForking and merging\n\n\nJoining streams\n\n\nMatching batches between streams\n\n\nJoining points between batches\n\n\nJoining points in a single stream\n\n\n\n\n\n\n\n\n\n\nDataflow Language\n\n\nJuttle is a declarative dataflow or stream processing language.\n\n\nEach Juttle program consists of one or more flowgraphs that generate or read in\nstreams of data, process the streams, and output results.\n\n\nIn each Juttle flowgraph, you pull data into your program with \nsource nodes\n that either \nemit\n synthetic data or \nread\n from an external data source. In the middle of your flowgraph, you can perform operations on your data using \nprocessor nodes\n. You can chain and arrange these nodes arbitrarily to enable parallel processing and forking/merging of the data stream. Then each branch of the flowgraph output can either \nview\n the data in a graph using a client side visualization library, or \nwrite\n data to a backend for storage or to take an action like sending an alert.\n\n\n\n\nThe language is declarative, in that a program specifies at a high level what data to retrieve, what to do to it, and how to visualize it, not the programming details of how to actually perform these operations. This high level specification is mapped into an execution plan by the Juttle compiler, which can delegate filtering and computation to back end data stores via \nadapters\n, and which passes the data and configuration options to client-side \nviews\n to render data visualizations.\n\n\nData Streams\n\n\nIn a Juttle, data is represented as a stream of points, where each point\nconsists of a number of key/value fields of various \ndata types\n.\n\n\nWhen they enter a Juttle flowgraph, data points are represented and\nhandled identically, regardless of whether they were synthetically generated, originated by tapping into a live data source, read from a persistent storage back end, or a combination. Data points may represent structured time series metrics or semi-structured log events.\n\n\nExample: three points in a stream of metric data\n\n\n{\nname\n : \nerrors\n, \nvalue\n : 42, \ntime\n : \n2014-05-28T22:11:35.332Z\n}\n{\nname\n : \nerrors\n, \nvalue\n : 44, \ntime\n : \n2014-05-28T22:11:35.353Z\n}\n{\nname\n : \nerrors\n, \nvalue\n : 46, \ntime\n : \n2014-05-28T22:11:35.532Z\n}\n\n\n\n\nExample: two points in a stream of Web log data\n\n\n{\ndate\n:\n2014-07-03T10:38:44-07:00\n,\nip\n:\n79.122.57.6\n,\nreferer\n:\n-\n,\nagent\n:\n-\n,\nctry\n:\nUS\n,\nuri\n:\n/iotest/reply/core/utf/krw\n,\nmethod\n:\nGET\n,\nbytes\n:18270,\nbytes_in\n:1768,\nstatus\n:200,\ntime\n:\n2014-05-28T22:11:35.332Z\n}\n{\ndate\n:\n2014-07-03T10:38:45-07:00\n,\nip\n:\n161.182.107.224\n,\nreferer\n:\n-\n,\nagent\n:\nMozilla/4.0\n(compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; SLCC2; .NET CLR\n2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2)\n,\nctry\n:\nUS\n,\nuri\n:\n/flick/token/scss/pause/api\n,\nmethod\n:\nGET\n,\nbytes\n:8219,\nbytes_in\n:682,\nstatus\n:200,\ntime\n:\n2014-05-28T22:11:35.332Z\n}\n\n\n\n\nTimeliness\n\n\nPoints streaming through Juttle flowgraphs must always be ordered by time, and\nthey must arrive in a timely manner to avoid buffering problems and delays in\nthe runtime.\n\n\n\n\n\n\nPoints flow through each processor in chronological order.\n\n\nIf a point has a time field, the value of its time field should be\ngreater than or equal to the value of the time field of the previous\npoint (that had a time field). Points for which that is not true are\ncalled \"out of order\".\n\n\n\n\n\n\n\nConsecutive points must arrive in a timely fashion with respect to their\n    time fields.\n\n\nFor example, if two consecutive points with time stamps one millisecond\napart arrive 20 seconds apart, the second one has arrived \"late\". Based on\nthe specific system resources available in the Juttle runtime, this may end\nup causing the point to not be included in the flowgraph because it arrived too late.\n\n\n\n\n\n\nPoints that do not have a time field (aka \"timeless points\") are treated as if they occurred at the dawn of time. Therefore when merging or joining a stream of timeless points with a stream of timed data points, all points from the timeless branch are treated as if they occurred \"before\" the timed points.\n\n\nBatching\n\n\nMany Juttle processors operate point-by-point, but others operate over\nbatches of consecutive points in a given time interval.\n\n\nThe meanings of\n\nsort\n,\n\nhead\n,\nand\n\ntail\n\nare probably known to anyone with exposure to the UNIX shell. But there\nis a twist when using these in Juttle: sort, head, and tail processors\ndon't operate on files like their UNIX counterparts; they operate on\nstreams of indeterminate lengths. And\n\nsort\n\nor\n\ntail\n\ncan't be applied to a stream of unknown (and potentially unbounded)\nlength, because you need to know where the stream ends in order to start\nthe sort or tail operation.\n\n\nTo use these on streams of indeterminate length, you segment the stream of\npoints into consecutive batches that can be processed one-at-a-time by these\nprocessors. Think of segmentation as inserting time division markers into the\nstream of points; these marker carry no data and serve no purpose other than to\ndelineate batch boundaries that are used by processors to determine which points\nto process together. Segmenting points in this way can be done with the\n\nbatch\n processor or by specifying the \n-every\n option to the processors themselves.\n\n\nFor streams that are of bounded length (such as pulled from long-term storage),\nan end of batch marker is always present at the end of the stream.\n\n\nThis simple example illustrates how \nbatch\n\nchanges the data in two different ways when starting from the same initial data\nstream. A diagram shows the data flow with markers pointing out the logical\nseparation of batches of points.\n\n\nExample\n\n\nconst data = [\n  {\ntime\n: 0, \nvalue\n: 1},\n  {\ntime\n: 300, \nvalue\n: 2},\n  {\ntime\n: 900, \nvalue\n: 3},\n  {\ntime\n: 1200, \nvalue\n: 4},\n  {\ntime\n: 1500, \nvalue\n: 5},\n  {\ntime\n: 2300, \nvalue\n: 6}\n];\n\nemit -points data\n| (\n  tail 2\n  | view table -title 'Unbatched';\n\n  batch 1000\n  | tail 2\n  | view table -title 'Batched'\n)\n\n\n\n\n\n\nIn addition to\n\nsort\n,\n\nhead\n,\nand\n\ntail\n,\nanother important processor that operates over batches is\n\nreduce\n.\nReduce is used for statistics and computations over groups of points.\nFor example, you might want to compute the average of a metric over\nfive-second intervals, or you might want to compute the count of a given\nevent over one-minute interval.\n\n\nBatching can also be specified by passing the \n-every\n option to relevant procs.\n\n\nFor example:\n\n\nconst data = [\n  {\ntime\n: :0:, \nvalue\n: 1},\n  {\ntime\n: :300:, \nvalue\n: 2},\n  {\ntime\n: :900:, \nvalue\n: 3},\n  {\ntime\n: :1200:, \nvalue\n: 4},\n  {\ntime\n: :1500:, \nvalue\n: 5},\n  {\ntime\n: :2300:, \nvalue\n: 6}\n];\n\nemit -points data\n| reduce -every :1000s:\n| (\n  tail 2\n  | view table -title 'Unbatched';\n\n  batch 1000\n  | tail 2\n  | view table -title 'Batched'\n)\n\n\n\n\nGrouping\n\n\nSeveral processors use the \nby\n operator to specify grouping fields. The \nby\n operator causes the processor to execute its logic separately for each unique combination of the grouping\nfields.\n\n\nThe following processors accept a \nby\n clause:\n\n\n\n\nhead\n\n\nput\n\n\nreduce\n\n\nsort\n\n\ntail\n\n\nuniq\n\n\n\n\nEach point in the output of the processor contains all of the grouping\nfields in addition to fields created by the processor. Consider the\nfollowing program, which uses modulo arithmetic to create groupings by\nremainder:\n\n\nemit -from :0: -limit 10\n| put x = Date.unix(time)%3\n| reduce y = count() by x\n| view table\n\n\n\n\nWhich will output:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x        \u2502 y        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 3        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2        \u2502 3        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn this example, the \ncount\n reducer is\napplied to each group of unique value of x (0, 1, and 2). There are 4 points\nwith an x value of 0 (time: 0, 3, 6, 9) and 3 points with each of the other values\nof x.\n\n\nPassing multiple fields into the by operator outputs one point per unique\ncombination of the grouping fields. For example the following Juttle program\noutputs 6 points, because there are 2 unique values of x, and 3 unique values of\ny:\n\n\nemit -from :0: -limit 10\n| put x = Date.unix(time)%2, y = Date.unix(time)%3\n| reduce by x,y\n| view table\n\n\n\n\nThis program outputs the following:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x        \u2502 y        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 0        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 0        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 2        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThese processors can use assignments and grouping operators together to\naggregate data over a batch and group by a field. For example if a\nstream of data had points with the fields time, host, metric, and value,\nthen \nreduce\n\ncould be used to aggregate values and group them by host.\n\n\nExample: Here's a more complex example that uses grouping in a few different ways\n\n\n//\n// Example of using grouping with reduce with both historical\n// and live sources\n//\n\n\n// Classify the point into either side given the parameter c\nfunction choose_side(c) {\n  return (c%2 == 0) ? \nleft\n : \nright\n;\n}\n\n// Generate synthetic historical data with a value and a grouping\nsub historical_points() {\n  emit -from :0: -limit 10\n  | put value=10 * count(), group=choose_side(count())\n}\n\n// Generate similar synthetic data but starting at now so it runs live\nsub live_points() {\n  emit -limit 10\n  | put value=count(), group=choose_side(count())\n}\n\n// Read the historical points in one big batch and reduce all points to a\n// single result table.\nhistorical_points\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'group','cnt','sum','avg','stdev'\n    -title \nHistorical statistics by group\n;\n\n// Read the historical points in 5 second batches\nhistorical_points\n| batch :5 seconds:\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'time','group','cnt','sum','avg','stdev'\n    -title \nHistorical 5-second statistics by group\n;\n\n// Read the live points and divide into 3 second batches\nlive_points\n| batch :3 seconds:\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'time','group','cnt','sum','avg','stdev'\n    -title \nLive 3-second statistics by group\n;\n\n\n\n\nForking and merging\n\n\nThe Juttle language supports forking and merging, by which you can\nprocess the same data multiple ways in parallel or combine two streams\ninto one.\n\n\nA semicolon (\n;\n) indicates the end of a branch of the flowgraph. Parentheses\ngroup flowgraph statements, and these two are used together for both forking and merging.\n\n\nWhen flowgraph branches are merged together, the points are ordered with respect to time so that the downstream processors can continue to operate on the incoming data in time order.\n\n\nHere's an imaginary program that both forks and merges:\n\n\nread mydata -last :day:\n| (\n    filter ...\n    | head 10;\n\n    reduce ...\n  )\n| view table\n\n\n\n\nThat example can also be illustrated like this:\n\n\n                         filter ... | head ...\n                       /                       \\\nread mydata -last :day:                          view\n                       \\                       /\n                             reduce ...\n\n\n\n\nHere are some other examples:\n\n\nExample: Forking to two views\n\n\nInstead of one output, this flowgraph ends with a group of two parallel views:\n\n\nemit -from :2 minutes ago: -every :1s: -limit 100\n| put message = \nHello World!\n, num_points = count(time)\n| ( view table;\n    view timechart\n      -valueField 'num_points'\n      -duration 100\n  )\n\n\n\n\nExample: Merging two sources into one output\n\n\nTwo sources are processed separately, then merged and processed\ntogether, and finally displayed in a single table:\n\n\n// Example: Merging two sources into one output\n// 9 parts having board_ids are joined against table of board_id-\nboardname\n\n( emit -from :0: -limit 9\n  | put part_id=count(), part=\npart-${part_id}\n, board_id=count() % 3 + 1\n  | remove time;\n\n  emit -from :0: -limit 3\n  | put board_id=count(), board=\nboard-${board_id}\n\n  | remove time;\n)\n| join board_id\n| keep part, board\n| view table\n\n\n\n\nJoining streams\n\n\nThe \njoin\n\nprocessor overrides the usual stream merging behavior (which reorders and\nforwards its input points). It consumes its input points and emits new\noutput points that combine their values.\n\n\nThe processor groups its input points into batches, either by using\nbatch information in the stream, or by treating each time stamp value as\na batch (there may be more than one point having the same time stamp).\nIf there are no time stamps for an input stream, all its points are\ntreated as one batch. join creates an output point by selecting a batch\nfrom each input and performing a relational join on the batches (but see\nfurther below for how join operates differently when given a single\nstream). To understand join, you need to understand how it matches\nbatches for joining, and how it joins points from batches as they are\nmatched.\n\n\nMatching batches between streams\n\n\nIt is easiest to think of batch matching by imagining points arriving in\nreal-time streams at the join. Whenever a complete batch arrives from\none input stream, that batch is joined against the most-recent complete\nbatch from the other input stream. Imagine that points with the same\ntime stamp arrive at the same instant and are processed simultaneously.\nIf two streams of input points having identical sequences of unique time\nstamps enter a join, each IS matched with the other by time stamp, and a\nsequence of output points is produced having those same time stamps.\n\n\nWhen batches arrive with different time stamps, or at different\nfrequencies, match-against-most-recent implies there is an output point\nfor each unique time stamp on any input. It also implies that a batch\nmight be used more than once if the other input has new batches arriving\nmore rapidly. An example of this arrangement is a join between a stream\nof points that contains customer IDs, and a table (a single batch) of\npoints that contains customer IDs and additional customer information.\nThe customer information batch remains parked at one join input while\nmany customer points arrive at the other input and are joined against it.\n\n\nSometimes you want a one-to-one match between time stamps in input\nstreams. For this behavior, include time in fieldName list of the join.\nA point is only matched and joined against another point that has the\nsame time stamp. You can relax this by specifying \n-maxoffset\n, \nwhich allows points to match\nif their time stamps differ by less than this (it is still treated as a\none-to-one match, so a time stamp will still be matched at most once in\nthis mode). If points are arriving more quickly on one input, or if the\ntime stamps for one input are much older, that input is \"fast-forwarded\"\nto a newer point with the best match, and any older points are\ndiscarded. The expected use for this is in joining streams that have the\nsame frequency but that are staggered in time. If join such streams\nwithout specifying the time field, the join produces twice as many\noutput points, with each input batch participating in two join\noperations as the inputs ratchet past each other.\n\n\nAlthough we have been imagining points with the same time stamp arriving\nsimultaneously, points with the same time stamp may actually arrive at\ndifferent times. join emulates simultaneous arrival by waiting until a\nnew batch has begun arriving before processing a waiting batch (and\ncoordinating this wait between inputs). join does not attempt to\nestimate or compensate for skewed input timing. It minimizes the\ndifference between matched inputs by possibly fast-forwarding one of the\ninputs if it has a backlogged time stamp closer in value. It produces\nthe same results on historic data and real-time data, or a combination\nof them.\n\n\nJoining points between batches\n\n\nOnce a batch has been chosen from each input, for each pair of points\nhaving matching join field values, an output point is produced that\ncombines the fields of the input points. In case of collisions, a\ntie-breaker remains to be specified. This is similar to an SQL join,\nwith the batches being like tables, and the points in the batches being\nlike rows in the tables. The input batching and join fields are usually\nspecified in a way that yields a one-to-one match between points in the\nbatch, and you get an output point for each input point. If this is not\nthe case, you will have output points for multi-way combinations of\ninput points, and you would likely apply an additional filter or\naggregation downstream to reduce the result set.\n\n\nTime stamps are ignored during the relational join operation, and the\noutput point is given the newest time stamp of the input batches.\n\n\nJoining points in a single stream\n\n\nAlthough there is no formal notion of a relational join for a single\nstream, it is convenient to be able to operate on the points of a single\nstream as if they come from different inputs and are merged upstream.\njoin does this when applied to a single input stream: for each batch of\npoints (or each group of points having the same time stamp), it\nseparates them into groups based on matching join keys, and then\ngenerates an output point that is the union of the fields of all the\npoints in a group (a many-to-many join of all the points in the group).\nThe output contains one point for each time stamp and unique join key\nvalue within the points.", 
            "title": "Dataflow"
        }, 
        {
            "location": "/concepts/dataflow/#dataflow-concepts", 
            "text": "This section lays out some of the conceptual framework for the Juttle dataflow language.    Dataflow Concepts  Dataflow Language  Data Streams  Timeliness  Batching  Grouping  Forking and merging  Joining streams  Matching batches between streams  Joining points between batches  Joining points in a single stream", 
            "title": "Dataflow Concepts"
        }, 
        {
            "location": "/concepts/dataflow/#dataflow-language", 
            "text": "Juttle is a declarative dataflow or stream processing language.  Each Juttle program consists of one or more flowgraphs that generate or read in\nstreams of data, process the streams, and output results.  In each Juttle flowgraph, you pull data into your program with  source nodes  that either  emit  synthetic data or  read  from an external data source. In the middle of your flowgraph, you can perform operations on your data using  processor nodes . You can chain and arrange these nodes arbitrarily to enable parallel processing and forking/merging of the data stream. Then each branch of the flowgraph output can either  view  the data in a graph using a client side visualization library, or  write  data to a backend for storage or to take an action like sending an alert.   The language is declarative, in that a program specifies at a high level what data to retrieve, what to do to it, and how to visualize it, not the programming details of how to actually perform these operations. This high level specification is mapped into an execution plan by the Juttle compiler, which can delegate filtering and computation to back end data stores via  adapters , and which passes the data and configuration options to client-side  views  to render data visualizations.", 
            "title": "Dataflow Language"
        }, 
        {
            "location": "/concepts/dataflow/#data-streams", 
            "text": "In a Juttle, data is represented as a stream of points, where each point\nconsists of a number of key/value fields of various  data types .  When they enter a Juttle flowgraph, data points are represented and\nhandled identically, regardless of whether they were synthetically generated, originated by tapping into a live data source, read from a persistent storage back end, or a combination. Data points may represent structured time series metrics or semi-structured log events.  Example: three points in a stream of metric data  { name  :  errors ,  value  : 42,  time  :  2014-05-28T22:11:35.332Z }\n{ name  :  errors ,  value  : 44,  time  :  2014-05-28T22:11:35.353Z }\n{ name  :  errors ,  value  : 46,  time  :  2014-05-28T22:11:35.532Z }  Example: two points in a stream of Web log data  { date : 2014-07-03T10:38:44-07:00 , ip : 79.122.57.6 , referer : - , agent : - , ctry : US , uri : /iotest/reply/core/utf/krw , method : GET , bytes :18270, bytes_in :1768, status :200, time : 2014-05-28T22:11:35.332Z }\n{ date : 2014-07-03T10:38:45-07:00 , ip : 161.182.107.224 , referer : - , agent : Mozilla/4.0\n(compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; SLCC2; .NET CLR\n2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2) , ctry : US , uri : /flick/token/scss/pause/api , method : GET , bytes :8219, bytes_in :682, status :200, time : 2014-05-28T22:11:35.332Z }", 
            "title": "Data Streams"
        }, 
        {
            "location": "/concepts/dataflow/#timeliness", 
            "text": "Points streaming through Juttle flowgraphs must always be ordered by time, and\nthey must arrive in a timely manner to avoid buffering problems and delays in\nthe runtime.    Points flow through each processor in chronological order.  If a point has a time field, the value of its time field should be\ngreater than or equal to the value of the time field of the previous\npoint (that had a time field). Points for which that is not true are\ncalled \"out of order\".    Consecutive points must arrive in a timely fashion with respect to their\n    time fields.  For example, if two consecutive points with time stamps one millisecond\napart arrive 20 seconds apart, the second one has arrived \"late\". Based on\nthe specific system resources available in the Juttle runtime, this may end\nup causing the point to not be included in the flowgraph because it arrived too late.    Points that do not have a time field (aka \"timeless points\") are treated as if they occurred at the dawn of time. Therefore when merging or joining a stream of timeless points with a stream of timed data points, all points from the timeless branch are treated as if they occurred \"before\" the timed points.", 
            "title": "Timeliness"
        }, 
        {
            "location": "/concepts/dataflow/#batching", 
            "text": "Many Juttle processors operate point-by-point, but others operate over\nbatches of consecutive points in a given time interval.  The meanings of sort , head ,\nand tail \nare probably known to anyone with exposure to the UNIX shell. But there\nis a twist when using these in Juttle: sort, head, and tail processors\ndon't operate on files like their UNIX counterparts; they operate on\nstreams of indeterminate lengths. And sort \nor tail \ncan't be applied to a stream of unknown (and potentially unbounded)\nlength, because you need to know where the stream ends in order to start\nthe sort or tail operation.  To use these on streams of indeterminate length, you segment the stream of\npoints into consecutive batches that can be processed one-at-a-time by these\nprocessors. Think of segmentation as inserting time division markers into the\nstream of points; these marker carry no data and serve no purpose other than to\ndelineate batch boundaries that are used by processors to determine which points\nto process together. Segmenting points in this way can be done with the batch  processor or by specifying the  -every  option to the processors themselves.  For streams that are of bounded length (such as pulled from long-term storage),\nan end of batch marker is always present at the end of the stream.  This simple example illustrates how  batch \nchanges the data in two different ways when starting from the same initial data\nstream. A diagram shows the data flow with markers pointing out the logical\nseparation of batches of points.  Example  const data = [\n  { time : 0,  value : 1},\n  { time : 300,  value : 2},\n  { time : 900,  value : 3},\n  { time : 1200,  value : 4},\n  { time : 1500,  value : 5},\n  { time : 2300,  value : 6}\n];\n\nemit -points data\n| (\n  tail 2\n  | view table -title 'Unbatched';\n\n  batch 1000\n  | tail 2\n  | view table -title 'Batched'\n)   In addition to sort , head ,\nand tail ,\nanother important processor that operates over batches is reduce .\nReduce is used for statistics and computations over groups of points.\nFor example, you might want to compute the average of a metric over\nfive-second intervals, or you might want to compute the count of a given\nevent over one-minute interval.  Batching can also be specified by passing the  -every  option to relevant procs.  For example:  const data = [\n  { time : :0:,  value : 1},\n  { time : :300:,  value : 2},\n  { time : :900:,  value : 3},\n  { time : :1200:,  value : 4},\n  { time : :1500:,  value : 5},\n  { time : :2300:,  value : 6}\n];\n\nemit -points data\n| reduce -every :1000s:\n| (\n  tail 2\n  | view table -title 'Unbatched';\n\n  batch 1000\n  | tail 2\n  | view table -title 'Batched'\n)", 
            "title": "Batching"
        }, 
        {
            "location": "/concepts/dataflow/#grouping", 
            "text": "Several processors use the  by  operator to specify grouping fields. The  by  operator causes the processor to execute its logic separately for each unique combination of the grouping\nfields.  The following processors accept a  by  clause:   head  put  reduce  sort  tail  uniq   Each point in the output of the processor contains all of the grouping\nfields in addition to fields created by the processor. Consider the\nfollowing program, which uses modulo arithmetic to create groupings by\nremainder:  emit -from :0: -limit 10\n| put x = Date.unix(time)%3\n| reduce y = count() by x\n| view table  Which will output:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x        \u2502 y        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 3        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2        \u2502 3        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In this example, the  count  reducer is\napplied to each group of unique value of x (0, 1, and 2). There are 4 points\nwith an x value of 0 (time: 0, 3, 6, 9) and 3 points with each of the other values\nof x.  Passing multiple fields into the by operator outputs one point per unique\ncombination of the grouping fields. For example the following Juttle program\noutputs 6 points, because there are 2 unique values of x, and 3 unique values of\ny:  emit -from :0: -limit 10\n| put x = Date.unix(time)%2, y = Date.unix(time)%3\n| reduce by x,y\n| view table  This program outputs the following:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x        \u2502 y        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 0        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 0        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0        \u2502 1        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 2        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  These processors can use assignments and grouping operators together to\naggregate data over a batch and group by a field. For example if a\nstream of data had points with the fields time, host, metric, and value,\nthen  reduce \ncould be used to aggregate values and group them by host.  Example: Here's a more complex example that uses grouping in a few different ways  //\n// Example of using grouping with reduce with both historical\n// and live sources\n//\n\n\n// Classify the point into either side given the parameter c\nfunction choose_side(c) {\n  return (c%2 == 0) ?  left  :  right ;\n}\n\n// Generate synthetic historical data with a value and a grouping\nsub historical_points() {\n  emit -from :0: -limit 10\n  | put value=10 * count(), group=choose_side(count())\n}\n\n// Generate similar synthetic data but starting at now so it runs live\nsub live_points() {\n  emit -limit 10\n  | put value=count(), group=choose_side(count())\n}\n\n// Read the historical points in one big batch and reduce all points to a\n// single result table.\nhistorical_points\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'group','cnt','sum','avg','stdev'\n    -title  Historical statistics by group ;\n\n// Read the historical points in 5 second batches\nhistorical_points\n| batch :5 seconds:\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'time','group','cnt','sum','avg','stdev'\n    -title  Historical 5-second statistics by group ;\n\n// Read the live points and divide into 3 second batches\nlive_points\n| batch :3 seconds:\n| reduce cnt=count(value), sum=sum(value), avg=avg(value), stdev=sigma(value) by group\n| view table\n    -columnOrder 'time','group','cnt','sum','avg','stdev'\n    -title  Live 3-second statistics by group ;", 
            "title": "Grouping"
        }, 
        {
            "location": "/concepts/dataflow/#forking-and-merging", 
            "text": "The Juttle language supports forking and merging, by which you can\nprocess the same data multiple ways in parallel or combine two streams\ninto one.  A semicolon ( ; ) indicates the end of a branch of the flowgraph. Parentheses\ngroup flowgraph statements, and these two are used together for both forking and merging.  When flowgraph branches are merged together, the points are ordered with respect to time so that the downstream processors can continue to operate on the incoming data in time order.  Here's an imaginary program that both forks and merges:  read mydata -last :day:\n| (\n    filter ...\n    | head 10;\n\n    reduce ...\n  )\n| view table  That example can also be illustrated like this:                           filter ... | head ...\n                       /                       \\\nread mydata -last :day:                          view\n                       \\                       /\n                             reduce ...  Here are some other examples:  Example: Forking to two views  Instead of one output, this flowgraph ends with a group of two parallel views:  emit -from :2 minutes ago: -every :1s: -limit 100\n| put message =  Hello World! , num_points = count(time)\n| ( view table;\n    view timechart\n      -valueField 'num_points'\n      -duration 100\n  )  Example: Merging two sources into one output  Two sources are processed separately, then merged and processed\ntogether, and finally displayed in a single table:  // Example: Merging two sources into one output\n// 9 parts having board_ids are joined against table of board_id- boardname\n\n( emit -from :0: -limit 9\n  | put part_id=count(), part= part-${part_id} , board_id=count() % 3 + 1\n  | remove time;\n\n  emit -from :0: -limit 3\n  | put board_id=count(), board= board-${board_id} \n  | remove time;\n)\n| join board_id\n| keep part, board\n| view table", 
            "title": "Forking and merging"
        }, 
        {
            "location": "/concepts/dataflow/#joining-streams", 
            "text": "The  join \nprocessor overrides the usual stream merging behavior (which reorders and\nforwards its input points). It consumes its input points and emits new\noutput points that combine their values.  The processor groups its input points into batches, either by using\nbatch information in the stream, or by treating each time stamp value as\na batch (there may be more than one point having the same time stamp).\nIf there are no time stamps for an input stream, all its points are\ntreated as one batch. join creates an output point by selecting a batch\nfrom each input and performing a relational join on the batches (but see\nfurther below for how join operates differently when given a single\nstream). To understand join, you need to understand how it matches\nbatches for joining, and how it joins points from batches as they are\nmatched.", 
            "title": "Joining streams"
        }, 
        {
            "location": "/concepts/dataflow/#matching-batches-between-streams", 
            "text": "It is easiest to think of batch matching by imagining points arriving in\nreal-time streams at the join. Whenever a complete batch arrives from\none input stream, that batch is joined against the most-recent complete\nbatch from the other input stream. Imagine that points with the same\ntime stamp arrive at the same instant and are processed simultaneously.\nIf two streams of input points having identical sequences of unique time\nstamps enter a join, each IS matched with the other by time stamp, and a\nsequence of output points is produced having those same time stamps.  When batches arrive with different time stamps, or at different\nfrequencies, match-against-most-recent implies there is an output point\nfor each unique time stamp on any input. It also implies that a batch\nmight be used more than once if the other input has new batches arriving\nmore rapidly. An example of this arrangement is a join between a stream\nof points that contains customer IDs, and a table (a single batch) of\npoints that contains customer IDs and additional customer information.\nThe customer information batch remains parked at one join input while\nmany customer points arrive at the other input and are joined against it.  Sometimes you want a one-to-one match between time stamps in input\nstreams. For this behavior, include time in fieldName list of the join.\nA point is only matched and joined against another point that has the\nsame time stamp. You can relax this by specifying  -maxoffset , \nwhich allows points to match\nif their time stamps differ by less than this (it is still treated as a\none-to-one match, so a time stamp will still be matched at most once in\nthis mode). If points are arriving more quickly on one input, or if the\ntime stamps for one input are much older, that input is \"fast-forwarded\"\nto a newer point with the best match, and any older points are\ndiscarded. The expected use for this is in joining streams that have the\nsame frequency but that are staggered in time. If join such streams\nwithout specifying the time field, the join produces twice as many\noutput points, with each input batch participating in two join\noperations as the inputs ratchet past each other.  Although we have been imagining points with the same time stamp arriving\nsimultaneously, points with the same time stamp may actually arrive at\ndifferent times. join emulates simultaneous arrival by waiting until a\nnew batch has begun arriving before processing a waiting batch (and\ncoordinating this wait between inputs). join does not attempt to\nestimate or compensate for skewed input timing. It minimizes the\ndifference between matched inputs by possibly fast-forwarding one of the\ninputs if it has a backlogged time stamp closer in value. It produces\nthe same results on historic data and real-time data, or a combination\nof them.", 
            "title": "Matching batches between streams"
        }, 
        {
            "location": "/concepts/dataflow/#joining-points-between-batches", 
            "text": "Once a batch has been chosen from each input, for each pair of points\nhaving matching join field values, an output point is produced that\ncombines the fields of the input points. In case of collisions, a\ntie-breaker remains to be specified. This is similar to an SQL join,\nwith the batches being like tables, and the points in the batches being\nlike rows in the tables. The input batching and join fields are usually\nspecified in a way that yields a one-to-one match between points in the\nbatch, and you get an output point for each input point. If this is not\nthe case, you will have output points for multi-way combinations of\ninput points, and you would likely apply an additional filter or\naggregation downstream to reduce the result set.  Time stamps are ignored during the relational join operation, and the\noutput point is given the newest time stamp of the input batches.", 
            "title": "Joining points between batches"
        }, 
        {
            "location": "/concepts/dataflow/#joining-points-in-a-single-stream", 
            "text": "Although there is no formal notion of a relational join for a single\nstream, it is convenient to be able to operate on the points of a single\nstream as if they come from different inputs and are merged upstream.\njoin does this when applied to a single input stream: for each batch of\npoints (or each group of points having the same time stamp), it\nseparates them into groups based on matching join keys, and then\ngenerates an output point that is the union of the fields of all the\npoints in a group (a many-to-many join of all the points in the group).\nThe output contains one point for each time stamp and unique join key\nvalue within the points.", 
            "title": "Joining points in a single stream"
        }, 
        {
            "location": "/concepts/programming_constructs/", 
            "text": "Programming Constructs\n\n\nJuttle supports several programming constructs that are similar to other programming languages and make it easier to write and compose Juttle flowgraphs.\n\n\nIf you already know JavaScript and UNIX command-line tools, then elements of of Juttle will look familiar to you. Functions, variables, \nif\n statements, and other elements of the Juttle syntax are similar to their JavaScript equivalents.\nAt the same time Juttle also borrows elements from UNIX command-line tools, with pipes and options that look like -option arguments to shell programs.\n\n\n\n\n\n\nProgramming Constructs\n\n\nConstants\n\n\nFunctions\n\n\nVariables\n\n\nString Templating\n\n\nModules\n\n\nExporting symbols\n\n\nImporting modules\n\n\nModule Behavior\n\n\n\n\n\n\nSubgraphs\n\n\n\n\n\n\nConstants\n\n\nYou can write constants in Juttle and use them in context within the flowgraph. By their nature, constants can't be assigned to or modified after being defined.\n\n\nJuttle uses lexical scoping, so constants are scoped to the context in which\nthey're written.\n\n\nFor example:\n\n\nconst n = 10;\nemit -limit n -from :0: | reduce count() | view table -title \n10 points\n;\n\nconst n2 = n * n;\nemit -limit n -from :0: | reduce count() |  view table -title \n100 points\n;\n\n\n\n\nFunctions\n\n\nA Juttle function performs a calculation based on provided inputs and returns a\nsingle result. Juttle comes with some built-in functions exposed through the built-in \nmodules\n, and you can also define your own functions.\n\n\nA function can operate over any number of arguments, and can be invoked\nwith literals, variables, or field values as the arguments to the function.\n\n\nFor example, the following computes a point count and the square of the point count using the function \nMath.pow\n:\n\n\nemit -limit 10\n| put num_points=count(),\n      points_squared=Math.pow(num_points, 2),\n      points_cubed=Math.pow(num_points, 3)\n\n\n\n\nUsers can define their own Juttle functions, using the \nfunction\n keyword. The syntax is similar to Javascript functions, although parameters can be given default values.\n\n\nfunction functionName(parameter1[=default_value1], parameter2[=default_value2]...) {\n   code to execute\n}\n\n\n\n\nFunctions can contain variable or constant declarations, assignments,\n\nif\n statements, and \nreturn\n statements. Juttle functions can also be\nnested by declaring one function inside another function.\n\n\nFor example, the following is a recursive implementation of \nMath.pow\n with a default exponent of 2:\n\n\nfunction pow(value, exponent=2) {\n    if (exponent == 1) {\n        return value;\n    } else {\n        return value * pow(value, exponent - 1);\n    }\n}\n\nemit -limit 10\n| put points=count(),\n      points_2=pow(points), // default exponent = 2\n      points_3=pow(points, 3)\n\n\n\n\nVariables\n\n\nVariables can be defined inside reducers and functions, but not inside\nsubgraphs or at the top level of a flowgraph. Since Juttle uses lexical\nscoping, variables are scoped to the reducer or function in which\nthey're defined.\n\n\nUnlike constants, variables support assignment after they are defined:\n\n\nFor example:\n\n\nconst x = 5;\nfunction xplus(y) {\n    var z = x;\n    z = x + y;\n    return z;\n}\nemit | put value=xplus(10);\n\n\n\n\nString Templating\n\n\nStrings in Juttle support template syntax to enable embedded expressions to be evaluated and converted to a string more conveniently.\n\n\nThe syntax for embedded expressions is \n${expression}\n, where the referenced expression can be a \nconstant\n, variable, literal, a value from a field in the point, or the result of an expression.\n\n\nFor example, the following juttle builds the same text string using both templating and manual construction of the string values.\n\n\nconst N = 10;\nemit -limit N\n| put text1=\npoint ${count()} of ${N}, elapsed time ${time - :now:}\n\n| put text2=\npoint \n + Number.toString(count())  + \n of \n + Number.toString(N) +\n            \n, elapsed time \n + Duration.toString(time - :now:);\n\n\n\n\nModules\n\n\nJuttle modules are a way of reusing code across multiple Juttle programs.\n\n\nExporting symbols\n\n\nAny saved juttle program can be used as a module if it exports functions, \nconstants\n, \ninputs\n, \nsubgraphs\n, or \nreducers\n by prefixing them with the \nexport\n keyword.\n\n\nFor example:\n\n\n    export const answer = 42;\n    export function ask(question) { return answer; }\n    export sub elucidate { put a = answer }\n    export reducer answerable(q) { /* ... */ }\n\n\n\n\nImporting modules\n\n\nGiven a juttle program containing exported symbols that is saved at a known\nlocation, then in another program you can import it as a module, using the\n\nimport\n keyword:\n\n\n    import \nmodule_path\n as local_name;\n\n\n\n\nHow the \nmodule_path\n is resolved depends on the specific capabilities of the\napplication in which Juttle is running. For the Juttle CLI, it can be either a\nrelative or absolute filesystem path or a URL to a remote Juttle file containing\nexported code.\n\n\nOnce it is resolved, the import statement pulls all exported symbols from the\nspecified Juttle module and makes them available under the specified local\nnamespace.\n\n\nYou can reference the imported symbol(s) like this:\n\n\n    local_name.symbol_name\n\n\n\n\n\n\nlocal_name is the local name from the import command above.\n\n\nsymbol_name is the name of the exported subgraph, constant, function, \n   or other code fragment from the imported program file.\n\n\n\n\nModule Behavior\n\n\nWhen defining or importing modules:\n\n\n\n\n\n\nAn \nimport\n or \nexport\n statement is only valid in the top-level context of a program. This means that an import or export statement cannot be used inside\n    \nsubgraphs\n,\n    \nfunctions\n,\n    or\n    \nreducers\n.\n\n\n\n\n\n\nCyclic module dependencies are forbidden. For example, a compilation error occurs if module1 imports module2 and module2 also imports module1.\n\n\n\n\n\n\nTop-level flowgraph statements are not exported, instantiated, or executed upon module import.\n\n\nIn the first example above, \nemit | stamper -mark \"test\"\n is not exported.\n\n\n\n\n\n\nExample: export from module\n\n\nThis juttle module exports a function, a subgraph, and a constant. \n\n\nexport const pi = 3.14;                                // exported constant\nconst not_exported = 2;                                     // not exported\nexport function double(x) { return x * not_exported; } // exported function\n\nexport sub stamper(mark) {                             // exported subgraph\n    put stamp = mark\n    | put stamp2 = mark\n}\n\n// top-level flowgraph is not exported\nemit\n| stamper -mark \ntest\n\n| view table;\n\n\n\n\nExample: import by filename\n\n\nThis example assumes that the program with exports is saved at path\n\ndocs/examples/concepts/export_module.juttle\n.\n\n\n// This program is runnable from CLI from the juttle repo root,\n// node ./bin/juttle docs/examples/concepts/import_module.juttle\n\nimport 'docs/examples/concepts/export_module.juttle' as my_module;\nemit\n| my_module.stamper -mark 'test'\n\n\n\n\nSubgraphs\n\n\nSubgraphs are reusable Juttle fragments that can be invoked within the\ncurrent program or exported as modules that other programs can import.\n\n\nSubgraphs are declared with the \nsub\n keyword, like this:\n\n\nsub sub_name([arg1,arg2]) {\n     juttle code\n}\n\n\n\n\nA subgraph can take as many arguments as it needs, or none.\n\n\nExample: simple sub with no arguments\n\n\n// declare a subgraph called \nmy_viz\n with no params\nsub my_viz() {\n  put num_points = count()\n  | (view table; view timechart)\n};\n\nemit -limit 2\n| put message = \nHello World!\n\n| my_viz; // invoke the subgraph here\n\n\n\n\n\n\nWith this approach, experienced coders can express complex business\nlogic and rich visualizations, then make them available to other Juttle\nauthors to reuse.\n\n\nExample: sub with optional argument\n\n\nHere the default value will be used if the program that invokes the subgraph\ndoes not specify a different value:\n\n\nsub banner(title='Generic title') {\n  emit -limit 1 | put value=title | view tile -timeField '';\n}\nbanner -title 'Custom title';\n\n\n\n\nSee \nField referencing\n\nfor tips on working with the contents of fields from within a subgraph.", 
            "title": "Programming Constructs"
        }, 
        {
            "location": "/concepts/programming_constructs/#programming-constructs", 
            "text": "Juttle supports several programming constructs that are similar to other programming languages and make it easier to write and compose Juttle flowgraphs.  If you already know JavaScript and UNIX command-line tools, then elements of of Juttle will look familiar to you. Functions, variables,  if  statements, and other elements of the Juttle syntax are similar to their JavaScript equivalents.\nAt the same time Juttle also borrows elements from UNIX command-line tools, with pipes and options that look like -option arguments to shell programs.    Programming Constructs  Constants  Functions  Variables  String Templating  Modules  Exporting symbols  Importing modules  Module Behavior    Subgraphs", 
            "title": "Programming Constructs"
        }, 
        {
            "location": "/concepts/programming_constructs/#constants", 
            "text": "You can write constants in Juttle and use them in context within the flowgraph. By their nature, constants can't be assigned to or modified after being defined.  Juttle uses lexical scoping, so constants are scoped to the context in which\nthey're written.  For example:  const n = 10;\nemit -limit n -from :0: | reduce count() | view table -title  10 points ;\n\nconst n2 = n * n;\nemit -limit n -from :0: | reduce count() |  view table -title  100 points ;", 
            "title": "Constants"
        }, 
        {
            "location": "/concepts/programming_constructs/#functions", 
            "text": "A Juttle function performs a calculation based on provided inputs and returns a\nsingle result. Juttle comes with some built-in functions exposed through the built-in  modules , and you can also define your own functions.  A function can operate over any number of arguments, and can be invoked\nwith literals, variables, or field values as the arguments to the function.  For example, the following computes a point count and the square of the point count using the function  Math.pow :  emit -limit 10\n| put num_points=count(),\n      points_squared=Math.pow(num_points, 2),\n      points_cubed=Math.pow(num_points, 3)  Users can define their own Juttle functions, using the  function  keyword. The syntax is similar to Javascript functions, although parameters can be given default values.  function functionName(parameter1[=default_value1], parameter2[=default_value2]...) {\n   code to execute\n}  Functions can contain variable or constant declarations, assignments, if  statements, and  return  statements. Juttle functions can also be\nnested by declaring one function inside another function.  For example, the following is a recursive implementation of  Math.pow  with a default exponent of 2:  function pow(value, exponent=2) {\n    if (exponent == 1) {\n        return value;\n    } else {\n        return value * pow(value, exponent - 1);\n    }\n}\n\nemit -limit 10\n| put points=count(),\n      points_2=pow(points), // default exponent = 2\n      points_3=pow(points, 3)", 
            "title": "Functions"
        }, 
        {
            "location": "/concepts/programming_constructs/#variables", 
            "text": "Variables can be defined inside reducers and functions, but not inside\nsubgraphs or at the top level of a flowgraph. Since Juttle uses lexical\nscoping, variables are scoped to the reducer or function in which\nthey're defined.  Unlike constants, variables support assignment after they are defined:  For example:  const x = 5;\nfunction xplus(y) {\n    var z = x;\n    z = x + y;\n    return z;\n}\nemit | put value=xplus(10);", 
            "title": "Variables"
        }, 
        {
            "location": "/concepts/programming_constructs/#string-templating", 
            "text": "Strings in Juttle support template syntax to enable embedded expressions to be evaluated and converted to a string more conveniently.  The syntax for embedded expressions is  ${expression} , where the referenced expression can be a  constant , variable, literal, a value from a field in the point, or the result of an expression.  For example, the following juttle builds the same text string using both templating and manual construction of the string values.  const N = 10;\nemit -limit N\n| put text1= point ${count()} of ${N}, elapsed time ${time - :now:} \n| put text2= point   + Number.toString(count())  +   of   + Number.toString(N) +\n             , elapsed time   + Duration.toString(time - :now:);", 
            "title": "String Templating"
        }, 
        {
            "location": "/concepts/programming_constructs/#modules", 
            "text": "Juttle modules are a way of reusing code across multiple Juttle programs.", 
            "title": "Modules"
        }, 
        {
            "location": "/concepts/programming_constructs/#exporting-symbols", 
            "text": "Any saved juttle program can be used as a module if it exports functions,  constants ,  inputs ,  subgraphs , or  reducers  by prefixing them with the  export  keyword.  For example:      export const answer = 42;\n    export function ask(question) { return answer; }\n    export sub elucidate { put a = answer }\n    export reducer answerable(q) { /* ... */ }", 
            "title": "Exporting symbols"
        }, 
        {
            "location": "/concepts/programming_constructs/#importing-modules", 
            "text": "Given a juttle program containing exported symbols that is saved at a known\nlocation, then in another program you can import it as a module, using the import  keyword:      import  module_path  as local_name;  How the  module_path  is resolved depends on the specific capabilities of the\napplication in which Juttle is running. For the Juttle CLI, it can be either a\nrelative or absolute filesystem path or a URL to a remote Juttle file containing\nexported code.  Once it is resolved, the import statement pulls all exported symbols from the\nspecified Juttle module and makes them available under the specified local\nnamespace.  You can reference the imported symbol(s) like this:      local_name.symbol_name   local_name is the local name from the import command above.  symbol_name is the name of the exported subgraph, constant, function, \n   or other code fragment from the imported program file.", 
            "title": "Importing modules"
        }, 
        {
            "location": "/concepts/programming_constructs/#module-behavior", 
            "text": "When defining or importing modules:    An  import  or  export  statement is only valid in the top-level context of a program. This means that an import or export statement cannot be used inside\n     subgraphs ,\n     functions ,\n    or\n     reducers .    Cyclic module dependencies are forbidden. For example, a compilation error occurs if module1 imports module2 and module2 also imports module1.    Top-level flowgraph statements are not exported, instantiated, or executed upon module import.  In the first example above,  emit | stamper -mark \"test\"  is not exported.    Example: export from module  This juttle module exports a function, a subgraph, and a constant.   export const pi = 3.14;                                // exported constant\nconst not_exported = 2;                                     // not exported\nexport function double(x) { return x * not_exported; } // exported function\n\nexport sub stamper(mark) {                             // exported subgraph\n    put stamp = mark\n    | put stamp2 = mark\n}\n\n// top-level flowgraph is not exported\nemit\n| stamper -mark  test \n| view table;  Example: import by filename  This example assumes that the program with exports is saved at path docs/examples/concepts/export_module.juttle .  // This program is runnable from CLI from the juttle repo root,\n// node ./bin/juttle docs/examples/concepts/import_module.juttle\n\nimport 'docs/examples/concepts/export_module.juttle' as my_module;\nemit\n| my_module.stamper -mark 'test'", 
            "title": "Module Behavior"
        }, 
        {
            "location": "/concepts/programming_constructs/#subgraphs", 
            "text": "Subgraphs are reusable Juttle fragments that can be invoked within the\ncurrent program or exported as modules that other programs can import.  Subgraphs are declared with the  sub  keyword, like this:  sub sub_name([arg1,arg2]) {\n     juttle code\n}  A subgraph can take as many arguments as it needs, or none.  Example: simple sub with no arguments  // declare a subgraph called  my_viz  with no params\nsub my_viz() {\n  put num_points = count()\n  | (view table; view timechart)\n};\n\nemit -limit 2\n| put message =  Hello World! \n| my_viz; // invoke the subgraph here   With this approach, experienced coders can express complex business\nlogic and rich visualizations, then make them available to other Juttle\nauthors to reuse.  Example: sub with optional argument  Here the default value will be used if the program that invokes the subgraph\ndoes not specify a different value:  sub banner(title='Generic title') {\n  emit -limit 1 | put value=title | view tile -timeField '';\n}\nbanner -title 'Custom title';  See  Field referencing \nfor tips on working with the contents of fields from within a subgraph.", 
            "title": "Subgraphs"
        }, 
        {
            "location": "/concepts/fields/", 
            "text": "Fields\n\n\nEach Juttle data point consists of a set of named fields. The name of a field is always a string, but the value can be of various \ntypes\n.\n\n\n This needs to be updated to clarify stream context vs function context.\n\n\nReferencing\n\n\nJuttle includes two operators for field referencing -- the \n*\n operator and the \n#\n operator.\n\n\nConsider this simple Juttle:\n\n\nemit\n| put apple='fuji'\n| put fruit = apple\n| view text\n\n\n\n\nThe left-hand side (LHS) of this put expression is expressing \"I want to\nset the contents of the field called fruit to\u2026 .\" This is an example of\nwhat we call field referencing.\n\n\nWe could have written the above instead as:\n\n\nemit\n| put apple = 'fuji'\n| put *'fruit' = *'apple'\n| view text\n\n\n\n\nIn this case, 'fruit' is actually a string literal, and the \n*\n operator\ntells Juttle to affect the contents of a field with the name shown in\nthat string. Similarly 'apple' is also a string literal and the \n*\n operator tells Juttle to read from the specified field.\n\n\nAlternatively, we could have omitted the quotes and written it as:\n\n\nemit\n| put apple = 'fuji'\n| put #fruit = #apple\n| view text\n\n\n\n\nThe \n#\n operator functions similarly to the \n*\n operator except that what follows is an identifier and not a quoted string.\n\n\nHowever, Juttle recognizes that the LHS of a put expression\nalways becomes a field reference, and so a bare identifier such as \nfruit\n\nthat doesn't match a const or var that is in scope is assumed to be a field\ndereference and is always treated the same as if we'd typed the explicit \n*'fruit'\n or \n#fruit.\n\n\nThe use of explicit field referencing operators becomes apparent when there is a const or var with the same name as a field in scope:\n\n\nconst apple='gala';\nemit -limit 2\n| put apple='fuji'\n| (\n    put fruit='apple' | view text;     // fruit will be set to 'apple'\n    put fruit=apple | view text;       // fruit will be set to 'gala'\n    put fruit=*'apple' | view text;    // fruit will be set to 'fuji'\n    put fruit=#apple | view text;      // fruit will be set to 'fuji'\n  )\n\n\n\n\nThe rules above apply to the LHS of\n\nput\n,\n\nreduce\n,\nand\n\nfilter\n\nexpressions and the RHS of put and filter expressions.\n\n\nThere can be other cases where explicit field referencing is\nnecessary. The first is within\n\nsubgraphs\n\nthat take field names as parameters. In this case, what's being passed\nin as a parameter is just a string that is the name of the field, so the \n*\n operator allows access to the field of that name.\n\n\nsub cp(to, from) { put *to = *from }\nemit | put apple = 'fuji' | cp -to \nfruit\n -from \napple\n | view text\n\n\n\n\nAnother place is when the \"update\" function of a\n\nreducer\n\nneeds to access the contents of a field.\n\n\nA handy side-effect of the explicit field referencing is that\nit can allow us to work with fields whose names would not be legal as\nbare identifiers.\n\n\nemit\n| put *':) my favorite!' = 'honeycrisp'\n| view text\n\n\n\n\nTo summarize, this is how field referencing works in Juttle:\n\n\n\n\nOn the LHS of put, reduce, and filter expressions, a bare identifier\n    x is treated the same as the explicit \n*'x'\n or #x unless a variable \nx\n is in scope.\n\n\nOn the RHS of put and filter expressions, a bare identifier x is\n    treated the same as the explicit \n*'x'\n, unless a const/variable\n    named x is in scope, in which case the contents of const/variable x\n    is used.\n\n\nInside a subgraph, or reducer \"update\" function, the \n*\n operator\n    refers to the value of the field whose name follows the \n*\n operator,\n    on the current point.\n\n\nThe \n*\n operator can be used with any string (literal, const,\n    variable), which allows reference to a field name that would be\n    invalid as a bare identifier.\n\n\n\n\nTime Field\n\n\nThe field named \ntime\n is treated specially in the Juttle language as it is the field that is used by various processors to order points and group them into appropriate time intervals.\n\n\nThe time field must be a Moment (see \ndata types\n that represents the time at which the event occurred. Successive points in the same stream must have time values that are monotonically increasing.", 
            "title": "Fields"
        }, 
        {
            "location": "/concepts/fields/#fields", 
            "text": "Each Juttle data point consists of a set of named fields. The name of a field is always a string, but the value can be of various  types .   This needs to be updated to clarify stream context vs function context.", 
            "title": "Fields"
        }, 
        {
            "location": "/concepts/fields/#referencing", 
            "text": "Juttle includes two operators for field referencing -- the  *  operator and the  #  operator.  Consider this simple Juttle:  emit\n| put apple='fuji'\n| put fruit = apple\n| view text  The left-hand side (LHS) of this put expression is expressing \"I want to\nset the contents of the field called fruit to\u2026 .\" This is an example of\nwhat we call field referencing.  We could have written the above instead as:  emit\n| put apple = 'fuji'\n| put *'fruit' = *'apple'\n| view text  In this case, 'fruit' is actually a string literal, and the  *  operator\ntells Juttle to affect the contents of a field with the name shown in\nthat string. Similarly 'apple' is also a string literal and the  *  operator tells Juttle to read from the specified field.  Alternatively, we could have omitted the quotes and written it as:  emit\n| put apple = 'fuji'\n| put #fruit = #apple\n| view text  The  #  operator functions similarly to the  *  operator except that what follows is an identifier and not a quoted string.  However, Juttle recognizes that the LHS of a put expression\nalways becomes a field reference, and so a bare identifier such as  fruit \nthat doesn't match a const or var that is in scope is assumed to be a field\ndereference and is always treated the same as if we'd typed the explicit  *'fruit'  or  #fruit.  The use of explicit field referencing operators becomes apparent when there is a const or var with the same name as a field in scope:  const apple='gala';\nemit -limit 2\n| put apple='fuji'\n| (\n    put fruit='apple' | view text;     // fruit will be set to 'apple'\n    put fruit=apple | view text;       // fruit will be set to 'gala'\n    put fruit=*'apple' | view text;    // fruit will be set to 'fuji'\n    put fruit=#apple | view text;      // fruit will be set to 'fuji'\n  )  The rules above apply to the LHS of put , reduce ,\nand filter \nexpressions and the RHS of put and filter expressions.  There can be other cases where explicit field referencing is\nnecessary. The first is within subgraphs \nthat take field names as parameters. In this case, what's being passed\nin as a parameter is just a string that is the name of the field, so the  *  operator allows access to the field of that name.  sub cp(to, from) { put *to = *from }\nemit | put apple = 'fuji' | cp -to  fruit  -from  apple  | view text  Another place is when the \"update\" function of a reducer \nneeds to access the contents of a field.  A handy side-effect of the explicit field referencing is that\nit can allow us to work with fields whose names would not be legal as\nbare identifiers.  emit\n| put *':) my favorite!' = 'honeycrisp'\n| view text  To summarize, this is how field referencing works in Juttle:   On the LHS of put, reduce, and filter expressions, a bare identifier\n    x is treated the same as the explicit  *'x'  or #x unless a variable  x  is in scope.  On the RHS of put and filter expressions, a bare identifier x is\n    treated the same as the explicit  *'x' , unless a const/variable\n    named x is in scope, in which case the contents of const/variable x\n    is used.  Inside a subgraph, or reducer \"update\" function, the  *  operator\n    refers to the value of the field whose name follows the  *  operator,\n    on the current point.  The  *  operator can be used with any string (literal, const,\n    variable), which allows reference to a field name that would be\n    invalid as a bare identifier.", 
            "title": "Referencing"
        }, 
        {
            "location": "/concepts/fields/#time-field", 
            "text": "The field named  time  is treated specially in the Juttle language as it is the field that is used by various processors to order points and group them into appropriate time intervals.  The time field must be a Moment (see  data types  that represents the time at which the event occurred. Successive points in the same stream must have time values that are monotonically increasing.", 
            "title": "Time Field"
        }, 
        {
            "location": "/concepts/filtering/", 
            "text": "Filtering\n\n\nIn order to pare down a dataset, Juttle supports various ways of filtering the data.\n\n\n\n\n\n\nFiltering\n\n\nHow to Filter\n\n\nField comparisons\n\n\nFilter Expressions\n\n\n\n\n\n\nFull-text search\n\n\nQuoted terms match exact phrases only\n\n\nTerms analysis\n\n\nSearch is not available in filter\n\n\n\n\n\n\n\n\n\n\nHow to Filter\n\n\nThere are two basic forms of filtering in Juttle.\n\n\nAt any point in the flowgraph, you can can use the \nfilter\n processor to restrict the points that will flow to those that match the given predicate. For example:\n\n\nemit -from :0: -limit 10\n| put i=count(), even = (i % 2 == 0)\n| filter even == true\n| view table\n\n\n\n\nWill output:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 even     \u2502 i        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:01.000Z           \u2502 true     \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:03.000Z           \u2502 true     \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:05.000Z           \u2502 true     \u2502 6        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:07.000Z           \u2502 true     \u2502 8        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:09.000Z           \u2502 true     \u2502 10       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn addition, most \nadapters\n take a filter expression options that are given as part of the invocation of \nread\n and turn that into a corresponding query (or queries) to the backend.\n\n\nFor example, assuming you have configured an elasticsearch adapter, then the following juttle will translate the juttle query to search for all documents with a timestamp within the last hour and a message field containing the string \"error\" and an app field that contains \"syslog\":\n\n\nread elastic -from :1 hour ago: -to :now: message~\n*error*\n app=\nsyslog\n\n\n\n\n\nThis could have instead been written as:\n\n\nread elastic -from :1 hour ago: -to :now: | filter message~\n*error*\n app=\nsyslog\n\n\n\n\n\nWhile these would produce the same results, in the latter case the adapter would pull all of the documents for the last hour out of elasticsearch and into the Juttle runtime where they would be filtered, unlike the former example which sends the query to elasticsearch for execution.\n\n\nField comparisons\n\n\nThe basic form of field comparisons is:\n\n\nfield operator expression\n\n\n\n\nor\n\n\nexpression operator field\n\n\n\n\nDepending on the context, you may either be able to reference fields by name or you may use the \nfield reference operators\n.\n\n\nThe valid comparison operators include:\n\n\n\n\n\n\n\n\nOperator\n\n\nDescription\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n=, ==\n\n\nMatches exactly\n\n\nhostname = \"server1\"\n\n\n\n\n\n\n!=\n\n\nDoes not match\n\n\nhostname != \"server-\" + server_id\n\n\n\n\n\n\n, \n=, \n, \n=\n\n\nIs less than, is less than or equal to, is greater than, is greater than or equal to\n\n\ncpu \n= 1 + Math.max(4*20, 79)\n \ncpu \n max_cpu - 10\n\n\n\n\n\n\n~, =~\n\n\nWildcard operator for matching with \"glob\" or regular expressions\n\n\nTrue if the value of the \"hostname\" field is \"server\" followed by any number of characters: \nhostname ~ \"server*\"\nTrue if the value of the \"hostname\" field contains alphanumeric characters: \nhostname ~ /[A-Za-z0-9]*/\n\n\n\n\n\n\n!~\n\n\nWildcard negation operator\n\n\nTrue if the value of the \"hostname\" field does NOT begin with \"server\": \nhostname !~ \"server*\"\n\n\n\n\n\n\nin\n\n\nCheck for inclusion in an array\n\n\nTrue if the value of \"hostname\" field is one of \"host1\", \"host2\", or the value of the \"server\" field: \nhostname in [\"host1\", host2\", server]\n\n\n\n\n\n\n\n\nSee \noperators reference\n for more information.\n\n\nFilter Expressions\n\n\nField comparisons can be combined using the boolean operators \nAND\n, \nOR\n, and \nNOT\n, and can be nested using parentheses. Note that \nAND\n is implicitly added between two field comparison statements.\n\n\nFor example the following will read all points using a hypothetical adapter called \nemail\n containing the subject \"hello\", where the spam rating is either 0 or 1 and the sender is not \"self\":\n\n\nread email subject~\n*hello*\n (spam=0 OR spam=1) AND NOT sender=\nself\n\n\n\n\n\nFull-text search\n\n\nJuttle supports backend storage systems such as elasticsearch that implement\nfull-text search across all fields in a document through lexical analysis.\nFull-text searches match any point in which the string returned by expression is\npresent in any field.\n\n\nThe \nfilter\n processor does not support full-text search -- it can only be used as part of a read from an external backend that supports search.\n\n\nThe search terms for full-text search are currently expressed as standalone strings in the filter expression for \nread\n. Search terms and other filter expressions can be combined with the AND, OR, NOT operators.\n\n\nA full-text search is currently expressed in a filter statement as a standalone string.\n\n\n The syntax for full-text search will be changed soon to add a \n?\n operator.\n\n\nFor example the following searches all documents in the last day for the term \"alarm\":\n\n\nread elastic -last :1 day: \nalarm\n\n\n\n\n\nAnd the following searches all documents in the last day containing the term \"alarm\" and where the \nenv\n field is not equal to \"test\".\n\n\nread elastic -last :1 day: 'alarm' AND NOT env = 'test'\n\n\n\n\nQuoted terms match exact phrases only\n\n\nFor example, the following matches points in which one or more fields contain the \nexact phrase\n \"alarm failed\":\n\n\nread elastic -last :1 day: \nalarm failed\n\n\n\n\n\nIt does not match points in which one field contains \"alarm\" and\nanother field contains \"failed\", nor does it match points in which a\nfield contains \"alarm has failed\". To match those points, use this\ninstead:\n\n\nread elastic -last :1 day: \nalarm\n \nfailed\n\n\n\n\n\nTerms analysis\n\n\nThere are many different ways in which a backend storage system may map an incoming document into terms that are available for full-text search.\n\n\nFrom the standpoint of Juttle, the terms are passed through to the back end and the specific matching is implemented there.\n\n\nSearch is not available in filter\n\n\nThe filter processor does not implement full-text search. It is only available when interacting with a suitable backend.", 
            "title": "Filtering"
        }, 
        {
            "location": "/concepts/filtering/#filtering", 
            "text": "In order to pare down a dataset, Juttle supports various ways of filtering the data.    Filtering  How to Filter  Field comparisons  Filter Expressions    Full-text search  Quoted terms match exact phrases only  Terms analysis  Search is not available in filter", 
            "title": "Filtering"
        }, 
        {
            "location": "/concepts/filtering/#how-to-filter", 
            "text": "There are two basic forms of filtering in Juttle.  At any point in the flowgraph, you can can use the  filter  processor to restrict the points that will flow to those that match the given predicate. For example:  emit -from :0: -limit 10\n| put i=count(), even = (i % 2 == 0)\n| filter even == true\n| view table  Will output:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 even     \u2502 i        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:01.000Z           \u2502 true     \u2502 2        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:03.000Z           \u2502 true     \u2502 4        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:05.000Z           \u2502 true     \u2502 6        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:07.000Z           \u2502 true     \u2502 8        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1970-01-01T00:00:09.000Z           \u2502 true     \u2502 10       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In addition, most  adapters  take a filter expression options that are given as part of the invocation of  read  and turn that into a corresponding query (or queries) to the backend.  For example, assuming you have configured an elasticsearch adapter, then the following juttle will translate the juttle query to search for all documents with a timestamp within the last hour and a message field containing the string \"error\" and an app field that contains \"syslog\":  read elastic -from :1 hour ago: -to :now: message~ *error*  app= syslog   This could have instead been written as:  read elastic -from :1 hour ago: -to :now: | filter message~ *error*  app= syslog   While these would produce the same results, in the latter case the adapter would pull all of the documents for the last hour out of elasticsearch and into the Juttle runtime where they would be filtered, unlike the former example which sends the query to elasticsearch for execution.", 
            "title": "How to Filter"
        }, 
        {
            "location": "/concepts/filtering/#field-comparisons", 
            "text": "The basic form of field comparisons is:  field operator expression  or  expression operator field  Depending on the context, you may either be able to reference fields by name or you may use the  field reference operators .  The valid comparison operators include:     Operator  Description  Examples      =, ==  Matches exactly  hostname = \"server1\"    !=  Does not match  hostname != \"server-\" + server_id    ,  =,  ,  =  Is less than, is less than or equal to, is greater than, is greater than or equal to  cpu  = 1 + Math.max(4*20, 79)   cpu   max_cpu - 10    ~, =~  Wildcard operator for matching with \"glob\" or regular expressions  True if the value of the \"hostname\" field is \"server\" followed by any number of characters:  hostname ~ \"server*\" True if the value of the \"hostname\" field contains alphanumeric characters:  hostname ~ /[A-Za-z0-9]*/    !~  Wildcard negation operator  True if the value of the \"hostname\" field does NOT begin with \"server\":  hostname !~ \"server*\"    in  Check for inclusion in an array  True if the value of \"hostname\" field is one of \"host1\", \"host2\", or the value of the \"server\" field:  hostname in [\"host1\", host2\", server]     See  operators reference  for more information.", 
            "title": "Field comparisons"
        }, 
        {
            "location": "/concepts/filtering/#filter-expressions", 
            "text": "Field comparisons can be combined using the boolean operators  AND ,  OR , and  NOT , and can be nested using parentheses. Note that  AND  is implicitly added between two field comparison statements.  For example the following will read all points using a hypothetical adapter called  email  containing the subject \"hello\", where the spam rating is either 0 or 1 and the sender is not \"self\":  read email subject~ *hello*  (spam=0 OR spam=1) AND NOT sender= self", 
            "title": "Filter Expressions"
        }, 
        {
            "location": "/concepts/filtering/#full-text-search", 
            "text": "Juttle supports backend storage systems such as elasticsearch that implement\nfull-text search across all fields in a document through lexical analysis.\nFull-text searches match any point in which the string returned by expression is\npresent in any field.  The  filter  processor does not support full-text search -- it can only be used as part of a read from an external backend that supports search.  The search terms for full-text search are currently expressed as standalone strings in the filter expression for  read . Search terms and other filter expressions can be combined with the AND, OR, NOT operators.  A full-text search is currently expressed in a filter statement as a standalone string.   The syntax for full-text search will be changed soon to add a  ?  operator.  For example the following searches all documents in the last day for the term \"alarm\":  read elastic -last :1 day:  alarm   And the following searches all documents in the last day containing the term \"alarm\" and where the  env  field is not equal to \"test\".  read elastic -last :1 day: 'alarm' AND NOT env = 'test'", 
            "title": "Full-text search"
        }, 
        {
            "location": "/concepts/filtering/#quoted-terms-match-exact-phrases-only", 
            "text": "For example, the following matches points in which one or more fields contain the  exact phrase  \"alarm failed\":  read elastic -last :1 day:  alarm failed   It does not match points in which one field contains \"alarm\" and\nanother field contains \"failed\", nor does it match points in which a\nfield contains \"alarm has failed\". To match those points, use this\ninstead:  read elastic -last :1 day:  alarm   failed", 
            "title": "Quoted terms match exact phrases only"
        }, 
        {
            "location": "/concepts/filtering/#terms-analysis", 
            "text": "There are many different ways in which a backend storage system may map an incoming document into terms that are available for full-text search.  From the standpoint of Juttle, the terms are passed through to the back end and the specific matching is implemented there.", 
            "title": "Terms analysis"
        }, 
        {
            "location": "/concepts/filtering/#search-is-not-available-in-filter", 
            "text": "The filter processor does not implement full-text search. It is only available when interacting with a suitable backend.", 
            "title": "Search is not available in filter"
        }, 
        {
            "location": "/concepts/inputs/", 
            "text": "Inputs\n\n\nInput controls enable Juttle programs to be parameterized by user interface elements such as text inputs or dropdowns.\n\n\nThe syntax for inputs is:\n\n\ninput \nname\n: \ntype\n \noptions\n\n\n\n\n\nThe \nname\n can be any valid identifier and is treated in the program like a  \nconstant\n. This way the value can be used as an option to a proc, an element in a filter expression, a view parameter, or more.\n\n\nInputs can be declared in a Juttle program either at the top level of a flowgraph or inside a \nsubgraph\n.\n\n\nBefore the program is run, an application environment like \noutrigger\n first evaluates the flowgraph to determine which inputs are in the program and renders them for a user. Then once the user has made their selection, the application gathers the selections and includes them when running the program.\n\n\nNote: The specific types of supported inputs and the supported options are supplied from the application environment and are not part of Juttle itself. The Juttle CLI has limited support for text and number inputs using command line options.\n\n\nExample: Parameterizing a simple program with inputs using the CLI\n\n\nA simple example of a program that accepts two inputs -- one to control the number of data points and one to control the message that is printed:\n\n\ninput num: number -default 5 -label \nNumber of Points\n;\ninput text: text -default \nHello World\n -label \nMessage\n;\n\nemit -limit num | put message=text\n\n\n\n\nBy default this will emit 5 points with the message \"Hello World\".\n\n\nHowever both the number of points and the message are configurable, so the program can be run with other inputs to control the number of points and the message.\n\n\n This document needs to be completed.", 
            "title": "Inputs"
        }, 
        {
            "location": "/concepts/inputs/#inputs", 
            "text": "Input controls enable Juttle programs to be parameterized by user interface elements such as text inputs or dropdowns.  The syntax for inputs is:  input  name :  type   options   The  name  can be any valid identifier and is treated in the program like a   constant . This way the value can be used as an option to a proc, an element in a filter expression, a view parameter, or more.  Inputs can be declared in a Juttle program either at the top level of a flowgraph or inside a  subgraph .  Before the program is run, an application environment like  outrigger  first evaluates the flowgraph to determine which inputs are in the program and renders them for a user. Then once the user has made their selection, the application gathers the selections and includes them when running the program.  Note: The specific types of supported inputs and the supported options are supplied from the application environment and are not part of Juttle itself. The Juttle CLI has limited support for text and number inputs using command line options.  Example: Parameterizing a simple program with inputs using the CLI  A simple example of a program that accepts two inputs -- one to control the number of data points and one to control the message that is printed:  input num: number -default 5 -label  Number of Points ;\ninput text: text -default  Hello World  -label  Message ;\n\nemit -limit num | put message=text  By default this will emit 5 points with the message \"Hello World\".  However both the number of points and the message are configurable, so the program can be run with other inputs to control the number of points and the message.   This document needs to be completed.", 
            "title": "Inputs"
        }, 
        {
            "location": "/concepts/views/", 
            "text": "Views\n\n\n This document needs to be written to describe how the language interacts with the client-side visualization library", 
            "title": "Views"
        }, 
        {
            "location": "/concepts/views/#views", 
            "text": "This document needs to be written to describe how the language interacts with the client-side visualization library", 
            "title": "Views"
        }, 
        {
            "location": "/reference/data_types/", 
            "text": "Data types\n\n\nHere are the data types supported in Juttle data points.\n\n\n\n\n\n\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNull\n\n\nNon-existing or unknown value\n\n\n\n\n\n\nBoolean\n\n\nA logical truth value, either \"true\" or \"false\"\n\n\n\n\n\n\nNumber\n\n\nA number in IEEE 754 64-bit double-precision format\n\n\n\n\n\n\nString\n\n\nZero or more Unicode characters\n\n\n\n\n\n\nRegExp\n\n\nA regular expression, \nas implemented in Javascript\n\n\n\n\n\n\nDate\n\n\nAn exact moment in time, represented by a number\n\n\n\n\n\n\nDuration\n\n\nAn interval between to moments in time, represented by a numerical length\n\n\n\n\n\n\nArray\n\n\nAn ordered sequence of zero or more values\n\n\n\n\n\n\nObject\n\n\nAn unordered collection of zero or more properties, each consisting of a key and a value\n\n\n\n\n\n\n\n\nAll data types supported by Juttle data points are equivalent to their\ncorresponding \nJavaScript types\n,\nexcept Date and Duration which are specific to Juttle.\n\n\nYou can also convert a field to or from type string, using these data\ntyping functions:\n\n\n\n\nBoolean.toString()\n\n\nDate.toString()\n\n\nDuration.toString()\n\n\nNumber.fromString()\n\n\nNumber.toString()\n\n\nRegExp.toString()", 
            "title": "____ Data Types"
        }, 
        {
            "location": "/reference/data_types/#data-types", 
            "text": "Here are the data types supported in Juttle data points.     Type  Description      Null  Non-existing or unknown value    Boolean  A logical truth value, either \"true\" or \"false\"    Number  A number in IEEE 754 64-bit double-precision format    String  Zero or more Unicode characters    RegExp  A regular expression,  as implemented in Javascript    Date  An exact moment in time, represented by a number    Duration  An interval between to moments in time, represented by a numerical length    Array  An ordered sequence of zero or more values    Object  An unordered collection of zero or more properties, each consisting of a key and a value     All data types supported by Juttle data points are equivalent to their\ncorresponding  JavaScript types ,\nexcept Date and Duration which are specific to Juttle.  You can also convert a field to or from type string, using these data\ntyping functions:   Boolean.toString()  Date.toString()  Duration.toString()  Number.fromString()  Number.toString()  RegExp.toString()", 
            "title": "Data types"
        }, 
        {
            "location": "/concepts/juttle_tutorial/", 
            "text": "Juttle Tutorial\n\n\n XXX/dmehra this is unfinished\n\n\nIf you're here, you're probably just starting out with Juttle.\n\n\nJuttle is a high-level language that is used to process and visualize\nstreams of data. At its core lies a simple dataflow language which is\nused to declare and define processing flowgraphs, by stitching together\nindividual processing steps into a directed graph.\n\n\nWe'll introduce you to the key concepts in Juttle, starting off with\nsimple synthetic data, and then graduating to a real dataset coming from\nGitHub.\n\n\nAfter completing these tutorials, you will know enough Juttle to start\nexploring and analyzing your own data. For more detailed Juttle\ndocumentation than this, see \nJuttle concepts\n. \n\n\nIn Juttle, \nthe basic unit of data is a point\n. A point consists of a\nnumber of key/value pairs, where the keys are strings and the values are\nnumbers, strings, Booleans, or null. Points flow through flowgraphs and\ncan be transformed, aggregated, or joined at each processing step.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/concepts/juttle_tutorial/#juttle-tutorial", 
            "text": "XXX/dmehra this is unfinished  If you're here, you're probably just starting out with Juttle.  Juttle is a high-level language that is used to process and visualize\nstreams of data. At its core lies a simple dataflow language which is\nused to declare and define processing flowgraphs, by stitching together\nindividual processing steps into a directed graph.  We'll introduce you to the key concepts in Juttle, starting off with\nsimple synthetic data, and then graduating to a real dataset coming from\nGitHub.  After completing these tutorials, you will know enough Juttle to start\nexploring and analyzing your own data. For more detailed Juttle\ndocumentation than this, see  Juttle concepts .   In Juttle,  the basic unit of data is a point . A point consists of a\nnumber of key/value pairs, where the keys are strings and the values are\nnumbers, strings, Booleans, or null. Points flow through flowgraphs and\ncan be transformed, aggregated, or joined at each processing step.", 
            "title": "Juttle Tutorial"
        }, 
        {
            "location": "/concepts/terminology/", 
            "text": "Terminology\n\n\nbatch\n\n\nA sub-sequence of points in a stream, such as delineated by the \nbatch\n processor.\n\nReducers\n and certain \nprocessor nodes\n are unique in their ability to work with batches of points.\n\n\nflowgraph\n\n\nAn arrangement of one or more source nodes, one or more sink nodes, and zero or more processor nodes, chained together to operate on one or more streams of points. Juttle flowgraphs are different from pipelines in other languages, in that they may be forked and merged.\n\n\nfunction\n\n\nMuch like a function in other programming languages, a Juttle function performs a calculation based on provided inputs and returns a single result. Juttle comes with some built-in functions, and you can also define your own.\n\n\njob\n\n\nA running instance of a Juttle program.\n\n\nmodule\n\n\nA collection of functionality that can be shared among multiple Juttle programs. A module can export constants, shared variables, subgraphs, reducers, and functions, making them available for import by other programs. See \nmodules\n. \n\n\nnode\n\n\nA primitive element in a Juttle flowgraph. Types of nodes include sources, sinks, and processors.\n\n\nout-of-order points\n\n\nData points that arrive out of sequence with respect to their time stamps.\n\n\npoint\n\n\nThe primitive unit of data that travels through a Juttle flowgraph, made up of arbitrary name-value properties.\n\n\nprocessor\n\n\nA processor, or processor node, resides in the middle of a Juttle flowgraph, modifying streams as they pass through. Example modifications include transforming, reducing, and filtering.\n\n\nprogram\n\n\nJuttle code that includes at least one flowgraph as well as any additional supporting code.\n\n\nreducer\n\n\nJuttle reducers operate on batches of points, carrying out a running computation over values contained in the points. Several reducers come built-in to Juttle, and users can also define their own.\n\n\nsource\n\n\nSources, or source nodes, appear at the beginning of a flowgraph to stream data points synthetically or from your own data. Sources can stream live data in real time or historical data all at once.\n\n\nstraggler\n\n\nA data point that arrives after an excessive interval. For example, if point A and point B have time stamps one second apart, but point B arrives ten seconds after point A, then point B is a straggler.\n\n\nsubgraph\n\n\nA reusable, user-defined element consisting of an arbitrary arrangement of nodes and other supporting code.\n\n\nview\n\n\nA sink node that displays data visually, such as in a table or a chart.", 
            "title": "____ Juttle Terminology"
        }, 
        {
            "location": "/concepts/terminology/#terminology", 
            "text": "", 
            "title": "Terminology"
        }, 
        {
            "location": "/concepts/terminology/#batch", 
            "text": "A sub-sequence of points in a stream, such as delineated by the  batch  processor. Reducers  and certain  processor nodes  are unique in their ability to work with batches of points.", 
            "title": "batch"
        }, 
        {
            "location": "/concepts/terminology/#flowgraph", 
            "text": "An arrangement of one or more source nodes, one or more sink nodes, and zero or more processor nodes, chained together to operate on one or more streams of points. Juttle flowgraphs are different from pipelines in other languages, in that they may be forked and merged.", 
            "title": "flowgraph"
        }, 
        {
            "location": "/concepts/terminology/#function", 
            "text": "Much like a function in other programming languages, a Juttle function performs a calculation based on provided inputs and returns a single result. Juttle comes with some built-in functions, and you can also define your own.", 
            "title": "function"
        }, 
        {
            "location": "/concepts/terminology/#job", 
            "text": "A running instance of a Juttle program.", 
            "title": "job"
        }, 
        {
            "location": "/concepts/terminology/#module", 
            "text": "A collection of functionality that can be shared among multiple Juttle programs. A module can export constants, shared variables, subgraphs, reducers, and functions, making them available for import by other programs. See  modules .", 
            "title": "module"
        }, 
        {
            "location": "/concepts/terminology/#node", 
            "text": "A primitive element in a Juttle flowgraph. Types of nodes include sources, sinks, and processors.", 
            "title": "node"
        }, 
        {
            "location": "/concepts/terminology/#out-of-order-points", 
            "text": "Data points that arrive out of sequence with respect to their time stamps.", 
            "title": "out-of-order points"
        }, 
        {
            "location": "/concepts/terminology/#point", 
            "text": "The primitive unit of data that travels through a Juttle flowgraph, made up of arbitrary name-value properties.", 
            "title": "point"
        }, 
        {
            "location": "/concepts/terminology/#processor", 
            "text": "A processor, or processor node, resides in the middle of a Juttle flowgraph, modifying streams as they pass through. Example modifications include transforming, reducing, and filtering.", 
            "title": "processor"
        }, 
        {
            "location": "/concepts/terminology/#program", 
            "text": "Juttle code that includes at least one flowgraph as well as any additional supporting code.", 
            "title": "program"
        }, 
        {
            "location": "/concepts/terminology/#reducer", 
            "text": "Juttle reducers operate on batches of points, carrying out a running computation over values contained in the points. Several reducers come built-in to Juttle, and users can also define their own.", 
            "title": "reducer"
        }, 
        {
            "location": "/concepts/terminology/#source", 
            "text": "Sources, or source nodes, appear at the beginning of a flowgraph to stream data points synthetically or from your own data. Sources can stream live data in real time or historical data all at once.", 
            "title": "source"
        }, 
        {
            "location": "/concepts/terminology/#straggler", 
            "text": "A data point that arrives after an excessive interval. For example, if point A and point B have time stamps one second apart, but point B arrives ten seconds after point A, then point B is a straggler.", 
            "title": "straggler"
        }, 
        {
            "location": "/concepts/terminology/#subgraph", 
            "text": "A reusable, user-defined element consisting of an arbitrary arrangement of nodes and other supporting code.", 
            "title": "subgraph"
        }, 
        {
            "location": "/concepts/terminology/#view", 
            "text": "A sink node that displays data visually, such as in a table or a chart.", 
            "title": "view"
        }, 
        {
            "location": "/concepts/juttle_standard_library/", 
            "text": "Juttle standard library\n\n\n The Juttle standard library is under development.\n\n\nJut provides some useful modules in a \nstandard library\n. Explore the\nlibrary to find modules you can use to enhance your Juttle programs.\n\n\nEach program in the standard library contains multiple modules that you can \nimport\n into your own programs. The developers at Jut have provided comments inside\neach program to explain each module's purpose and usage.\n\n\nWe're adding new modules all the time, so check the standard library for\nthe latest additions.", 
            "title": "____ Standard Library"
        }, 
        {
            "location": "/concepts/juttle_standard_library/#juttle-standard-library", 
            "text": "The Juttle standard library is under development.  Jut provides some useful modules in a  standard library . Explore the\nlibrary to find modules you can use to enhance your Juttle programs.  Each program in the standard library contains multiple modules that you can  import  into your own programs. The developers at Jut have provided comments inside\neach program to explain each module's purpose and usage.  We're adding new modules all the time, so check the standard library for\nthe latest additions.", 
            "title": "Juttle standard library"
        }, 
        {
            "location": "/sources/", 
            "text": "Sources\n\n\nSource procs appear at the beginning of a flowgraph to feed data points into it. \n\n\n(source; source) | processor | (processor; processor) | processor | (sink; sink)\n\n\n\n\nemit\n\n\nUse \nemit\n source to synthetically generate points that contain only a timestamp, to compose test/example Juttle programs.\n\n\nread\n\n\nRead data via adapters such as the built-in \nread file\n, \nread stochastic\n, \nread http\n, or plug in external \nadapters\n to read from remote backends.", 
            "title": "Sources"
        }, 
        {
            "location": "/sources/#sources", 
            "text": "Source procs appear at the beginning of a flowgraph to feed data points into it.   (source; source) | processor | (processor; processor) | processor | (sink; sink)  emit  Use  emit  source to synthetically generate points that contain only a timestamp, to compose test/example Juttle programs.  read  Read data via adapters such as the built-in  read file ,  read stochastic ,  read http , or plug in external  adapters  to read from remote backends.", 
            "title": "Sources"
        }, 
        {
            "location": "/sources/emit/", 
            "text": "emit\n\n\nSynthetically generate live or\nhistorical points containing only a time stamp. This simple source is\nlike a blank canvas for building synthetic points; you can customize\npoints by adding fields using\n\nput\n.\n\n\nemit -every :duration: -from moment -to moment -limit limit -points point-array\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-every\n\n\nThe interval at which to emit points, specified as a :duration: \nSee \nTime notation in Juttle\n for syntax information. \n\n\nNo\n\n\n\n\n\n\n-from\n\n\nStream points whose time stamps are greater than or equal to the specified moment \nSee \nTime notation in Juttle\n for syntax information.\n\n\nNo; defaults to :now:\n\n\n\n\n\n\n-to\n\n\nStream points whose time stamps are less than the specified moment, which is less than or equal to :now: \nSee \nTime notation in Juttle\n for syntax information.\n\n\nNo; defaults to forever\n\n\n\n\n\n\n-limit\n\n\nTotal number of points to emit\n\n\nNo; defaults to 1.\n\n\n\n\n\n\n-points\n\n\nAn array of points to stream\n\n\nNo\n\n\n\n\n\n\n\n\nThe time field can be a moment expression, a quoted date/time string as generated by Juttle output, or a number of seconds since the Unix epoch. If the points have no time stamps, then a time field is added with values determined by the \n-from\n options, or the current time if those options are omitted.\n\n\nBy default, this source node emits one point per second starting at the\ncurrent time for a total of 100. Points with time in the past are\nemitted all at once, while points with time in the future are emitted in\nreal time.\n\n\nExample: Emit a point every second containing the current time, for a total of 100 points\n\n\nemit -limit 100\n| view text\n\n\n\n\nExample: Emit 50 points in one-second intervals starting one day ago\n\n\nemit -limit 50 -from :1 day ago:\n\n\n\n\nAll points are emitted at once, since their times are in the past. To\nemit them in real time, see\n\npace\n.\n\n\nExample: Replay points having a variety of timestamp formats text window\n\n\nconst points = [\n    {time: \n1970-01-01\n, n:1},\n    {time: 1, n:2},\n    {time: \n1970-01-01T00:00:02.000Z\n, n:3},\n    {time: Date.parse(\nThu Jan 1 01:00:03 1970 UTC\n), n:4},\n    {time: :1970-01-01T00:00:04:, n:5},\n    {time: :5:, n:6},\n];\nemit -points points", 
            "title": "____ emit"
        }, 
        {
            "location": "/sources/emit/#emit", 
            "text": "Synthetically generate live or\nhistorical points containing only a time stamp. This simple source is\nlike a blank canvas for building synthetic points; you can customize\npoints by adding fields using put .  emit -every :duration: -from moment -to moment -limit limit -points point-array     Parameter  Description  Required?      -every  The interval at which to emit points, specified as a :duration:  See  Time notation in Juttle  for syntax information.   No    -from  Stream points whose time stamps are greater than or equal to the specified moment  See  Time notation in Juttle  for syntax information.  No; defaults to :now:    -to  Stream points whose time stamps are less than the specified moment, which is less than or equal to :now:  See  Time notation in Juttle  for syntax information.  No; defaults to forever    -limit  Total number of points to emit  No; defaults to 1.    -points  An array of points to stream  No     The time field can be a moment expression, a quoted date/time string as generated by Juttle output, or a number of seconds since the Unix epoch. If the points have no time stamps, then a time field is added with values determined by the  -from  options, or the current time if those options are omitted.  By default, this source node emits one point per second starting at the\ncurrent time for a total of 100. Points with time in the past are\nemitted all at once, while points with time in the future are emitted in\nreal time.  Example: Emit a point every second containing the current time, for a total of 100 points  emit -limit 100\n| view text  Example: Emit 50 points in one-second intervals starting one day ago  emit -limit 50 -from :1 day ago:  All points are emitted at once, since their times are in the past. To\nemit them in real time, see pace .  Example: Replay points having a variety of timestamp formats text window  const points = [\n    {time:  1970-01-01 , n:1},\n    {time: 1, n:2},\n    {time:  1970-01-01T00:00:02.000Z , n:3},\n    {time: Date.parse( Thu Jan 1 01:00:03 1970 UTC ), n:4},\n    {time: :1970-01-01T00:00:04:, n:5},\n    {time: :5:, n:6},\n];\nemit -points points", 
            "title": "emit"
        }, 
        {
            "location": "/sources/read/", 
            "text": "read\n\n\nReads events or metrics from various remote backends via \nadapters supported by Juttle\n and emits them into the flowgraph. The read source honors standard parameters as well as adapter-specific options.\n\n\nread \nadapter\n -from moment -to moment [-last duration]\n  [filter-expression] [adapter-options]\n| ...\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nadapter\n\n\nName of the adapter to connect to the backend.\n\n\nYes\n\n\n\n\n\n\n-from\n\n\nStart time, inclusive: tream points whose time stamps are greater than or equal to the specified moment. \nSee \nTime notation in Juttle\n for syntax information.\n\n\nRequired only if -to is present; defaults to :now:\n\n\n\n\n\n\n-to\n\n\nEnd time, exclusive: stream points whose time stamps are less than the specified moment, which is less than or equal to :now: \nSee \nTime notation in Juttle\n for syntax information. \n:information_source: \nNote:\nTo stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.\n\n\nNo; defaults to forever\n\n\n\n\n\n\n-last\n\n\nAn alternative to -from moment -to :now:, this specifies a historical time window that ends at the present moment.\n\n\nNo\n\n\n\n\n\n\nfilter-expression\n\n\nA full-text search expression, a field comparison, or a combination of both, where multiple terms are joined with AND, OR, or NOT \nSee \nFiltering\n for additional details.\n\n\nNo\n\n\n\n\n\n\nadapter-options\n\n\nOptions specific to reading from the chosen backend, handled by the adapter.\n\n\nSee adapter documentation\n\n\n\n\n\n\n\n\nExample\n\n\nread influxdb -db \ntest\n -measurements \ncpu\n -from :1 hour ago: -to :now:\n| reduce max() by host", 
            "title": "____ read"
        }, 
        {
            "location": "/sources/read/#read", 
            "text": "Reads events or metrics from various remote backends via  adapters supported by Juttle  and emits them into the flowgraph. The read source honors standard parameters as well as adapter-specific options.  read  adapter  -from moment -to moment [-last duration]\n  [filter-expression] [adapter-options]\n| ...     Parameter  Description  Required?      adapter  Name of the adapter to connect to the backend.  Yes    -from  Start time, inclusive: tream points whose time stamps are greater than or equal to the specified moment.  See  Time notation in Juttle  for syntax information.  Required only if -to is present; defaults to :now:    -to  End time, exclusive: stream points whose time stamps are less than the specified moment, which is less than or equal to :now:  See  Time notation in Juttle  for syntax information.  :information_source:  Note: To stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.  No; defaults to forever    -last  An alternative to -from moment -to :now:, this specifies a historical time window that ends at the present moment.  No    filter-expression  A full-text search expression, a field comparison, or a combination of both, where multiple terms are joined with AND, OR, or NOT  See  Filtering  for additional details.  No    adapter-options  Options specific to reading from the chosen backend, handled by the adapter.  See adapter documentation     Example  read influxdb -db  test  -measurements  cpu  -from :1 hour ago: -to :now:\n| reduce max() by host", 
            "title": "read"
        }, 
        {
            "location": "/processors/", 
            "text": "Processors\n\n\nProcessor nodes reside in the middle of a Juttle flowgraph, modifying streams as they pass through.\n\n\n(source; source) | processor | (processor; processor) | processor | (sink; sink)\n\n\n\n\nExample modifications include transforming, reducing, and filtering.\n\n\nbatch\n\n\nCreate batches by segmenting a sequence of points ordered by time stamp,\neach segment spanning a specified interval of time.\n\n\nfilter\n\n\nOutput only the points for which the given filtering expression\nevaluates to true.\n\n\nhead\n\n\nOnly emit the first limit points from each batch.\n\n\njoin\n\n\nCreate new points from the points of multiple input streams.\n\n\nkeep\n\n\nPrune the input stream by removing all fields except the specified ones.\n\n\npace\n\n\nPlay back historic points in real time or at the specified pace.\n\n\nput\n\n\nSet the specified field of each point to the result of an expression.\n\n\nreduce\n\n\nAccumulate collections of points and aggregate them using reducers,\noptionally within a moving time window.\n\n\nremove\n\n\nPrune the input stream by removing the specified fields.\n\n\nsort\n\n\nSort points in order based on values of one or more specified fields.\n\n\nsplit\n\n\nSplit each incoming point, emitting one new point for each of the\nspecified fields in the original point.\n\n\ntail\n\n\nOnly emit the last limit points from each batch.\n\n\nunbatch\n\n\nTurn a batched stream into an unbatched stream.\n\n\nuniq\n\n\nCompare adjacent points and discard any duplicates.", 
            "title": "Processors"
        }, 
        {
            "location": "/processors/#processors", 
            "text": "Processor nodes reside in the middle of a Juttle flowgraph, modifying streams as they pass through.  (source; source) | processor | (processor; processor) | processor | (sink; sink)  Example modifications include transforming, reducing, and filtering.  batch  Create batches by segmenting a sequence of points ordered by time stamp,\neach segment spanning a specified interval of time.  filter  Output only the points for which the given filtering expression\nevaluates to true.  head  Only emit the first limit points from each batch.  join  Create new points from the points of multiple input streams.  keep  Prune the input stream by removing all fields except the specified ones.  pace  Play back historic points in real time or at the specified pace.  put  Set the specified field of each point to the result of an expression.  reduce  Accumulate collections of points and aggregate them using reducers,\noptionally within a moving time window.  remove  Prune the input stream by removing the specified fields.  sort  Sort points in order based on values of one or more specified fields.  split  Split each incoming point, emitting one new point for each of the\nspecified fields in the original point.  tail  Only emit the last limit points from each batch.  unbatch  Turn a batched stream into an unbatched stream.  uniq  Compare adjacent points and discard any duplicates.", 
            "title": "Processors"
        }, 
        {
            "location": "/processors/batch/", 
            "text": "batch\n\n\nCreate batches by segmenting a sequence of points ordered by time stamp,\neach segment spanning a specified interval of time.\n\n\nbatch\n  -every duration\n  -on duration-or-calendar-offset batch-interval\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-every\nor \nbatch-interval\n\n\nThe time interval for batches, specified as one of the following: \nn\nThe number of seconds in the time interval \n:duration:\nThe time interval expressed as a \nmoment literal\n\n\nYes\n\n\n\n\n\n\n-on\n\n\nA time alignment for the batches. It may be a duration or a calendar offset less than batch interval. For example, \n-every :hour: -on :00:30:00:\nbatches points over an hour on the half-hour, while \n-every :month: -on :day 10:\nbatches monthly starting on day 10 of the month. If the beginning or ending of your data does not align evenly with these times, the first and last batch will contain less than the specified interval.\n\n\nNo; if \n-on\nis not specified, output batches are aligned with the UNIX epoch. If \nbatch-interval\nequals one day, then batch boundaries are at midnight UTC.\n\n\n\n\n\n\n\n\nMany processors do their work over groups of points called batches. For\nexample, the \nsort\n processor orders everything within a batch and the \nreduce\n processor aggregates points within a batch.\n\n\nThe batch processor creates batches by segmenting a sequence of points\nordered by time stamp, each segment spanning batch-interval seconds of\ntime. It does not alter points or their travel in any way. Instead, it\nadds information to the sequence that segments it into disjoint groups\nof points.\n\n\nThe end time of one batch is the start time of the following batch.\nBatch boundaries are time values that exist independent of the points,\nand there may or may not be points having these values as their time\nstamps. When batching is in place, any points that share a given time\nstamp are guaranteed to lie within the same batch. A batch processor\ndownstream from an earlier batch replaces the earlier grouping with the\nnew one.\n\n\nExample: Call records, day by day\n\n\n// Call Record billing example:\n//\n// Call records arrive as a stream of points indicating duration in minutes.\n// Your phone bill is the total of these, charged at $.05/minute, from the\n// 20th of each month.\n//\n// This program displays a day-by-day running total of your bill:\n//\nsub call_record() {\n  emit -from :2014-01-01: -limit 4000 -every :h:\n  | put name = 'duration'\n  | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\ncall_record\n| batch \n    -every :month:  \n    -on :day 20: \n| put name = 'total', value = sum(value) * 0.05\n| view timechart\n;\n//\n// This program displays a table with monthly totals\n//\ncall_record\n| reduce \n    -every :month:\n    -on :day 20: value = sum(value) * 0.05\n| put name = \ntotal\n, value = Math.floor(value * 100) / 100 \n| view table \n\n\n\n\n\nExample: create one-second interval batches\n \n\n\nIn this example, \nemit\n\nsends 20 points at a rate of 5 points per second. The points are then\ndivided into batches at one-second intervals. The \ntail\n\nprocessor outputs the last point from each batch, resulting in 4 output\npoints. Without the batch processor, tail would handle all 20 points at\nonce, resulting in a single output point.\n\n\nemit -from :0: -hz 5 -limit 20 \n| batch 1 \n| tail 1 \n| view text\n\n\n\n\n\nExample: batching live data\n\n\nIn contrast with the earlier example that used historical mode to emit data with past timestamps,\nall at once, this example generates data in real time, divides into batches, and uses the\n\ntail\n processor to output the last 2 points from each batch.\n\n\nemit -hz 10 -limit 100 \n| batch 1\n| tail 2 \n| view text", 
            "title": "____ batch"
        }, 
        {
            "location": "/processors/batch/#batch", 
            "text": "Create batches by segmenting a sequence of points ordered by time stamp,\neach segment spanning a specified interval of time.  batch\n  -every duration\n  -on duration-or-calendar-offset batch-interval     Parameter  Description  Required?      -every or  batch-interval  The time interval for batches, specified as one of the following:  n The number of seconds in the time interval  :duration: The time interval expressed as a  moment literal  Yes    -on  A time alignment for the batches. It may be a duration or a calendar offset less than batch interval. For example,  -every :hour: -on :00:30:00: batches points over an hour on the half-hour, while  -every :month: -on :day 10: batches monthly starting on day 10 of the month. If the beginning or ending of your data does not align evenly with these times, the first and last batch will contain less than the specified interval.  No; if  -on is not specified, output batches are aligned with the UNIX epoch. If  batch-interval equals one day, then batch boundaries are at midnight UTC.     Many processors do their work over groups of points called batches. For\nexample, the  sort  processor orders everything within a batch and the  reduce  processor aggregates points within a batch.  The batch processor creates batches by segmenting a sequence of points\nordered by time stamp, each segment spanning batch-interval seconds of\ntime. It does not alter points or their travel in any way. Instead, it\nadds information to the sequence that segments it into disjoint groups\nof points.  The end time of one batch is the start time of the following batch.\nBatch boundaries are time values that exist independent of the points,\nand there may or may not be points having these values as their time\nstamps. When batching is in place, any points that share a given time\nstamp are guaranteed to lie within the same batch. A batch processor\ndownstream from an earlier batch replaces the earlier grouping with the\nnew one.  Example: Call records, day by day  // Call Record billing example:\n//\n// Call records arrive as a stream of points indicating duration in minutes.\n// Your phone bill is the total of these, charged at $.05/minute, from the\n// 20th of each month.\n//\n// This program displays a day-by-day running total of your bill:\n//\nsub call_record() {\n  emit -from :2014-01-01: -limit 4000 -every :h:\n  | put name = 'duration'\n  | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\ncall_record\n| batch \n    -every :month:  \n    -on :day 20: \n| put name = 'total', value = sum(value) * 0.05\n| view timechart\n;\n//\n// This program displays a table with monthly totals\n//\ncall_record\n| reduce \n    -every :month:\n    -on :day 20: value = sum(value) * 0.05\n| put name =  total , value = Math.floor(value * 100) / 100 \n| view table   Example: create one-second interval batches    In this example,  emit \nsends 20 points at a rate of 5 points per second. The points are then\ndivided into batches at one-second intervals. The  tail \nprocessor outputs the last point from each batch, resulting in 4 output\npoints. Without the batch processor, tail would handle all 20 points at\nonce, resulting in a single output point.  emit -from :0: -hz 5 -limit 20 \n| batch 1 \n| tail 1 \n| view text  Example: batching live data  In contrast with the earlier example that used historical mode to emit data with past timestamps,\nall at once, this example generates data in real time, divides into batches, and uses the tail  processor to output the last 2 points from each batch.  emit -hz 10 -limit 100 \n| batch 1\n| tail 2 \n| view text", 
            "title": "batch"
        }, 
        {
            "location": "/processors/filter/", 
            "text": "filter\n\n\nOutput only the points for which the given filtering expression evaluates to true.\n\n\nfilter filter-expression\n\n\n\n\nThe filter expression can match fields using various \noperators\n.\n\n\nSee \nFiltering\n for more information about valid expressions to filter points. Also see \nField referencing\n for more information about how to refer to fields in filters.\n\n\nExample: display incoming points\n\n\nThis example displays all incoming points where the value for requests\nis greater than 90 and host field is not server, servers, dbserver,\ndbservers, webserver, or webservers. It uses metric data similar to \n\ndataset 3\n.\n\n\nread stochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'\n| filter name = \nrequests\n value \n 10 host !~ /^(web|db)?servers?$/ \n| view table -limit 20\n\n\n\n\nExample: Display only points containing a field named code\n\n\nThis example also uses metric data similar to \ndataset 3\n.\n\n\nread stochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'\n| filter code != null\n| view table -limit 20", 
            "title": "____ filter"
        }, 
        {
            "location": "/processors/filter/#filter", 
            "text": "Output only the points for which the given filtering expression evaluates to true.  filter filter-expression  The filter expression can match fields using various  operators .  See  Filtering  for more information about valid expressions to filter points. Also see  Field referencing  for more information about how to refer to fields in filters.  Example: display incoming points  This example displays all incoming points where the value for requests\nis greater than 90 and host field is not server, servers, dbserver,\ndbservers, webserver, or webservers. It uses metric data similar to  dataset 3 .  read stochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'\n| filter name =  requests  value   10 host !~ /^(web|db)?servers?$/ \n| view table -limit 20  Example: Display only points containing a field named code  This example also uses metric data similar to  dataset 3 .  read stochastic -source 'cdn' -nhosts 3 -dos 0.5 -last :5 minutes: -source_type 'metrics' name = 'requests' OR name = 'response_ms' OR name = 'responses'\n| filter code != null\n| view table -limit 20", 
            "title": "filter"
        }, 
        {
            "location": "/processors/head/", 
            "text": "head\n\n\nOnly emit the first limit points from each batch.\n\n\nhead limit [by groupfield1, [groupfield2, ...]]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nlimit\n\n\nThe number of points to emit; may be an expression that evaluates to an integer\n\n\nNo\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nExample: Generate batches containing ten points each but only log the first two points from each batch\n \n\n\nemit -hz 10 -limit 100 \n| batch 1 \n| head 2 \n| view text", 
            "title": "____ head"
        }, 
        {
            "location": "/processors/head/#head", 
            "text": "Only emit the first limit points from each batch.  head limit [by groupfield1, [groupfield2, ...]]     Parameter  Description  Required?      limit  The number of points to emit; may be an expression that evaluates to an integer  No    by  One or more fields by which to group. See  Grouping fields with by .  No     Example: Generate batches containing ten points each but only log the first two points from each batch    emit -hz 10 -limit 100 \n| batch 1 \n| head 2 \n| view text", 
            "title": "head"
        }, 
        {
            "location": "/processors/join/", 
            "text": "join\n\n\nCreate new points from the points of multiple input streams.\n\n\njoin\n   [-nearest :duration: | -zip (true | :duration:) | -once true]\n   [-table inputNumber, inputNumber, ...]\n   [-outer inputNumber]\n   [fieldName1, fieldName2,...fieldNameN]\n\n\n\n\nIt should be placed after a merge point in a parallel Juttle expression,\nlike this:\n\n\n(... ; ...) | join ...\n\n\n\n\nIn its simplest form, join aligns its input streams by time stamp and\nproduces new output points by unioning the fields of successive matching\ninput points.\n\n\nIf used on a single stream, its points are grouped by their time stamp\nor batch, and all points in a group are unioned to create an output\npoint. If optional joinFields are specified for the single stream, they\nact like group-by fields, and there is an output point for each distinct\nvalue of the joinFields among the points.\n\n\nOptions to \njoin\n can cause it to\noperate like an SQL relational join across multiple input streams, where\nbatches of points on an input are analogous to SQL tables, and\nindividual points are analogous to rows in those tables. An optional\nlist of fieldNames specifies relational join keys between the input\nbatches. When points with matching key values appear on each input, new\npoints are produced containing the union of these matching points'\nfields.\n\n\nStreaming joins are computed continually as points or batches of points\narrive at the inputs. When a stream is batched, all points in a batch\nare treated as a group for joining (as if they all had the batch's\nending time stamp). For an unbatched stream, all points having the same\ntime stamp are treated as a batch. It is okay to join a batched stream\nwith an unbatched stream.\n\n\nTo require exact time stamp matching for batches to be joined, specify\ntime as a join key along with any other join keys. If time is not listed\nas a join key, inexact time matches are allowed. Equivalent time stamps\nwill always be joined when they are present. A streaming one-to-one join\nwithout exact time stamp matching is specified with -zip true or -zip\n:duration:. A streaming many-to-one join with -nearest :duration: (or\nsimply as join with no options). A one-to-one join matches each\nsuccessive input batch with one on its other inputs, matching by nearest\ntime stamp, and drops any extra input batches that do not have matches.\nA many-to-one join performs a similar matching, but will re-use an input\nbatch if it remains the best match. \nBoth \n-zip\n and \n-nearest\n accept a duration limiting the\ndifference in time stamps allowed between matched batches, with :0:\nbehaving as if time had been specified as a join key. When time stamps\ndo not match exactly, an output point will be given the maximum time\nstamp of its input points. A join without \n-zip\n or \n-nearest\n or time as a join key is\nimplicitly a \n-nearest\n join with no limiting duration.\n\n\nA streaming join can treat some of its input batches as passive tables\nwith no associated time stamp. For example, this is useful for joining a\nstream of event points having user IDs against a table of user names,\nannotating the event point with its particular user name. \nThe \n-table\n option lists which inputs are to be\ntreated in this way. A batch on a table input remains there until\nupdated by a later batch, and joins are always performed against the\nmost recently received complete batch. Joins are only triggered by the\narrival of new points on a timeful input. Because tables are timeless, a\njoin is never triggered by an update to a table, and no guarantees can\nbe made about precisely when (in stream time) an update will displace a\ncurrent table batch.\n\n\nA non-streaming join over all points at once (at the end of the run) can\nbe specified by the \n-once\n flag. \nThe join is not computed until all points have arrived at the inputs (that\nis, it only makes sense for historic queries or bounded live queries).\nTime stamps will be ignored unless the time field is specified as a join\nfield, which forces time stamps to be matched exactly between the input\nsets.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-nearest :duration:\n\n\nStreaming many-to-one join of each input point or batch with the points or batches on the other inputs having the nearest time stamp. \nIf one stream has fewer points than the others, its points may be joined multiple times as needed so that all points or batches on the other inputs participate in a join. The duration is the maximum acceptable difference in time stamps between matched groups of points, and will cause points to be dropped rather than joined if the gap in time is too large.\n:information_source: \nNote:\n-nearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.\n\n\nNo\n\n\n\n\n\n\n-zip true\n\\\n\n\n:duration:\n\n\nStreaming one-to-one join of each input point or batch with the points or batched on the other inputs having the nearest time stamp. \nIf one stream has more points than the others, extra points are dropped, such that any point is joined at most once. \nIf specified, the \nduration\nis the maximum acceptable difference in time stamps between matched groups of points. If instead \n-zip true\nis specified, there is no limit to the difference in time stamps. \n:information_source: \nNote:\n-nearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.\n\n\n\n\n\n\n-once true\n\n\nNon-streaming join over the entire set of input points, after all points have been received. \nThe time field may be specified as a join field if desired, giving results similar to a streaming one-to-one join with exactly matching time stamps. There is no analogue to the duration parameter of -zip and -nearest for non-streaming joins. \n:information_source: \nNote:\nnearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.\n\n\nNo\n\n\n\n\n\n\n-table\n\n\nTreat the specified input streams as passive tables with no associated time stamp; input streams are numbered top to bottom (or left to right) from 1 in the program. \n:baby_symbol: \nexperimental:\n We're still working on this feature. Try it and see what you think, then \nchat with us\n to provide feedback. \n\n\nNo\n\n\n\n\n\n\n-outer inputNumber\n\n\nPerform an outer join preserving the specified input stream. Input streams are numbered top to bottom (or left to right) from 1 in the program (that is, \n-outer 1\nspecifies an outer left join where the first series in the flowgraph is preserved). When a point on the outer input does not have a matching join key on the other inputs, \n-outer\nforwards it unchanged, or partially joined against any matching inputs. Without \n-outer\n an inner join is performed, which only produces a points when all inputs match.\n\n\nNo\n\n\n\n\n\n\nfieldnameN\n\n\nThe fields to match when joining points.\n\n\nNo\n\n\n\n\n\n\n\n\nExample: merging metric points\n\n\nOne use for join is with a source that provides several different\nmetrics as name/value pairs on separate points having identical time\nstamps. Use join to merge these onto a single point for each time stamp:\n\n\nemit -points [\n  {time:\n2015-11-26T11:22:33.000Z\n, \nname\n:\nfurlongs\n, value:25},\n  {time:\n2015-11-26T11:22:33.000Z\n, \nname\n:\nfortnights\n, value:7},\n  {time:\n2015-11-27T10:20:50.000Z\n, \nname\n:\nfurlongs\n, value:34},\n  {time:\n2015-11-27T10:20:50.000Z\n, \nname\n:\nfortnights\n, value:5}\n]\n  | put *name = value      // turn {name:\nfurlongs\n, value:20} into {furlongs:20}\n  | remove name, value       // those fields are no longer needed\n  | join                   // combine points that have the same time value\n  | put speed = furlongs/fortnights\n\n\n\n\nExample: streaming one-to-one relational join\n\n\nIn this example, two input streams containing distance and time\nmeasurements are joined so that a rate can be computed:\n\n\n// Join 1-1\n//\n// This demonstrates a synchronized join between two metric\n// streams, allowing us to \ndivide\n one stream by another\n// to create a new metric. It does this by creating new points\n// containing one metric from each input stream.\n//\n// one-to-one joining is triggered by 'join -zip \nduration\n'.\n// an input point is matched to the point on the other\n// input that is nearest (within \nduration\n) without being in the future.\n// If we had specified \nduration\n of :0s:, timestamps would need to be equal.\n// We instead specified -zip true, so any offset is allowed (but exact\n// matches will always be made when they are present)\n//\n( emit \n  | put furlongs = Math.random()\n  ; \n  emit -from :+0.5 s: \n  | put fortnights = Math.random()\n) | join -zip true \n| put speed = furlongs / fortnights \n| view table\n\n\n\n\nThe output is the result of the join: points containing a new metric:\n\"speed\".\n\n\nExample: streaming many-to-one relational join\n\n\nIn this example, two emits create two input streams for the join:\n\n\n\n\nA stream of \"parts\" that include a board-ID\n\n\nA \"table\" of board-id -\n board name mappings. The example strips\n   their time stamps away so that they are treated as a group (similar\n   results occur if they all have the same time stamp)\n\n\n\n\nJoining the stream of parts against the table of board IDs creates\noutput points containing both the part name and its board name.\n\n\n// 9 parts having board_ids are joined against table of board_id-\nboardname\n( emit -hz 1000 -limit 9\n  | put part = \npart-\n+Number.toString(count()), board_id = count()%3 + 1, t = time-:now:  \n  ; \n  emit -from :0: -hz 1000 -limit 3 \n| put board = \nboard-\n + Number.toString(count()), board_id = count() \n| remove time\n) | join board_id \n| batch 100 \n| keep t, part, board \n| view table\n\n\n\n\nThe input to the join is two tables: 3 rows of board_id-\nboard\nnames, and 9 rows of parts (table commands are not included in the\nJuttle above).\n\n\nThe output is the result of the join: points containing part names and\nboard names.\n\n\nExample: streaming four-way right outer join\n\n\nIn this example, a stream of IDs (on points replayed by emit -points) is\nsimultaneously outer joined against three tables of personal\ninformation.\n\n\nJoining the stream of parts against the table of board IDs creates\noutput points containing both the part name and its board name.\n\n\n// 4-way right outer join of a point stream of ids against  tables of personal information.\n//\n// The points in the \ntables\n all have the same timestamp.\n// For the join, the ID in each emit point\n// is matched against each table, and an output point is created that is the union of all \n// matching points. This demonstrates partial joins when not all tables have an entry for \n// an ID. There are no matches at all for ID 5, so that point is passed through unchanged.\n//---------------------------------------------------------------------------\n    const name = [\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:1, \nname\n:\nfred\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:2, \nname\n:\nwilma\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:3, \nname\n:\ndino\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:4, \nname\n:\nbarney\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:6, \nname\n:\nbambam\n},\n       ]\n    ;\n    const haircolor = [\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:1, \nhaircolor\n:\nblack\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:2, \nhaircolor\n:\norange\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:4, \nhaircolor\n:\nblonde\n},\n      {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:6, \nhaircolor\n:\nblonde\n}\n       ]\n    ;\n    const hobby = [\n       {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:1, \nhobby\n:\nbowling\n},\n       {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:3, \nhobby\n:\nsinging\n},\n       {time:\n1970-01-01T00:00:00.000Z\n, \nid\n:6, \nhobby\n:\nhome improvement\n},\n       ]\n    ;\n    ( emit -points name\n      ;\n      emit -points haircolor\n      ;\n      emit -points hobby\n      ;\n      emit -from :0: -limit 6\n      | put id = count()\n    ) | join -outer 4 id\n    | remove time, type\n    | view table", 
            "title": "____ join"
        }, 
        {
            "location": "/processors/join/#join", 
            "text": "Create new points from the points of multiple input streams.  join\n   [-nearest :duration: | -zip (true | :duration:) | -once true]\n   [-table inputNumber, inputNumber, ...]\n   [-outer inputNumber]\n   [fieldName1, fieldName2,...fieldNameN]  It should be placed after a merge point in a parallel Juttle expression,\nlike this:  (... ; ...) | join ...  In its simplest form, join aligns its input streams by time stamp and\nproduces new output points by unioning the fields of successive matching\ninput points.  If used on a single stream, its points are grouped by their time stamp\nor batch, and all points in a group are unioned to create an output\npoint. If optional joinFields are specified for the single stream, they\nact like group-by fields, and there is an output point for each distinct\nvalue of the joinFields among the points.  Options to  join  can cause it to\noperate like an SQL relational join across multiple input streams, where\nbatches of points on an input are analogous to SQL tables, and\nindividual points are analogous to rows in those tables. An optional\nlist of fieldNames specifies relational join keys between the input\nbatches. When points with matching key values appear on each input, new\npoints are produced containing the union of these matching points'\nfields.  Streaming joins are computed continually as points or batches of points\narrive at the inputs. When a stream is batched, all points in a batch\nare treated as a group for joining (as if they all had the batch's\nending time stamp). For an unbatched stream, all points having the same\ntime stamp are treated as a batch. It is okay to join a batched stream\nwith an unbatched stream.  To require exact time stamp matching for batches to be joined, specify\ntime as a join key along with any other join keys. If time is not listed\nas a join key, inexact time matches are allowed. Equivalent time stamps\nwill always be joined when they are present. A streaming one-to-one join\nwithout exact time stamp matching is specified with -zip true or -zip\n:duration:. A streaming many-to-one join with -nearest :duration: (or\nsimply as join with no options). A one-to-one join matches each\nsuccessive input batch with one on its other inputs, matching by nearest\ntime stamp, and drops any extra input batches that do not have matches.\nA many-to-one join performs a similar matching, but will re-use an input\nbatch if it remains the best match. \nBoth  -zip  and  -nearest  accept a duration limiting the\ndifference in time stamps allowed between matched batches, with :0:\nbehaving as if time had been specified as a join key. When time stamps\ndo not match exactly, an output point will be given the maximum time\nstamp of its input points. A join without  -zip  or  -nearest  or time as a join key is\nimplicitly a  -nearest  join with no limiting duration.  A streaming join can treat some of its input batches as passive tables\nwith no associated time stamp. For example, this is useful for joining a\nstream of event points having user IDs against a table of user names,\nannotating the event point with its particular user name. \nThe  -table  option lists which inputs are to be\ntreated in this way. A batch on a table input remains there until\nupdated by a later batch, and joins are always performed against the\nmost recently received complete batch. Joins are only triggered by the\narrival of new points on a timeful input. Because tables are timeless, a\njoin is never triggered by an update to a table, and no guarantees can\nbe made about precisely when (in stream time) an update will displace a\ncurrent table batch.  A non-streaming join over all points at once (at the end of the run) can\nbe specified by the  -once  flag. \nThe join is not computed until all points have arrived at the inputs (that\nis, it only makes sense for historic queries or bounded live queries).\nTime stamps will be ignored unless the time field is specified as a join\nfield, which forces time stamps to be matched exactly between the input\nsets.     Parameter  Description  Required?      -nearest :duration:  Streaming many-to-one join of each input point or batch with the points or batches on the other inputs having the nearest time stamp.  If one stream has fewer points than the others, its points may be joined multiple times as needed so that all points or batches on the other inputs participate in a join. The duration is the maximum acceptable difference in time stamps between matched groups of points, and will cause points to be dropped rather than joined if the gap in time is too large. :information_source:  Note: -nearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.  No    -zip true \\  :duration:  Streaming one-to-one join of each input point or batch with the points or batched on the other inputs having the nearest time stamp.  If one stream has more points than the others, extra points are dropped, such that any point is joined at most once.  If specified, the  duration is the maximum acceptable difference in time stamps between matched groups of points. If instead  -zip true is specified, there is no limit to the difference in time stamps.  :information_source:  Note: -nearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.    -once true  Non-streaming join over the entire set of input points, after all points have been received.  The time field may be specified as a join field if desired, giving results similar to a streaming one-to-one join with exactly matching time stamps. There is no analogue to the duration parameter of -zip and -nearest for non-streaming joins.  :information_source:  Note: nearest, -zip, and -once are mutually exclusive. If none are specified, the behavior is as if -nearest was specified with an unbounded duration.  No    -table  Treat the specified input streams as passive tables with no associated time stamp; input streams are numbered top to bottom (or left to right) from 1 in the program.  :baby_symbol:  experimental:  We're still working on this feature. Try it and see what you think, then  chat with us  to provide feedback.   No    -outer inputNumber  Perform an outer join preserving the specified input stream. Input streams are numbered top to bottom (or left to right) from 1 in the program (that is,  -outer 1 specifies an outer left join where the first series in the flowgraph is preserved). When a point on the outer input does not have a matching join key on the other inputs,  -outer forwards it unchanged, or partially joined against any matching inputs. Without  -outer  an inner join is performed, which only produces a points when all inputs match.  No    fieldnameN  The fields to match when joining points.  No     Example: merging metric points  One use for join is with a source that provides several different\nmetrics as name/value pairs on separate points having identical time\nstamps. Use join to merge these onto a single point for each time stamp:  emit -points [\n  {time: 2015-11-26T11:22:33.000Z ,  name : furlongs , value:25},\n  {time: 2015-11-26T11:22:33.000Z ,  name : fortnights , value:7},\n  {time: 2015-11-27T10:20:50.000Z ,  name : furlongs , value:34},\n  {time: 2015-11-27T10:20:50.000Z ,  name : fortnights , value:5}\n]\n  | put *name = value      // turn {name: furlongs , value:20} into {furlongs:20}\n  | remove name, value       // those fields are no longer needed\n  | join                   // combine points that have the same time value\n  | put speed = furlongs/fortnights  Example: streaming one-to-one relational join  In this example, two input streams containing distance and time\nmeasurements are joined so that a rate can be computed:  // Join 1-1\n//\n// This demonstrates a synchronized join between two metric\n// streams, allowing us to  divide  one stream by another\n// to create a new metric. It does this by creating new points\n// containing one metric from each input stream.\n//\n// one-to-one joining is triggered by 'join -zip  duration '.\n// an input point is matched to the point on the other\n// input that is nearest (within  duration ) without being in the future.\n// If we had specified  duration  of :0s:, timestamps would need to be equal.\n// We instead specified -zip true, so any offset is allowed (but exact\n// matches will always be made when they are present)\n//\n( emit \n  | put furlongs = Math.random()\n  ; \n  emit -from :+0.5 s: \n  | put fortnights = Math.random()\n) | join -zip true \n| put speed = furlongs / fortnights \n| view table  The output is the result of the join: points containing a new metric:\n\"speed\".  Example: streaming many-to-one relational join  In this example, two emits create two input streams for the join:   A stream of \"parts\" that include a board-ID  A \"table\" of board-id -  board name mappings. The example strips\n   their time stamps away so that they are treated as a group (similar\n   results occur if they all have the same time stamp)   Joining the stream of parts against the table of board IDs creates\noutput points containing both the part name and its board name.  // 9 parts having board_ids are joined against table of board_id- boardname\n( emit -hz 1000 -limit 9\n  | put part =  part- +Number.toString(count()), board_id = count()%3 + 1, t = time-:now:  \n  ; \n  emit -from :0: -hz 1000 -limit 3 \n| put board =  board-  + Number.toString(count()), board_id = count() \n| remove time\n) | join board_id \n| batch 100 \n| keep t, part, board \n| view table  The input to the join is two tables: 3 rows of board_id- board\nnames, and 9 rows of parts (table commands are not included in the\nJuttle above).  The output is the result of the join: points containing part names and\nboard names.  Example: streaming four-way right outer join  In this example, a stream of IDs (on points replayed by emit -points) is\nsimultaneously outer joined against three tables of personal\ninformation.  Joining the stream of parts against the table of board IDs creates\noutput points containing both the part name and its board name.  // 4-way right outer join of a point stream of ids against  tables of personal information.\n//\n// The points in the  tables  all have the same timestamp.\n// For the join, the ID in each emit point\n// is matched against each table, and an output point is created that is the union of all \n// matching points. This demonstrates partial joins when not all tables have an entry for \n// an ID. There are no matches at all for ID 5, so that point is passed through unchanged.\n//---------------------------------------------------------------------------\n    const name = [\n      {time: 1970-01-01T00:00:00.000Z ,  id :1,  name : fred },\n      {time: 1970-01-01T00:00:00.000Z ,  id :2,  name : wilma },\n      {time: 1970-01-01T00:00:00.000Z ,  id :3,  name : dino },\n      {time: 1970-01-01T00:00:00.000Z ,  id :4,  name : barney },\n      {time: 1970-01-01T00:00:00.000Z ,  id :6,  name : bambam },\n       ]\n    ;\n    const haircolor = [\n      {time: 1970-01-01T00:00:00.000Z ,  id :1,  haircolor : black },\n      {time: 1970-01-01T00:00:00.000Z ,  id :2,  haircolor : orange },\n      {time: 1970-01-01T00:00:00.000Z ,  id :4,  haircolor : blonde },\n      {time: 1970-01-01T00:00:00.000Z ,  id :6,  haircolor : blonde }\n       ]\n    ;\n    const hobby = [\n       {time: 1970-01-01T00:00:00.000Z ,  id :1,  hobby : bowling },\n       {time: 1970-01-01T00:00:00.000Z ,  id :3,  hobby : singing },\n       {time: 1970-01-01T00:00:00.000Z ,  id :6,  hobby : home improvement },\n       ]\n    ;\n    ( emit -points name\n      ;\n      emit -points haircolor\n      ;\n      emit -points hobby\n      ;\n      emit -from :0: -limit 6\n      | put id = count()\n    ) | join -outer 4 id\n    | remove time, type\n    | view table", 
            "title": "join"
        }, 
        {
            "location": "/processors/keep/", 
            "text": "keep\n\n\nRemove all fields from the input stream except those that are specified.\nUse this processor to prune the input stream, keeping only the fields of\ninterest.\n\n\n \nNote:\n The time field must be specified\nexplicitly in order to keep it; otherwise you end up with timeless\npoints.\n\n\nkeep fieldName1, fieldName2,...fieldNameN\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfieldnameN\n\n\nThe fields to propagate from input to output\n\n\nAt least one\n\n\n\n\n\n\n\n\nExample: Remove all fields except the a field, then log the remaining data points\n\n\nemit -hz 10 -limit 10 \n| put a = 1, b = 2 \n| keep a \n| view text", 
            "title": "____ keep"
        }, 
        {
            "location": "/processors/keep/#keep", 
            "text": "Remove all fields from the input stream except those that are specified.\nUse this processor to prune the input stream, keeping only the fields of\ninterest.    Note:  The time field must be specified\nexplicitly in order to keep it; otherwise you end up with timeless\npoints.  keep fieldName1, fieldName2,...fieldNameN     Parameter  Description  Required?      fieldnameN  The fields to propagate from input to output  At least one     Example: Remove all fields except the a field, then log the remaining data points  emit -hz 10 -limit 10 \n| put a = 1, b = 2 \n| keep a \n| view text", 
            "title": "keep"
        }, 
        {
            "location": "/processors/pace/", 
            "text": "pace\n\n\nPlay back historic points in real time or as a multiple of the input's\nnatural speed. If the input is batched, thenpace emits one batch of\npoints at each output time. Otherwise it emits any points sharing the\nnext time stamp in the input sequence.\n\n\npace [-from :moment:] [-every :duration:] [-x multiple]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-from\n\n\nShift the time stamp of the first point to this specified moment, and shift subsequent points by the same amount of time\n\n\nNo\n\n\n\n\n\n\n-every\n\n\nThe interval at which to emit points, specified as a \n:duration:\n See \nTime notation in Juttle\n for syntax information.\n\n\nNo\n\n\n\n\n\n\n-x\n\n\nA multiple of the real-time pace of the input\n\n\nNo\n\n\n\n\n\n\n\n\nThe playback speed may be specified in one of two ways:\n\n\n\n\nAs a real-time interval between output times (\n-every\n)\n\n\nAs a multiple of the input's natural speed (\n-x\n)\n\n\n\n\nIf neither \n-every\n nor \n-x\n is specified, then playback occurs at a real-time rate.\n\n\nIf the input stream includes live points, then pace gradually\ndecelerates its historic playback to arrive smoothly at the real-time\nrate for the live data.\n\n\nExample: Emit ten synthetic points and play them back in real time\n\n\nemit -from :0: -limit 10\n| pace", 
            "title": "____ pace"
        }, 
        {
            "location": "/processors/pace/#pace", 
            "text": "Play back historic points in real time or as a multiple of the input's\nnatural speed. If the input is batched, thenpace emits one batch of\npoints at each output time. Otherwise it emits any points sharing the\nnext time stamp in the input sequence.  pace [-from :moment:] [-every :duration:] [-x multiple]     Parameter  Description  Required?      -from  Shift the time stamp of the first point to this specified moment, and shift subsequent points by the same amount of time  No    -every  The interval at which to emit points, specified as a  :duration:  See  Time notation in Juttle  for syntax information.  No    -x  A multiple of the real-time pace of the input  No     The playback speed may be specified in one of two ways:   As a real-time interval between output times ( -every )  As a multiple of the input's natural speed ( -x )   If neither  -every  nor  -x  is specified, then playback occurs at a real-time rate.  If the input stream includes live points, then pace gradually\ndecelerates its historic playback to arrive smoothly at the real-time\nrate for the live data.  Example: Emit ten synthetic points and play them back in real time  emit -from :0: -limit 10\n| pace", 
            "title": "pace"
        }, 
        {
            "location": "/processors/pass/", 
            "text": "pass\n\n\nPass the output of the preceding flowgraph into the next one.\n\n\npass\n\n\n\n\nThe pass processor has no options.\n\n\nExample: Using pass to debug a flowgraph\n\n\npass is handy for outputting data from the middle of a flowgraph, then\nallowing the flowgraph to continue unchanged. This simple example\noutputs some raw points as text,\nthen passes the output unchanged to a \nput\n\nstatement before ultimately feeding into a timechart:\n\n\nemit -limit 10\n| (view text ; pass)\n| put value=Math.random()\n| view timechart\n\n\n\n\nThe (view text ; pass) statement is handy for debugging; just insert it at\nany point in your flowgraph to check whether your data is being\nprocessed as expected.", 
            "title": "____ pass"
        }, 
        {
            "location": "/processors/pass/#pass", 
            "text": "Pass the output of the preceding flowgraph into the next one.  pass  The pass processor has no options.  Example: Using pass to debug a flowgraph  pass is handy for outputting data from the middle of a flowgraph, then\nallowing the flowgraph to continue unchanged. This simple example\noutputs some raw points as text,\nthen passes the output unchanged to a  put \nstatement before ultimately feeding into a timechart:  emit -limit 10\n| (view text ; pass)\n| put value=Math.random()\n| view timechart  The (view text ; pass) statement is handy for debugging; just insert it at\nany point in your flowgraph to check whether your data is being\nprocessed as expected.", 
            "title": "pass"
        }, 
        {
            "location": "/processors/put/", 
            "text": "put\n\n\nSet the specified field of each point to the result of an expression, optionally computed over a \nmoving time window\n.\n\n\nput [-over duration]\n  [-reset true|false]\n  fieldname1=expr1 [, fieldnameN=exprN]\n  [by field1, [field2, ...]]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-over\n\n\nThe moving time window over which the value will be computed, as a \nduration literal\n. \nIf \n-over\nis :forever:, the results are cumulative, over all points seen so far. \nSee \nMoving time windows\n for more information about moving time windows. \n\n\nNo; if \n-over\nis not specified, all points since the beginning of the current batch are used\n\n\n\n\n\n\n-reset\n\n\nSet this to 'false' to emit results cumulatively, over all points seen so far, when a reducer expression is present (by default, a reducer in a batched flow will be reset at the beginning of each batch).\n\n\nNo\n\n\n\n\n\n\nfieldname=expr\n\n\nSet the field named fieldname to the value specified by expr, where expr can be a literal value, an arithmetic expression, a \nfunction\n invocation, or a \nreducer\n invocation\n\n\nYes\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nSee \nField referencing\n \nfor more about this syntax.\n\n\nExample: Set the field foo of every point to 5\n\n\nemit -limit 5 \n| put foo = 5 \n| view table\n\n\n\n\nExample: Set the field foo to 5 and the bar field to \"string\" for every point\n\n\nemit -limit 5 \n| put foo = 5, bar = \nstring\n \n| view table\n\n\n\n\nExample: Set the field foo to the value of the field bar multiplied by 10\n\n\nemit -limit 5 \n| put bar = count(), foo = bar * 10 \n| view table \n\n\n\n\nSet the field foo of every point to a random number between 0 and 1\n\n\nemit -limit 5\n| put foo = Math.random() \n| view table\n\n\n\n\nExample: Superimposing yesterday's CPU usage over today's\n\n\n// Day over Day graph example:\n//\n// display a graph of cpu usage superimposed over the previous day\n// by using a moving window to get the value from 24 hours ago.\n//\n// The graph will begin at 2014-01-01, but we need to start the data\n// source a day earlier so the windowed reducer can produce a point\n// for 2014-01-01\n//\nread stochastic -source 'cdn' -from :2013-12-31: -to :2014-01-05: -daily .5 -source_type 'metric' name = 'cpu'\n| reduce \n    -every :2h: value = avg(value)\n| put \n    -over :25h: prev = first(value)\n| filter time \n= :2014-01-01: // discard unfilled window points from Dec.\n| split value,prev\n| view timechart", 
            "title": "____ put"
        }, 
        {
            "location": "/processors/put/#put", 
            "text": "Set the specified field of each point to the result of an expression, optionally computed over a  moving time window .  put [-over duration]\n  [-reset true|false]\n  fieldname1=expr1 [, fieldnameN=exprN]\n  [by field1, [field2, ...]]     Parameter  Description  Required?      -over  The moving time window over which the value will be computed, as a  duration literal .  If  -over is :forever:, the results are cumulative, over all points seen so far.  See  Moving time windows  for more information about moving time windows.   No; if  -over is not specified, all points since the beginning of the current batch are used    -reset  Set this to 'false' to emit results cumulatively, over all points seen so far, when a reducer expression is present (by default, a reducer in a batched flow will be reset at the beginning of each batch).  No    fieldname=expr  Set the field named fieldname to the value specified by expr, where expr can be a literal value, an arithmetic expression, a  function  invocation, or a  reducer  invocation  Yes    by  One or more fields by which to group. See  Grouping fields with by .  No     See  Field referencing  \nfor more about this syntax.  Example: Set the field foo of every point to 5  emit -limit 5 \n| put foo = 5 \n| view table  Example: Set the field foo to 5 and the bar field to \"string\" for every point  emit -limit 5 \n| put foo = 5, bar =  string  \n| view table  Example: Set the field foo to the value of the field bar multiplied by 10  emit -limit 5 \n| put bar = count(), foo = bar * 10 \n| view table   Set the field foo of every point to a random number between 0 and 1  emit -limit 5\n| put foo = Math.random() \n| view table  Example: Superimposing yesterday's CPU usage over today's  // Day over Day graph example:\n//\n// display a graph of cpu usage superimposed over the previous day\n// by using a moving window to get the value from 24 hours ago.\n//\n// The graph will begin at 2014-01-01, but we need to start the data\n// source a day earlier so the windowed reducer can produce a point\n// for 2014-01-01\n//\nread stochastic -source 'cdn' -from :2013-12-31: -to :2014-01-05: -daily .5 -source_type 'metric' name = 'cpu'\n| reduce \n    -every :2h: value = avg(value)\n| put \n    -over :25h: prev = first(value)\n| filter time  = :2014-01-01: // discard unfilled window points from Dec.\n| split value,prev\n| view timechart", 
            "title": "put"
        }, 
        {
            "location": "/processors/reduce/", 
            "text": "reduce\n\n\nAccumulate collections of points and aggregate them using reducers,\noptionally within a \nmoving time window\n\n\nreduce [-every duration [-on duration-or-calendar-offset]]\n  [-over duration]\n  [-reset true|false]\n  [-forget false]\n  fieldname1=expr1 [, fieldnameN=exprN]\n  [by field1, [field2, ...]]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-every\n\n\nThe interval at which values will be computed, as a duration literal. A new result is produced each time this duration passes.\n\n\nNo; if not specified, the upstream batch interval is used. If there is no upstream batch, a single result is produced after the stream has ended.\n\n\n\n\n\n\nover\n\n\nThe moving time window over which the value will be computed, as a \nduration literal\n. This is typically some multiple of the \n-every\ninterval (for example, \nreduce -every :minute: -over :hour: value=avg(value)\nupdates a trailing hourly average every minute). \nWhen \n-over\nis specified, results will only be produced for full windows of data. No results will be produced until \n-over\nhas passed after the first point. No result will be produced for any final points that span interval less than \n-over\n \nIf \n-over\nis :forever:, the reducer emits results cumulatively, over all points seen so far. \nSee \nMoving time windows\n for more information about moving time windows.\n\n\nNo\n\n\n\n\n\n\n-reset\n\n\nSet this to 'false' to emit results cumulatively, over all points seen so far.\n\n\nNo\n\n\n\n\n\n\n-forget\n\n\nWhen grouping reducer results with 'by', set this to 'false' to cause results to be emitted for every value of 'by' that has been seen in previous batches. Values that do not appear in the current batch will report their 'empty' value (for example, count() will produce 0, avg() will produce null)\n\n\nNo\n\n\n\n\n\n\n-from\n\n\nWhen used with \n-over\n the start time of the earliest full window of data.\n\n\nNo; if not specified, the earliest window begins with the earliest point or batch received.\n\n\n\n\n\n\n-to\n\n\nWhen used with \n-over\n the end time of the last full window of data.\n\n\nNo; if not specified, the last point or batch time stamp received is assumed.\n\n\n\n\n\n\n-on\n\n\nA time alignment for \n-every\n It may be a duration or a calendar offset less than \n-every\n For example, \n-every :hour: -on :00:30:00:\nruns every hour on the half-hour, while \n-every :month: -on :day 10:\nruns a monthly computation on day 10 of the month.\n\n\nNo; if \n-on\nis not specified, output batches are aligned with the UNIX epoch, as if from a batch node.\n\n\n\n\n\n\nfieldname=expr\n\n\nAn assignment expression, where expr can be a \nreducer\n. Each assignment expression results in an output field that has the left-hand side of the expression as its name and the computed right-hand side as its value.\n\n\nAt least one\n\n\n\n\n\n\nby\n\n\nOne or more fields for which the assignment expression is computed separately. \nIf grouping fields are present, then the assignment expression is computed independently for each unique combination of values present in the grouping fields. See \nProcessors\n for more about grouping with by.  \n\n\nNo; if no grouping fields are present, then the assignment expression is computed over all points in the batch.\n\n\n\n\n\n\n\n\nThe output points contain:\n\n\n\n\nAll the fields specified as assignment expressions or grouping fields in the arguments\n\n\nEach of the grouping fields with the unique value(s) for which the assignment expressions were computed\n\n\nWhen operating on batched points, every output point contains a time field containing the end time of the incoming batch.\n\n\n\n\nWhen reduce operates on batched points (that is, when there is a\n\nbatch\n processor preceding \nreduce\n), it generates a set of output points for each incoming batch of points, resetting its internal state after each\nbatch. When the points flowing into reduce are not batched, output\npoints are generated for all points when the stream ends.\n\n\nSee \nField referencing\n for additional information relevant to this processor.\n\n\nExample: Trailing ten-minute average\n\n\n// Trailing average example:\n//\n// Smooth a stream of random values occurring every 10 seconds\n// by computing a trailing 10 minute average\n// \nsub random() {\n    emit \n      -from :2014-01-01: \n      -limit 500 \n      -every :10s:\n    | put name = 'random'\n    | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\nrandom\n| reduce \n    -every :m: \n    -over :10m: avg = avg(value), value = last(value)\n| split value, avg\n| view timechart\n\n\n\n\nExample: Superimposing yesterday's CPU usage over today's\n\n\n// Day over Day graph example:\n//\n// display a graph of cpu usage superimposed over the previous day\n// by using a moving window to get the value from 24 hours ago.\n//\n// The graph will begin at 2014-01-01, but we need to start the data\n// source a day earlier so the windowed reducer can produce a point\n// for 2014-01-01\n//\nread stochastic -source 'cdn' -from :2013-12-31: -to :2014-01-05: -daily .5 -source_type 'metric' name = 'cpu'\n| reduce \n    -every :2h: value = avg(value)\n| put \n    -over :25h: prev = first(value)\n| filter time \n= :2014-01-01: // discard unfilled window points from Dec.\n| split value,prev\n| view timechart\n\n\n\n\nExample: Call records, day by day\n\n\n// Call Record billing example:\n//\n// Call records arrive as a stream of points indicating duration in minutes.\n// Your phone bill is the total of these, charged at $.05/minute, from the\n// 20th of each month.\n//\n// This program displays a day-by-day running total of your bill:\n//\nsub call_record() {\n  emit -from :2014-01-01: -limit 4000 -every :h:\n  | put name = 'duration'\n  | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\ncall_record\n| batch \n    -every :month:  \n    -on :day 20: \n| put name = 'total', value = sum(value) * 0.05\n| view timechart\n;\n//\n// This program displays a table with monthly totals\n//\ncall_record\n| reduce \n    -every :month:\n    -on :day 20: value = sum(value) * 0.05\n| put name = \ntotal\n, value = Math.floor(value * 100) / 100 \n| view table \n\n\n\n\n\nExample: Output a single point that counts the number of points in the data set (in this case 10)\n\n\nemit -hz 10 -limit 10 \n| reduce count() \n| view text\n\n\n\n\nExample: Any number of assignments can be used and they can be any expression\n\n\nemit -hz 10 -limit 10 \n| reduce x = 3 * 4, count() \n| view text\n\n\n\n\nExample: Aggregate the points in batches of three seconds\n\n\nemit -every :0.1s: -limit 100 \n| batch :2s:\n| reduce count()\n| view text\n\n\n\n\nExample: Aggregate the ten points by unique values of y\n\n\nemit -from :0: -limit 10\n| put x = Math.random() * 2, y = Math.floor(x)\n| reduce by y \n| view text\n\n\n\n\nExample: Count the number of points per unique value of y (over the entire stream of points)\n\n\nemit -from :0: -limit 10 \n| put x = Math.random() * 2, y = Math.floor(x) \n| reduce cnt = count() by y \n| view text\n\n\n\n\n\nExample: Count the number of points per unique value of y (over batches of five seconds of points)\n\n\nemit -from :0: -limit 10 \n| put x = Math.random() * 2, y = Math.floor(x) \n| batch :5s: \n| reduce cnt = count() by y \n| view text", 
            "title": "____ reduce"
        }, 
        {
            "location": "/processors/reduce/#reduce", 
            "text": "Accumulate collections of points and aggregate them using reducers,\noptionally within a  moving time window  reduce [-every duration [-on duration-or-calendar-offset]]\n  [-over duration]\n  [-reset true|false]\n  [-forget false]\n  fieldname1=expr1 [, fieldnameN=exprN]\n  [by field1, [field2, ...]]     Parameter  Description  Required?      -every  The interval at which values will be computed, as a duration literal. A new result is produced each time this duration passes.  No; if not specified, the upstream batch interval is used. If there is no upstream batch, a single result is produced after the stream has ended.    over  The moving time window over which the value will be computed, as a  duration literal . This is typically some multiple of the  -every interval (for example,  reduce -every :minute: -over :hour: value=avg(value) updates a trailing hourly average every minute).  When  -over is specified, results will only be produced for full windows of data. No results will be produced until  -over has passed after the first point. No result will be produced for any final points that span interval less than  -over   If  -over is :forever:, the reducer emits results cumulatively, over all points seen so far.  See  Moving time windows  for more information about moving time windows.  No    -reset  Set this to 'false' to emit results cumulatively, over all points seen so far.  No    -forget  When grouping reducer results with 'by', set this to 'false' to cause results to be emitted for every value of 'by' that has been seen in previous batches. Values that do not appear in the current batch will report their 'empty' value (for example, count() will produce 0, avg() will produce null)  No    -from  When used with  -over  the start time of the earliest full window of data.  No; if not specified, the earliest window begins with the earliest point or batch received.    -to  When used with  -over  the end time of the last full window of data.  No; if not specified, the last point or batch time stamp received is assumed.    -on  A time alignment for  -every  It may be a duration or a calendar offset less than  -every  For example,  -every :hour: -on :00:30:00: runs every hour on the half-hour, while  -every :month: -on :day 10: runs a monthly computation on day 10 of the month.  No; if  -on is not specified, output batches are aligned with the UNIX epoch, as if from a batch node.    fieldname=expr  An assignment expression, where expr can be a  reducer . Each assignment expression results in an output field that has the left-hand side of the expression as its name and the computed right-hand side as its value.  At least one    by  One or more fields for which the assignment expression is computed separately.  If grouping fields are present, then the assignment expression is computed independently for each unique combination of values present in the grouping fields. See  Processors  for more about grouping with by.    No; if no grouping fields are present, then the assignment expression is computed over all points in the batch.     The output points contain:   All the fields specified as assignment expressions or grouping fields in the arguments  Each of the grouping fields with the unique value(s) for which the assignment expressions were computed  When operating on batched points, every output point contains a time field containing the end time of the incoming batch.   When reduce operates on batched points (that is, when there is a batch  processor preceding  reduce ), it generates a set of output points for each incoming batch of points, resetting its internal state after each\nbatch. When the points flowing into reduce are not batched, output\npoints are generated for all points when the stream ends.  See  Field referencing  for additional information relevant to this processor.  Example: Trailing ten-minute average  // Trailing average example:\n//\n// Smooth a stream of random values occurring every 10 seconds\n// by computing a trailing 10 minute average\n// \nsub random() {\n    emit \n      -from :2014-01-01: \n      -limit 500 \n      -every :10s:\n    | put name = 'random'\n    | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\nrandom\n| reduce \n    -every :m: \n    -over :10m: avg = avg(value), value = last(value)\n| split value, avg\n| view timechart  Example: Superimposing yesterday's CPU usage over today's  // Day over Day graph example:\n//\n// display a graph of cpu usage superimposed over the previous day\n// by using a moving window to get the value from 24 hours ago.\n//\n// The graph will begin at 2014-01-01, but we need to start the data\n// source a day earlier so the windowed reducer can produce a point\n// for 2014-01-01\n//\nread stochastic -source 'cdn' -from :2013-12-31: -to :2014-01-05: -daily .5 -source_type 'metric' name = 'cpu'\n| reduce \n    -every :2h: value = avg(value)\n| put \n    -over :25h: prev = first(value)\n| filter time  = :2014-01-01: // discard unfilled window points from Dec.\n| split value,prev\n| view timechart  Example: Call records, day by day  // Call Record billing example:\n//\n// Call records arrive as a stream of points indicating duration in minutes.\n// Your phone bill is the total of these, charged at $.05/minute, from the\n// 20th of each month.\n//\n// This program displays a day-by-day running total of your bill:\n//\nsub call_record() {\n  emit -from :2014-01-01: -limit 4000 -every :h:\n  | put name = 'duration'\n  | put value = (Math.random() - .5) * 20 + (Math.random() - .5) * 10 + 5\n}\ncall_record\n| batch \n    -every :month:  \n    -on :day 20: \n| put name = 'total', value = sum(value) * 0.05\n| view timechart\n;\n//\n// This program displays a table with monthly totals\n//\ncall_record\n| reduce \n    -every :month:\n    -on :day 20: value = sum(value) * 0.05\n| put name =  total , value = Math.floor(value * 100) / 100 \n| view table   Example: Output a single point that counts the number of points in the data set (in this case 10)  emit -hz 10 -limit 10 \n| reduce count() \n| view text  Example: Any number of assignments can be used and they can be any expression  emit -hz 10 -limit 10 \n| reduce x = 3 * 4, count() \n| view text  Example: Aggregate the points in batches of three seconds  emit -every :0.1s: -limit 100 \n| batch :2s:\n| reduce count()\n| view text  Example: Aggregate the ten points by unique values of y  emit -from :0: -limit 10\n| put x = Math.random() * 2, y = Math.floor(x)\n| reduce by y \n| view text  Example: Count the number of points per unique value of y (over the entire stream of points)  emit -from :0: -limit 10 \n| put x = Math.random() * 2, y = Math.floor(x) \n| reduce cnt = count() by y \n| view text  Example: Count the number of points per unique value of y (over batches of five seconds of points)  emit -from :0: -limit 10 \n| put x = Math.random() * 2, y = Math.floor(x) \n| batch :5s: \n| reduce cnt = count() by y \n| view text", 
            "title": "reduce"
        }, 
        {
            "location": "/processors/remove/", 
            "text": "remove\n\n\nRemove the specified fields from the input stream. Like the keep\nprocessor, you can use this to prune the input stream, removing any\nuninteresting fields.\n\n\nremove fieldName1[, fieldName2,...fieldNameN]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfieldNameN\n\n\nThe name of one or more fields to remove from incoming points\n\n\nAt least one\n\n\n\n\n\n\n\n\nExample: Remove all fields except the a field (and the time field, always required), then log the remaining data points\n\n\nemit -hz 10 -limit 10 \n| put a = 1, b = 2, c = 3 \n| remove b, c \n| view text", 
            "title": "____ remove"
        }, 
        {
            "location": "/processors/remove/#remove", 
            "text": "Remove the specified fields from the input stream. Like the keep\nprocessor, you can use this to prune the input stream, removing any\nuninteresting fields.  remove fieldName1[, fieldName2,...fieldNameN]     Parameter  Description  Required?      fieldNameN  The name of one or more fields to remove from incoming points  At least one     Example: Remove all fields except the a field (and the time field, always required), then log the remaining data points  emit -hz 10 -limit 10 \n| put a = 1, b = 2, c = 3 \n| remove b, c \n| view text", 
            "title": "remove"
        }, 
        {
            "location": "/processors/reorder/", 
            "text": "reorder\n\n\nBuffer and reorder points based on their time fields.\n\n\nreorder [-delay milliseconds] [-limit n]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-delay\n\n\nThe duration of the buffer, in milliseconds\n\n\nNo; the default is 5000ms\n\n\n\n\n\n\n-limit\n\n\nThe number of points that are buffered. When the buffer is full, the oldest point (by time stamp) is passed through without delay.\n\n\nNo; the default is 100,000 points\n\n\n\n\n\n\n\n\nIf you encounter the following message while using this source, try increasing the value of the -delay parameter:   \n\n\n\n\nreorder dropping 2 point(s) that arrived late", 
            "title": "____ reorder"
        }, 
        {
            "location": "/processors/reorder/#reorder", 
            "text": "Buffer and reorder points based on their time fields.  reorder [-delay milliseconds] [-limit n]     Parameter  Description  Required?      -delay  The duration of the buffer, in milliseconds  No; the default is 5000ms    -limit  The number of points that are buffered. When the buffer is full, the oldest point (by time stamp) is passed through without delay.  No; the default is 100,000 points     If you encounter the following message while using this source, try increasing the value of the -delay parameter:      reorder dropping 2 point(s) that arrived late", 
            "title": "reorder"
        }, 
        {
            "location": "/processors/skip/", 
            "text": "skip\n\n\nDrop the first n points of a series.\n\n\nskip [n] [by field1, [field2, ...]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nn\n\n\nThe number of points to skip\n\n\nNo; if not specified, a single point is skipped\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nExample: For each value of id, skip the first two points\n\n\nemit -from Date.new(0) -limit 9\n  | put id = Math.ceil(count() / 3)\n  | skip 2 by id\n  | remove time\n  | view table", 
            "title": "____ skip"
        }, 
        {
            "location": "/processors/skip/#skip", 
            "text": "Drop the first n points of a series.  skip [n] [by field1, [field2, ...]     Parameter  Description  Required?      n  The number of points to skip  No; if not specified, a single point is skipped    by  One or more fields by which to group. See  Grouping fields with by .  No     Example: For each value of id, skip the first two points  emit -from Date.new(0) -limit 9\n  | put id = Math.ceil(count() / 3)\n  | skip 2 by id\n  | remove time\n  | view table", 
            "title": "skip"
        }, 
        {
            "location": "/processors/sort/", 
            "text": "sort\n\n\nSort points in order based on values of one or more specified fields,\nand remove the time field.\n\n\nsort [-limit N] name1 [-desc] [, name2 [-desc], ... nameN [-desc]] [by groupfield1, [groupfield2, ...]]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-limit\n\n\nSort only the first n points that arrive; additional points will be dropped. \nTo limit the output after sorting, use \nhead\n or \ntail\n after sort. \n\n\nNo\n\n\n\n\n\n\nnameN\n\n\nThe name of a field by which to sort. \nFirst the points are compared based on their name1 fields, then any ties among name1 fields are broken by comparing name2 fields, then any points whose name1 and name2 are the same are compared in their name3 field, and so forth.\n\n\nName1 is required; the rest are optional\n\n\n\n\n\n\n-desc\n\n\nIf you put \"-desc\" after the name of a field, the sort on that field will be in descending order; otherwise, it will be in ascending order.\n\n\nNo\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nOrdering is numeric (for number values) or alphabetical (for string\nvalues). It also supports multi-key sorting in ascending or descending\norder for alphabetical and numerical comparisons.\n\n\n \nNote:\n Sorting is limited to 10,000 data points by default. To sort more points, use the -limit flag to set a higher value.\n\n\nExample: Output a table of sorted random values\n\n\nemit -from :0: \n| put value = Math.random() \n| sort value \n| view table", 
            "title": "____ sort"
        }, 
        {
            "location": "/processors/sort/#sort", 
            "text": "Sort points in order based on values of one or more specified fields,\nand remove the time field.  sort [-limit N] name1 [-desc] [, name2 [-desc], ... nameN [-desc]] [by groupfield1, [groupfield2, ...]]     Parameter  Description  Required?      -limit  Sort only the first n points that arrive; additional points will be dropped.  To limit the output after sorting, use  head  or  tail  after sort.   No    nameN  The name of a field by which to sort.  First the points are compared based on their name1 fields, then any ties among name1 fields are broken by comparing name2 fields, then any points whose name1 and name2 are the same are compared in their name3 field, and so forth.  Name1 is required; the rest are optional    -desc  If you put \"-desc\" after the name of a field, the sort on that field will be in descending order; otherwise, it will be in ascending order.  No    by  One or more fields by which to group. See  Grouping fields with by .  No     Ordering is numeric (for number values) or alphabetical (for string\nvalues). It also supports multi-key sorting in ascending or descending\norder for alphabetical and numerical comparisons.    Note:  Sorting is limited to 10,000 data points by default. To sort more points, use the -limit flag to set a higher value.  Example: Output a table of sorted random values  emit -from :0: \n| put value = Math.random() \n| sort value \n| view table", 
            "title": "sort"
        }, 
        {
            "location": "/processors/split/", 
            "text": "split\n\n\nSplit each incoming point, emitting one new point for each of the\nspecified fields in the original point.\n\n\nsplit splitfield1,splitfield2,...\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nsplitfield\n\n\nOne or more fields on which to split the incoming points. \nOn each incoming point, split emits one point for each split field. In other words, if there are N split fields, then split emits N points for each incoming point.  \n\n\nNo; the default splits on all fields except the name, value, and time fields.\n\n\n\n\n\n\n-arrays\n\n\nSet this to \"0\" to prohibit splitting array-valued fields. \nBy default, when split encounters an array-valued split field on a point, it splits the array into individual values and generates a point for each of those values. \n\n\nNo; default is 1\n\n\n\n\n\n\n\n\nThe resulting split points are constructed by adding a field called\n\"name\" with the name of the split field and a field called \"value\" with\nthe value of the split field. All other fields are copied to the new\npoints. If you specify only one split field, then the output from each\ndata point is a single data point with the \"name\" and \"value\" fields set\nto the specified split field. And if a split field contains an array,\nthen the output is exploded. See the examples below.\n\n\nExample: Split a single point on two fields\n\n\nemit -limit 1 \n| put A = 5, B = 10, X = 'hello', Y = 'world', Z = 'boo' \n| split A,B \n| view text\n\n\n\n\nGiven this data point:\n\n\n\n\n{ A: 5, B: 10, X: 'hello', Y: 'world', Z: 'boo', time: t }  \n\n\n\n\nYields these two data points as output:\n\n\n\n\n{\"time\": t, \"X\":\"hello\", \"Y\":\"world\", \"Z\":\"boo\", \"name\":\"A\", \"value\":5}\n\n{\"time\": t, \"X\":\"hello\", \"Y\":\"world\", \"Z\":\"boo\", \"name\":\"B\", \"value\":10}  \n\n\n\n\nSplit with no options\n\n\nemit -limit 1 \n| put name = \nNAME\n, value = \nVALUE\n, wheels = 5, shape = \nBENT\n \n| split \n| view text\n\n\n\n\nYields the following output:\n\n\n\n\n{\"time\": t, \"name\":\"value\", \"value\":\"VALUE\"}\n\n{\"time\": t, \"name\":\"wheels\", \"value\":5}\n\n{\"time\": t, \"name\":\"shape\", \"value\":\"BENT\"}  \n\n\n\n\nExplode an array\n\n\nemit -limit 1 \n| put a = [1,2,3], b = \nfoo\n \n| split a,b\n| view text\n\n\n\n\nYields the following output:\n\n\n\n\n{\"time\": t, \"name\":\"a\", \"value\":1}\n\n{\"time\": t, \"name\":\"a\", \"value\":2}\n\n{\"time\": t, \"name\":\"a\", \"value\":3}\n\n{\"time\": t, \"name\":\"b\", \"value\":\"foo\"}", 
            "title": "____ split"
        }, 
        {
            "location": "/processors/split/#split", 
            "text": "Split each incoming point, emitting one new point for each of the\nspecified fields in the original point.  split splitfield1,splitfield2,...     Parameter  Description  Required?      splitfield  One or more fields on which to split the incoming points.  On each incoming point, split emits one point for each split field. In other words, if there are N split fields, then split emits N points for each incoming point.    No; the default splits on all fields except the name, value, and time fields.    -arrays  Set this to \"0\" to prohibit splitting array-valued fields.  By default, when split encounters an array-valued split field on a point, it splits the array into individual values and generates a point for each of those values.   No; default is 1     The resulting split points are constructed by adding a field called\n\"name\" with the name of the split field and a field called \"value\" with\nthe value of the split field. All other fields are copied to the new\npoints. If you specify only one split field, then the output from each\ndata point is a single data point with the \"name\" and \"value\" fields set\nto the specified split field. And if a split field contains an array,\nthen the output is exploded. See the examples below.  Example: Split a single point on two fields  emit -limit 1 \n| put A = 5, B = 10, X = 'hello', Y = 'world', Z = 'boo' \n| split A,B \n| view text  Given this data point:   { A: 5, B: 10, X: 'hello', Y: 'world', Z: 'boo', time: t }     Yields these two data points as output:   {\"time\": t, \"X\":\"hello\", \"Y\":\"world\", \"Z\":\"boo\", \"name\":\"A\", \"value\":5} \n{\"time\": t, \"X\":\"hello\", \"Y\":\"world\", \"Z\":\"boo\", \"name\":\"B\", \"value\":10}     Split with no options  emit -limit 1 \n| put name =  NAME , value =  VALUE , wheels = 5, shape =  BENT  \n| split \n| view text  Yields the following output:   {\"time\": t, \"name\":\"value\", \"value\":\"VALUE\"} \n{\"time\": t, \"name\":\"wheels\", \"value\":5} \n{\"time\": t, \"name\":\"shape\", \"value\":\"BENT\"}     Explode an array  emit -limit 1 \n| put a = [1,2,3], b =  foo  \n| split a,b\n| view text  Yields the following output:   {\"time\": t, \"name\":\"a\", \"value\":1} \n{\"time\": t, \"name\":\"a\", \"value\":2} \n{\"time\": t, \"name\":\"a\", \"value\":3} \n{\"time\": t, \"name\":\"b\", \"value\":\"foo\"}", 
            "title": "split"
        }, 
        {
            "location": "/processors/tail/", 
            "text": "tail\n\n\nOnly emit the last limit points from each batch.\n\n\ntail limit [by field1, [field2, ...]]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nlimit\n\n\nThe number of points to emit; may be an expression that evaluates to an integer\n\n\nNo; defaults to 1\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nExample: Generate batches containing 10 points each, but only log the last 2 points from each batch\n\n\nemit -hz 10 -limit 100 \n| batch 1\n| tail 2 \n| view text", 
            "title": "____ tail"
        }, 
        {
            "location": "/processors/tail/#tail", 
            "text": "Only emit the last limit points from each batch.  tail limit [by field1, [field2, ...]]     Parameter  Description  Required?      limit  The number of points to emit; may be an expression that evaluates to an integer  No; defaults to 1    by  One or more fields by which to group. See  Grouping fields with by .  No     Example: Generate batches containing 10 points each, but only log the last 2 points from each batch  emit -hz 10 -limit 100 \n| batch 1\n| tail 2 \n| view text", 
            "title": "tail"
        }, 
        {
            "location": "/processors/unbatch/", 
            "text": "unbatch\n\n\nTurn a batched stream into an unbatched stream.\n\n\nunbatch\n\n\n\n\nThis processor has no arguments.\n\n\nExample: Emit 20 points, batch every five seconds, count the result, then unbatch and count again\n\n\nemit -from :2015-01-01: -limit 20\n| batch :5s: \n| reduce count()\n| unbatch \n| reduce count()\n| view text", 
            "title": "____ unbatch"
        }, 
        {
            "location": "/processors/unbatch/#unbatch", 
            "text": "Turn a batched stream into an unbatched stream.  unbatch  This processor has no arguments.  Example: Emit 20 points, batch every five seconds, count the result, then unbatch and count again  emit -from :2015-01-01: -limit 20\n| batch :5s: \n| reduce count()\n| unbatch \n| reduce count()\n| view text", 
            "title": "unbatch"
        }, 
        {
            "location": "/processors/uniq/", 
            "text": "uniq\n\n\nCompare adjacent points and discard any duplicates, much like the UNIX\nuniq command.\n\n\nuniq [field1, ... fieldN] [by groupfield1, ... groupfieldN]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfieldN\n\n\nOne or more fields to check for changes. \nIf all of a point's fieldN values are the same as on the previous point, the point is discarded as being a duplicate. \n\n\nNo; the default is all fields other than time\n\n\n\n\n\n\nby\n\n\nOne or more fields by which to group. See \nGrouping fields with by\n.\n\n\nNo\n\n\n\n\n\n\n\n\nExample: Only output points where the random value of y is unique\n\n\nemit -from :0: -limit 10\n| put x = Math.random() * 3\n| put y = Math.floor(x)\n| uniq\n| skip 1\n| view text", 
            "title": "____ uniq"
        }, 
        {
            "location": "/processors/uniq/#uniq", 
            "text": "Compare adjacent points and discard any duplicates, much like the UNIX\nuniq command.  uniq [field1, ... fieldN] [by groupfield1, ... groupfieldN]     Parameter  Description  Required?      fieldN  One or more fields to check for changes.  If all of a point's fieldN values are the same as on the previous point, the point is discarded as being a duplicate.   No; the default is all fields other than time    by  One or more fields by which to group. See  Grouping fields with by .  No     Example: Only output points where the random value of y is unique  emit -from :0: -limit 10\n| put x = Math.random() * 3\n| put y = Math.floor(x)\n| uniq\n| skip 1\n| view text", 
            "title": "uniq"
        }, 
        {
            "location": "/sinks/", 
            "text": "Sinks\n\n\nFinal nodes of Juttle flowgraphs are referred to as sinks.\n\n\n(source; source) | processor | (processor; processor) | processor | (sink; sink)\n\n\n\n\nview\n \n\n\nVisualize data using the view sink with built-in views \nview table\n, \nview text\n, or charts available via juttle-viz library.\n\n\nwrite\n\n\nSend data out using the write sink with built-in adapters \nwrite file\n, \nwrite http\n, or plug in external \nadapters\n to write to remote backends.", 
            "title": "Sinks"
        }, 
        {
            "location": "/sinks/#sinks", 
            "text": "Final nodes of Juttle flowgraphs are referred to as sinks.  (source; source) | processor | (processor; processor) | processor | (sink; sink)  view    Visualize data using the view sink with built-in views  view table ,  view text , or charts available via juttle-viz library.  write  Send data out using the write sink with built-in adapters  write file ,  write http , or plug in external  adapters  to write to remote backends.", 
            "title": "Sinks"
        }, 
        {
            "location": "/sinks/view/", 
            "text": "view\n\n\nThe view sink syntax includes the keyword \nview\n followed by a particular view name.\n\n\n... | view \nview\n [view-parameters]\n\n\n\n\nThe built-in views available in the Juttle CLI are \nview table\n and \nview text\n.\n\n\nThe \njuttle-viz\n package provides a variety of customizable charts for output in the browser, see \nbelow\n.\n\n\nIf your program doesn't specify an output, the default is \nview table\n.\n\n\nA program can have multiple view sinks; their visual layout will depend on the visualization solution.\n\n\nCLI Views\n\n\ntable\n\n\nDisplay the output as text in rows and columns. \nThis is the default output if no other is specified.\n\n\ntext\n\n\nDisplay a raw dump of the output in a fixed-width, console-style font.\n\n\nBrowser Charts\n\n\nThese charts are supplied by the \njuttle-viz charting library\n which seamlessly integrates with Juttle.\n\n\nTo use these charts, follow installation instructions for juttle-viz, or try out the integrated Juttle analytics environment \noutrigger\n.\n\n\n\n\n\n\n\n\nSink\n\n\nDescription\n\n\nImage\n\n\n\n\n\n\n\n\n\n\nbarchart\n\n\nDisplay the output as vertical or horizontal bars for comparing different categories of data.\n\n\n\n\n\n\n\n\nevents\n\n\nOverlay events on top of a time chart, with a tooltip whose format can be customized.\n\n\n\n\n\n\n\n\nless\n\n\nView the \ntime\nand \nmessage\nfield values of points. You can search and navigate as you would with the UNIX less command.\n\n\n\n\n\n\n\n\ntext\n\n\nDisplay a raw dump of the output in a fixed-width, console-style font.\n\n\n\n\n\n\n\n\npiechart\n\n\nRender data as a pie chart, showing proportions of a whole.\n\n\n\n\n\n\n\n\nscatterchart\n\n\nPlot data points as individual dots across two axes sourced from the data fields.\n\n\n\n\n\n\n\n\ntable\n\n\nDisplay the output as text in rows and columns. This is the default output if no other is specified.\n\n\n\n\n\n\n\n\ntile\n\n\nRender a metric tile displaying exactly one value.\n\n\n\n\n\n\n\n\ntimechart\n\n\nCreate a time series chart. Time charts support multiple series and can be combined with \nview events\n\n\n\n\n\n\n\n\n\n\nDefining View Parameters\n\n\nParameters for a view can be specified individually,\nor as object literals using the \n-options\n parameter (\n-o\n for short).\n\n\nIndividually specified parameters are shown in the syntax reference for each view.\nA simple example with individual parameters looks like this:\n\n\n... | view barchart\n        -title \nCPU usage\n\n        -value value\n\n\n\n\nThe example above can also be expressed with object literals like this:\n\n\n... | view barchart -options { title: \nCPU usage\n, value: value }\n\n\n\n\nThe two formats can also be combined, like this:\n\n\n... | view barchart\n        -title \nCPU usage\n\n        -o { value: value }\n\n\n\n\nIf you've worked with JavaScript, the \n-options\n method will look familiar. It\nprovides additional flexibility by allowing you to store parameters as\nvars or consts, like this:\n\n\nconst timechartOptions = {\ntitle: 'Average CPU Usage'\n};\n... | view timechart -options timechartOptions\n\n\n\n\nThey can also be defined in a module and referenced in another program:\n\n\n// module \nstandards\n\nexport const cpu_chart_params = {\n  series: [ { field: 'cpu', color: 'blue', label: 'cpu usage', unit: 'percent' } ],\n  ...\n};\n\n// main program:\nimport \nstandards\n as standards;\n... | view timechart\n        -options standards.cpu_chart_params\n        -title \ncpu usage\n\n\n\n\n\n \nNote:\n If a parameter is specified more\nthan once, the last instance overrides any previous instances. For\nexample, both of these imaginary programs produce a time chart whose\ntitle is \"the real title\":\n\n\n... | view timechart\n        -title \nignored\n\n        -o { title: \nthe real title\n }\n... | view timechart\n        -o { title: \nnot the title\n }\n        -title \nthe real title", 
            "title": "____ view"
        }, 
        {
            "location": "/sinks/view/#view", 
            "text": "The view sink syntax includes the keyword  view  followed by a particular view name.  ... | view  view  [view-parameters]  The built-in views available in the Juttle CLI are  view table  and  view text .  The  juttle-viz  package provides a variety of customizable charts for output in the browser, see  below .  If your program doesn't specify an output, the default is  view table .  A program can have multiple view sinks; their visual layout will depend on the visualization solution.", 
            "title": "view"
        }, 
        {
            "location": "/sinks/view/#cli-views", 
            "text": "table  Display the output as text in rows and columns.  This is the default output if no other is specified.  text  Display a raw dump of the output in a fixed-width, console-style font.", 
            "title": "CLI Views"
        }, 
        {
            "location": "/sinks/view/#browser-charts", 
            "text": "These charts are supplied by the  juttle-viz charting library  which seamlessly integrates with Juttle.  To use these charts, follow installation instructions for juttle-viz, or try out the integrated Juttle analytics environment  outrigger .     Sink  Description  Image      barchart  Display the output as vertical or horizontal bars for comparing different categories of data.     events  Overlay events on top of a time chart, with a tooltip whose format can be customized.     less  View the  time and  message field values of points. You can search and navigate as you would with the UNIX less command.     text  Display a raw dump of the output in a fixed-width, console-style font.     piechart  Render data as a pie chart, showing proportions of a whole.     scatterchart  Plot data points as individual dots across two axes sourced from the data fields.     table  Display the output as text in rows and columns. This is the default output if no other is specified.     tile  Render a metric tile displaying exactly one value.     timechart  Create a time series chart. Time charts support multiple series and can be combined with  view events", 
            "title": "Browser Charts"
        }, 
        {
            "location": "/sinks/view/#defining-view-parameters", 
            "text": "Parameters for a view can be specified individually,\nor as object literals using the  -options  parameter ( -o  for short).  Individually specified parameters are shown in the syntax reference for each view.\nA simple example with individual parameters looks like this:  ... | view barchart\n        -title  CPU usage \n        -value value  The example above can also be expressed with object literals like this:  ... | view barchart -options { title:  CPU usage , value: value }  The two formats can also be combined, like this:  ... | view barchart\n        -title  CPU usage \n        -o { value: value }  If you've worked with JavaScript, the  -options  method will look familiar. It\nprovides additional flexibility by allowing you to store parameters as\nvars or consts, like this:  const timechartOptions = {\ntitle: 'Average CPU Usage'\n};\n... | view timechart -options timechartOptions  They can also be defined in a module and referenced in another program:  // module  standards \nexport const cpu_chart_params = {\n  series: [ { field: 'cpu', color: 'blue', label: 'cpu usage', unit: 'percent' } ],\n  ...\n};\n\n// main program:\nimport  standards  as standards;\n... | view timechart\n        -options standards.cpu_chart_params\n        -title  cpu usage     Note:  If a parameter is specified more\nthan once, the last instance overrides any previous instances. For\nexample, both of these imaginary programs produce a time chart whose\ntitle is \"the real title\":  ... | view timechart\n        -title  ignored \n        -o { title:  the real title  }\n... | view timechart\n        -o { title:  not the title  }\n        -title  the real title", 
            "title": "Defining View Parameters"
        }, 
        {
            "location": "/sinks/view_text/", 
            "text": "text\n\n\nDisplay a raw dump of the output, optionally as JSON or CSV.\n\n\nview text -o {\n   format: 'raw|json|jsonl|csv',\n   limit: n,\n   indent: n\n}\n\n\n\n\nor\n\n\nview text\n  -format 'raw|json|jsonl|csv'\n  -limit n\n  -indent n\n\n\n\n\nSee \nDefining view parameters\n for an explanation of how sink parameters can be expressed as object literals.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-format\n\n\nYou can set this to 'csv', 'json', or 'jsonl' to get CSV or pretty-printed JSON / JSON Lines: output, respectively. Batch delimiters are ignored when 'json', 'jsonl', or 'csv' is specified. The 'indent' is ignored unless 'json' is specified.\n\n\nNo; default is 'json'\n\n\n\n\n\n\n-limit\n\n\nThe maximum number of data points to display\n\n\nNo; default is complete output of the flowgraph\n\n\n\n\n\n\n-indent\n\n\nAn optional number specifying indentation size for pretty-printed JSON output\n\n\nNo; defaults to JSON with one data point per line\n\n\n\n\n\n\n\n\nExample: view raw output\n\n\nemit -limit 5 -every :0.1s:\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'raw'\n\n\n\n\n{\ntime\n:\n2015-12-17T17:41:09.193Z\n,\nvalue\n:378}\n{\ntime\n:\n2015-12-17T17:41:09.293Z\n,\nvalue\n:414}\n{\ntime\n:\n2015-12-17T17:41:09.393Z\n,\nvalue\n:301}\n{\ntime\n:\n2015-12-17T17:41:09.493Z\n,\nvalue\n:855}\n{\ntime\n:\n2015-12-17T17:41:09.593Z\n,\nvalue\n:508}\n==============================================================\n\n\n\n\nExample: view CSV\n\n\nemit -limit 5\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'csv'\n\n\n\n\ntime\n,\nvalue\n\n\n2015-12-17T17:41:52.326Z\n,\n870\n\n\n2015-12-17T17:41:53.326Z\n,\n12\n\n\n2015-12-17T17:41:54.326Z\n,\n332\n\n\n2015-12-17T17:41:55.326Z\n,\n453\n\n\n2015-12-17T17:41:56.326Z\n,\n581\n\n\n\n\n\nExample: view JSON\n\n\nemit -limit 5 -every :0.1s:\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'json'\n\n\n\n\n[\n{\ntime\n:\n2015-12-17T17:42:29.677Z\n,\nvalue\n:223},\n{\ntime\n:\n2015-12-17T17:42:29.777Z\n,\nvalue\n:341},\n{\ntime\n:\n2015-12-17T17:42:29.877Z\n,\nvalue\n:331},\n{\ntime\n:\n2015-12-17T17:42:29.977Z\n,\nvalue\n:787},\n{\ntime\n:\n2015-12-17T17:42:30.077Z\n,\nvalue\n:625}\n]", 
            "title": "____ view_text.md"
        }, 
        {
            "location": "/sinks/view_text/#text", 
            "text": "Display a raw dump of the output, optionally as JSON or CSV.  view text -o {\n   format: 'raw|json|jsonl|csv',\n   limit: n,\n   indent: n\n}  or  view text\n  -format 'raw|json|jsonl|csv'\n  -limit n\n  -indent n  See  Defining view parameters  for an explanation of how sink parameters can be expressed as object literals.     Parameter  Description  Required?      -format  You can set this to 'csv', 'json', or 'jsonl' to get CSV or pretty-printed JSON / JSON Lines: output, respectively. Batch delimiters are ignored when 'json', 'jsonl', or 'csv' is specified. The 'indent' is ignored unless 'json' is specified.  No; default is 'json'    -limit  The maximum number of data points to display  No; default is complete output of the flowgraph    -indent  An optional number specifying indentation size for pretty-printed JSON output  No; defaults to JSON with one data point per line     Example: view raw output  emit -limit 5 -every :0.1s:\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'raw'  { time : 2015-12-17T17:41:09.193Z , value :378}\n{ time : 2015-12-17T17:41:09.293Z , value :414}\n{ time : 2015-12-17T17:41:09.393Z , value :301}\n{ time : 2015-12-17T17:41:09.493Z , value :855}\n{ time : 2015-12-17T17:41:09.593Z , value :508}\n==============================================================  Example: view CSV  emit -limit 5\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'csv'  time , value  2015-12-17T17:41:52.326Z , 870  2015-12-17T17:41:53.326Z , 12  2015-12-17T17:41:54.326Z , 332  2015-12-17T17:41:55.326Z , 453  2015-12-17T17:41:56.326Z , 581   Example: view JSON  emit -limit 5 -every :0.1s:\n| put value = Math.round(Math.random() * 1000)\n| view text\n    -format 'json'  [\n{ time : 2015-12-17T17:42:29.677Z , value :223},\n{ time : 2015-12-17T17:42:29.777Z , value :341},\n{ time : 2015-12-17T17:42:29.877Z , value :331},\n{ time : 2015-12-17T17:42:29.977Z , value :787},\n{ time : 2015-12-17T17:42:30.077Z , value :625}\n]", 
            "title": "text"
        }, 
        {
            "location": "/sinks/view_table/", 
            "text": "table\n\n\nDisplay the output as text in rows and columns.\n\n\nThis is the default output if no other is specified.\n\n\nview table -o {\n   title: 'string',\n   limit: n,\n   columnOrder: 'col1',...'colN'\n}\n\n\n\n\n\nor\n\n\nview table -title 'string' -limit n -columnOrder 'col1',...'colN'\n\n\n\n\nSee \nDefining view parameters\n\nfor an explanation of how sink parameters can be expressed as object literals.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-title\n\n\nThe title of the table\n\n\nNo; defaults to no title\n\n\n\n\n\n\n-limit\n\n\nThe maximum number of total data points to display\n\n\nNo; defaults to the first 1000 to avoid consuming unbounded memory\n\n\n\n\n\n\n-progressive\n\n\nA boolean specifying whether the table should be displayed gradually as data arrives or all at once when the data stream ends\n\n\nNo; defaults to \ntrue\n\n\n\n\n\n\n-columnOrder\n\n\nAn array of field names specifying the order of the table columns from left to right. If the data stream includes unspecified fields, these are displayed to the right of the specified ones, in alphabetical order.\n\n\nNo; default is 'time','name','value' followed by the remaining columns in alphabetical order\n\n\n\n\n\n\n\n\nExample: Table with ordered columns\n\n\nemit -limit 10 -every :0.1s:\n| put a = count(), b = Math.random() + Math.floor(Math.random() * 10) \n| view table \n    -columnOrder 'a','b','time'", 
            "title": "____ view_table.md"
        }, 
        {
            "location": "/sinks/view_table/#table", 
            "text": "Display the output as text in rows and columns.  This is the default output if no other is specified.  view table -o {\n   title: 'string',\n   limit: n,\n   columnOrder: 'col1',...'colN'\n}  or  view table -title 'string' -limit n -columnOrder 'col1',...'colN'  See  Defining view parameters \nfor an explanation of how sink parameters can be expressed as object literals.     Parameter  Description  Required?      -title  The title of the table  No; defaults to no title    -limit  The maximum number of total data points to display  No; defaults to the first 1000 to avoid consuming unbounded memory    -progressive  A boolean specifying whether the table should be displayed gradually as data arrives or all at once when the data stream ends  No; defaults to  true    -columnOrder  An array of field names specifying the order of the table columns from left to right. If the data stream includes unspecified fields, these are displayed to the right of the specified ones, in alphabetical order.  No; default is 'time','name','value' followed by the remaining columns in alphabetical order     Example: Table with ordered columns  emit -limit 10 -every :0.1s:\n| put a = count(), b = Math.random() + Math.floor(Math.random() * 10) \n| view table \n    -columnOrder 'a','b','time'", 
            "title": "table"
        }, 
        {
            "location": "/sinks/write/", 
            "text": "write\n\n\nWrite sink uses \nadapters supported by Juttle\n to send data points generated by the preceding Juttle flowgraph to the remote backend, usually for permanent storage. The newly written points can then be queried from the backend via \nread \nadapter\n just like any other points in Juttle.\n\n\n... | write \nadapter\n adapter-options\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nadapter\n\n\nName of the adapter to connect to the backend.\n\n\nYes\n\n\n\n\n\n\nadapter-options\n\n\nOptions specific to writing to the chosen backend, handled by the adapter.\n\n\nSee adapter documentation\n\n\n\n\n\n\n\n\n As adapters don't support operations that delete data, care must be\ntaken when setting up a program ending in write. We recommend always testing\nyour Juttle program by ending it in \nview table\n or\n\nview text\n to see what data points it is\ngenerating, and replacing those outputs with \nwrite\n only when you are fully\nsatisfied with the result.\n\n\nExample: Write random values into the InfluxDB database\n\n\nemit\n| put value = Math.random()\n| write influxdb\n  -db \ntestdb\n\n  -measurement \nmymetric", 
            "title": "____ write"
        }, 
        {
            "location": "/sinks/write/#write", 
            "text": "Write sink uses  adapters supported by Juttle  to send data points generated by the preceding Juttle flowgraph to the remote backend, usually for permanent storage. The newly written points can then be queried from the backend via  read  adapter  just like any other points in Juttle.  ... | write  adapter  adapter-options     Parameter  Description  Required?      adapter  Name of the adapter to connect to the backend.  Yes    adapter-options  Options specific to writing to the chosen backend, handled by the adapter.  See adapter documentation      As adapters don't support operations that delete data, care must be\ntaken when setting up a program ending in write. We recommend always testing\nyour Juttle program by ending it in  view table  or view text  to see what data points it is\ngenerating, and replacing those outputs with  write  only when you are fully\nsatisfied with the result.  Example: Write random values into the InfluxDB database  emit\n| put value = Math.random()\n| write influxdb\n  -db  testdb \n  -measurement  mymetric", 
            "title": "write"
        }, 
        {
            "location": "/adapters/", 
            "text": "Adapters\n\n\nJuttle's \nread\n source and \nwrite\n sink integrate with remote backends via adapters. This enables a Juttle program to interact with various types of external systems, including databases, streaming data sources, object storage systems, network APIs, and more.\n\n\nBuilt In Adapters\n\n\nThese adapters are included in the Juttle runtime and can be used in Juttle programs directly without special configuration.\n\n\n\n\nfile\n\n\nhttp\n\n\nstochastic\n\n\n\n\nExternal Adapters\n\n\nThis is a partial list of adapters that are not included with the Juttle distribution but which can be optionally configured.\n\n\nSee the \nJuttle README\n for instructions on configuring external adapters to read from remote backends.\n\n\n\n\nelastic\n (ElasticSearch)\n\n\ngraphite\n (Graphite)\n\n\ninflux\n (InfluxDB)\n\n\nmysql\n(MySQL)\n\n\npostgres\n (PostgreSQL)\n\n\nsqlite\n (SQLite)\n\n\ntwitter\n (Twitter)", 
            "title": "Adapters"
        }, 
        {
            "location": "/adapters/#adapters", 
            "text": "Juttle's  read  source and  write  sink integrate with remote backends via adapters. This enables a Juttle program to interact with various types of external systems, including databases, streaming data sources, object storage systems, network APIs, and more.", 
            "title": "Adapters"
        }, 
        {
            "location": "/adapters/#built-in-adapters", 
            "text": "These adapters are included in the Juttle runtime and can be used in Juttle programs directly without special configuration.   file  http  stochastic", 
            "title": "Built In Adapters"
        }, 
        {
            "location": "/adapters/#external-adapters", 
            "text": "This is a partial list of adapters that are not included with the Juttle distribution but which can be optionally configured.  See the  Juttle README  for instructions on configuring external adapters to read from remote backends.   elastic  (ElasticSearch)  graphite  (Graphite)  influx  (InfluxDB)  mysql (MySQL)  postgres  (PostgreSQL)  sqlite  (SQLite)  twitter  (Twitter)", 
            "title": "External Adapters"
        }, 
        {
            "location": "/adapters/file/", 
            "text": "File Adapter\n\n\nThe \nfile\n adapter allows reading points from, or writing points to, a file on the local filesystem.\n\n\n\n\n\n\nFile Adapter\n\n\nread file\n\n\nwrite file\n\n\n\n\n\n\n\n\n\n\nread file\n\n\nSupported file formats are JSON array, and JSON lines; see examples below.\n\n\nread file -file \npath\n [-format \nformat\n] [-timeField \nfieldname\n]\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-file\n\n\nFile path on the local filesystem, absolute or relative to the current working directory\n\n\nYes\n\n\n\n\n\n\n-format\n\n\nInput file format: \ncsv\nfor \nCSV\n data, \njson\nfor \nJSON\n data, or \njsonl\nfor \nJSON lines\n data\n\n\nNo; defaults to \njson\n\n\n\n\n\n\n-timeField\n\n\nName of the field in the data which contains a valid timestamp\n\n\nNo; defaults to \ntime\n\n\n\n\n\n\n\n\nThe data is assumed to contain valid timestamps in a field named \ntime\n by default; a different name for the time field may be specified with \n-timeField\n option. If the data contains fields \ntime\n and another named field specified with \n-timeField\n, the original contents of field \ntime\n will be overwritten by the valid timestamp from \ntimeField\n. \n\n\nTimeless data, which contains no timestamps, is acceptable; however certain operations which expect time to be present in the points, such as \nreduce -every :interval:\n, will execute with warnings or error out. Timeless data can be joined in the Juttle flowgraph with other data which contains timestamps; a common use case for reading timeless data from a file or another backend is to join it with streaming data for enrichment.\n\n\nThe file adapter does not support any kind of filtering (neither filter expressions, nor full text search). In order to filter the data read from file, pipe to the \nfilter\n proc.\n\n\nExample: read from a JSON array data file\n\n\nThe source file has data in JSON array format:\n\n\n[ \n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n },\n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nwarn\n },\n{ \ntime\n: \n2015-11-06T04:28:42.405Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n },\n{ \ntime\n: \n2015-11-06T04:28:42.502Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nok\n }\n]\n\n\n\n\nThis program reads the file in, and filters on a specific hostname:\n\n\nread file -file 'docs/examples/datasets/input1.json'\n| filter hostname = 'lemoncake'\n| view table\n\n\n\n\nExample: read from a JSON lines data file\n\n\nThe source file has data in JSON lines format, i.e. newline separated JSON objects:\n\n\n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n }\n{ \ntime\n: \n2015-11-06T04:28:32.304Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nwarn\n }\n{ \ntime\n: \n2015-11-06T04:28:42.405Z\n, \nhostname\n: \nlemoncake\n, \nstate\n: \nok\n }\n{ \ntime\n: \n2015-11-06T04:28:42.502Z\n, \nhostname\n: \napplepie\n, \nstate\n: \nok\n }\n\n\n\n\nThis program reads the file in, and filters on a specific hostname:\n\n\nread file -file 'docs/examples/datasets/json_lines.jsonl'\n| filter hostname = 'lemoncake'\n| view table\n\n\n\n\nThe above examples produce the same output table:\n\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 hostname     \u2502 state    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-11-06T04:28:32.304Z           \u2502 lemoncake    \u2502 ok       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-11-06T04:28:42.405Z           \u2502 lemoncake    \u2502 ok       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nwrite file\n\n\nData is written out to a file as a JSON array; no other output formats are supported.\n\n\nwrite file -file \npath\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-file\n\n\nFile path on the local filesystem, absolute or relative to the current working directory\n\n\nYes\n\n\n\n\n\n\n\n\nIf the file already exists and contains a valid JSON array, the write will append new data rather than overwrite.\n\n\nExample: writing data to a file\n\n\nemit -from :2015-01-01: -limit 2 \n| put name = 'write_me', value = count() \n| write file -file '/tmp/write_me.json'\n\n\n\n\nThe resulting file \n/tmp/write_me.json\n contains:\n\n\n[\n    {\n        \ntime\n: \n2015-01-01T00:00:00.000Z\n,\n        \nname\n: \nwrite_me\n,\n        \nvalue\n: 1\n    },\n    {\n        \ntime\n: \n2015-01-01T00:00:01.000Z\n,\n        \nname\n: \nwrite_me\n,\n        \nvalue\n: 2\n    }\n]", 
            "title": "____ File Adapter"
        }, 
        {
            "location": "/adapters/file/#file-adapter", 
            "text": "The  file  adapter allows reading points from, or writing points to, a file on the local filesystem.    File Adapter  read file  write file", 
            "title": "File Adapter"
        }, 
        {
            "location": "/adapters/file/#read-file", 
            "text": "Supported file formats are JSON array, and JSON lines; see examples below.  read file -file  path  [-format  format ] [-timeField  fieldname ]     Parameter  Description  Required?      -file  File path on the local filesystem, absolute or relative to the current working directory  Yes    -format  Input file format:  csv for  CSV  data,  json for  JSON  data, or  jsonl for  JSON lines  data  No; defaults to  json    -timeField  Name of the field in the data which contains a valid timestamp  No; defaults to  time     The data is assumed to contain valid timestamps in a field named  time  by default; a different name for the time field may be specified with  -timeField  option. If the data contains fields  time  and another named field specified with  -timeField , the original contents of field  time  will be overwritten by the valid timestamp from  timeField .   Timeless data, which contains no timestamps, is acceptable; however certain operations which expect time to be present in the points, such as  reduce -every :interval: , will execute with warnings or error out. Timeless data can be joined in the Juttle flowgraph with other data which contains timestamps; a common use case for reading timeless data from a file or another backend is to join it with streaming data for enrichment.  The file adapter does not support any kind of filtering (neither filter expressions, nor full text search). In order to filter the data read from file, pipe to the  filter  proc.  Example: read from a JSON array data file  The source file has data in JSON array format:  [ \n{  time :  2015-11-06T04:28:32.304Z ,  hostname :  lemoncake ,  state :  ok  },\n{  time :  2015-11-06T04:28:32.304Z ,  hostname :  applepie ,  state :  warn  },\n{  time :  2015-11-06T04:28:42.405Z ,  hostname :  lemoncake ,  state :  ok  },\n{  time :  2015-11-06T04:28:42.502Z ,  hostname :  applepie ,  state :  ok  }\n]  This program reads the file in, and filters on a specific hostname:  read file -file 'docs/examples/datasets/input1.json'\n| filter hostname = 'lemoncake'\n| view table  Example: read from a JSON lines data file  The source file has data in JSON lines format, i.e. newline separated JSON objects:  {  time :  2015-11-06T04:28:32.304Z ,  hostname :  lemoncake ,  state :  ok  }\n{  time :  2015-11-06T04:28:32.304Z ,  hostname :  applepie ,  state :  warn  }\n{  time :  2015-11-06T04:28:42.405Z ,  hostname :  lemoncake ,  state :  ok  }\n{  time :  2015-11-06T04:28:42.502Z ,  hostname :  applepie ,  state :  ok  }  This program reads the file in, and filters on a specific hostname:  read file -file 'docs/examples/datasets/json_lines.jsonl'\n| filter hostname = 'lemoncake'\n| view table  The above examples produce the same output table:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                               \u2502 hostname     \u2502 state    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-11-06T04:28:32.304Z           \u2502 lemoncake    \u2502 ok       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2015-11-06T04:28:42.405Z           \u2502 lemoncake    \u2502 ok       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "read file"
        }, 
        {
            "location": "/adapters/file/#write-file", 
            "text": "Data is written out to a file as a JSON array; no other output formats are supported.  write file -file  path      Parameter  Description  Required?      -file  File path on the local filesystem, absolute or relative to the current working directory  Yes     If the file already exists and contains a valid JSON array, the write will append new data rather than overwrite.  Example: writing data to a file  emit -from :2015-01-01: -limit 2 \n| put name = 'write_me', value = count() \n| write file -file '/tmp/write_me.json'  The resulting file  /tmp/write_me.json  contains:  [\n    {\n         time :  2015-01-01T00:00:00.000Z ,\n         name :  write_me ,\n         value : 1\n    },\n    {\n         time :  2015-01-01T00:00:01.000Z ,\n         name :  write_me ,\n         value : 2\n    }\n]", 
            "title": "write file"
        }, 
        {
            "location": "/adapters/http/", 
            "text": "HTTP Adapter\n\n\nThe \nhttp\n adapter allows reading points from, or writing points to, an http endpoint.\n\n\n\n\n\n\nHTTP Adapter\n\n\nread http\n\n\nwrite http\n\n\n\n\n\n\n\n\n\n\nread http\n\n\nRead points by issuing an HTTP request and pushing the response into the Juttle flowgraph, with options:\n\n\nread http -url url\n          -method method\n          -headers headers\n          -body body\n          -timeField timeField\n          -includeHeaders true/false\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-url\n\n\nURL to issue the HTTP request at\n\n\nYes\n\n\n\n\n\n\n-method\n\n\nHTTP method to use\n\n\nNo; default: \nGET\n\n\n\n\n\n\n-headers\n\n\nheaders to attach to the HTTP request in the form \n{ key1: \"value1\", ..., keyN: \"valueN\" }\n\n\nNo; default: \n{}\n\n\n\n\n\n\n-body\n\n\nbody to send with the HTTP requests\n\n\nNo; default: \n{}\n\n\n\n\n\n\n-timeField\n\n\nThe name of the field to use as the time field \nThe specified field will be renamed to \ntime\nin the body of the HTTP request. If the points already contain a field called \ntime\n that field is overwritten. This is useful when the source data contains a time field whose values are not valid time stamps.\n\n\nNo; defaults to keeping the \ntime\nfield as is\n\n\n\n\n\n\n-includeHeaders\n\n\nWhen set to true the headers from the response are appended to each of the points in the response\n\n\nNo; default: \nfalse\n\n\n\n\n\n\n\n\nCurrently the \nread http\n adapter will automatically parse incoming data based off of the \ncontent-type\n header. Here are the currently supported content-types:\n\n\n* `text/csv`: for [CSV](https://tools.ietf.org/html/rfc4180) data\n* `application/json` for [JSON](https://tools.ietf.org/html/rfc7159) data\n* `application/json` for [JSON lines](http://jsonlines.org/) data\n\n\n\nExample\n\n\nExample of how to hit the Github Issues API and retrieve all issues from the\nbeginning of time on a specific repository in the correct chronological order:\n\n\n/*\n * List issues on github \n *\n * Input parameters:\n * \n *  owner: repository owner, ie 'nodejs'\n *  repo: repository name, ie 'node'\n *  page: which of the n pages of issues to display, default: 1\n *  perPage: how many results per page to pull, default 10\n *  state: which state of the bugs you're interested in, default 'all'\n * \n * For more information on the github Issues API, see: \n *  https://developer.github.com/v3/issues/ \n * \n */\n\ninput owner: text -default 'nodejs';\ninput repo: text -default 'node';\ninput page: number -default 1;\ninput perPage: number -default 10;\ninput state: text -default 'all'; \n\nread http -url \nhttps://api.github.com/repos/${owner}/${repo}/issues?page=${page}\nper_page=${perPage}\nstate=${state}\ndirection=asc\nsort=created\n\n          -headers { 'User-Agent': 'http-adapter-example' }\n| keep title, created_at, state, number\n| put time=Date.new(created_at)\n| view text\n\n\n\n\nwrite http\n\n\nWrite points out of the Juttle flowgraph by making an HTTP request, with options:\n\n\nwrite http -url url\n           -method method\n           -headers headers\n           -maxLength length\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-url\n\n\nURL to issue the HTTP request at\n\n\nYes\n\n\n\n\n\n\n-method\n\n\nHTTP method to use\n\n\nNo; default: \nPOST\n\n\n\n\n\n\n-headers\n\n\nheaders to attach to the HTTP request in the form \n{ key1: \"value1\", ..., keyN: \"valueN\" }\n\n\nNo; default: \n{}\n\n\n\n\n\n\n-maxLength\n\n\nmaximum payload length per HTTP request, as number of data points (not bytes) \nIf the number of data points out of the flowgraph exceeds \nmaxLength\nthen multiple HTTP requests will be sent.\n\n\nNo, default: 1 (each data point out of the flowgraph becomes one HTTP request)\n\n\n\n\n\n\n\n\nEach set of points passed along by the Juttle flowgraph will be placed into the body of an HTTP request, as defined by the specified options. The \nmaxLength\n parameter allows to dictate how many points to push at most with each HTTP request. On failure, a warning will be displayed.\n\n\nExample\n\n\nSimple example of using the \nhttp\n adapter to create gists in GitHub:\n\n\n/*\n * Create a Gist on Github\n *\n * Input parameters:\n * \n *  oauthToken: required  \n * \n * For more information on the github Issues API, see: \n *  https://developer.github.com/v3/issues/ \n * \n * Usage:\n * \n ./bin/juttle docs/examples/adapters/http_post_to_a_gist.juttle --input oauthToken=XXX\n * \n */\n\ninput oauthToken: text -default 'FILL_ME_IN';\n\nconst date = :now:;\n\nemit -limit 5\n| put value=count()\n| put description='Test Gist ${date} #${value}',\n      public=true,\n      files = {\n        'main.juttle' : {\n            'content': 'emit -limit 1 | put message \nhello world\n'\n        } \n      }\n| keep description, public, files\n| write http -url \nhttps://api.github.com/gists\n\n             -method 'POST'\n             -headers {\n                'User-Agent': 'http-adapter-example',\n                'Authorization': 'token ${oauthToken}'\n             }", 
            "title": "____ HTTP Adapter"
        }, 
        {
            "location": "/adapters/http/#http-adapter", 
            "text": "The  http  adapter allows reading points from, or writing points to, an http endpoint.    HTTP Adapter  read http  write http", 
            "title": "HTTP Adapter"
        }, 
        {
            "location": "/adapters/http/#read-http", 
            "text": "Read points by issuing an HTTP request and pushing the response into the Juttle flowgraph, with options:  read http -url url\n          -method method\n          -headers headers\n          -body body\n          -timeField timeField\n          -includeHeaders true/false     Parameter  Description  Required?      -url  URL to issue the HTTP request at  Yes    -method  HTTP method to use  No; default:  GET    -headers  headers to attach to the HTTP request in the form  { key1: \"value1\", ..., keyN: \"valueN\" }  No; default:  {}    -body  body to send with the HTTP requests  No; default:  {}    -timeField  The name of the field to use as the time field  The specified field will be renamed to  time in the body of the HTTP request. If the points already contain a field called  time  that field is overwritten. This is useful when the source data contains a time field whose values are not valid time stamps.  No; defaults to keeping the  time field as is    -includeHeaders  When set to true the headers from the response are appended to each of the points in the response  No; default:  false     Currently the  read http  adapter will automatically parse incoming data based off of the  content-type  header. Here are the currently supported content-types:  * `text/csv`: for [CSV](https://tools.ietf.org/html/rfc4180) data\n* `application/json` for [JSON](https://tools.ietf.org/html/rfc7159) data\n* `application/json` for [JSON lines](http://jsonlines.org/) data  Example  Example of how to hit the Github Issues API and retrieve all issues from the\nbeginning of time on a specific repository in the correct chronological order:  /*\n * List issues on github \n *\n * Input parameters:\n * \n *  owner: repository owner, ie 'nodejs'\n *  repo: repository name, ie 'node'\n *  page: which of the n pages of issues to display, default: 1\n *  perPage: how many results per page to pull, default 10\n *  state: which state of the bugs you're interested in, default 'all'\n * \n * For more information on the github Issues API, see: \n *  https://developer.github.com/v3/issues/ \n * \n */\n\ninput owner: text -default 'nodejs';\ninput repo: text -default 'node';\ninput page: number -default 1;\ninput perPage: number -default 10;\ninput state: text -default 'all'; \n\nread http -url  https://api.github.com/repos/${owner}/${repo}/issues?page=${page} per_page=${perPage} state=${state} direction=asc sort=created \n          -headers { 'User-Agent': 'http-adapter-example' }\n| keep title, created_at, state, number\n| put time=Date.new(created_at)\n| view text", 
            "title": "read http"
        }, 
        {
            "location": "/adapters/http/#write-http", 
            "text": "Write points out of the Juttle flowgraph by making an HTTP request, with options:  write http -url url\n           -method method\n           -headers headers\n           -maxLength length     Parameter  Description  Required?      -url  URL to issue the HTTP request at  Yes    -method  HTTP method to use  No; default:  POST    -headers  headers to attach to the HTTP request in the form  { key1: \"value1\", ..., keyN: \"valueN\" }  No; default:  {}    -maxLength  maximum payload length per HTTP request, as number of data points (not bytes)  If the number of data points out of the flowgraph exceeds  maxLength then multiple HTTP requests will be sent.  No, default: 1 (each data point out of the flowgraph becomes one HTTP request)     Each set of points passed along by the Juttle flowgraph will be placed into the body of an HTTP request, as defined by the specified options. The  maxLength  parameter allows to dictate how many points to push at most with each HTTP request. On failure, a warning will be displayed.  Example  Simple example of using the  http  adapter to create gists in GitHub:  /*\n * Create a Gist on Github\n *\n * Input parameters:\n * \n *  oauthToken: required  \n * \n * For more information on the github Issues API, see: \n *  https://developer.github.com/v3/issues/ \n * \n * Usage:\n *   ./bin/juttle docs/examples/adapters/http_post_to_a_gist.juttle --input oauthToken=XXX\n * \n */\n\ninput oauthToken: text -default 'FILL_ME_IN';\n\nconst date = :now:;\n\nemit -limit 5\n| put value=count()\n| put description='Test Gist ${date} #${value}',\n      public=true,\n      files = {\n        'main.juttle' : {\n            'content': 'emit -limit 1 | put message  hello world '\n        } \n      }\n| keep description, public, files\n| write http -url  https://api.github.com/gists \n             -method 'POST'\n             -headers {\n                'User-Agent': 'http-adapter-example',\n                'Authorization': 'token ${oauthToken}'\n             }", 
            "title": "write http"
        }, 
        {
            "location": "/adapters/stochastic/", 
            "text": "If you want to simulate real events or metrics data, try one of these\nstochastic source nodes:\n\n\n\n\n\n\nCDN\n - Simulate hosts and services in a content\n    distribution network (CDN), providing metric and event streams that vary\n    with demand throughout the day.\n\n\n\n\n\n\nLogs\n - Generate simulated streams of logs\n    as event points from a variety of sources, emitting live or historic points\n    using a configurable error rate.\n\n\n\n\n\n\nSearch cluster\n - Simulate a cluster\n    of search engine hosts. This source is similar to \nCDN\n    source\n, but with defaults that simulate\n    multiple hosts undergoing a denial-of-service attack.", 
            "title": "____ Stochastic Adapter"
        }, 
        {
            "location": "/adapters/stochastic_cdn/", 
            "text": "read stochastic -source 'cdn'\n\n\nSimulate hosts and services in a content\ndistribution network (CDN), providing metric and event streams that vary\nwith demand throughout the day.\n\n\nread stochastic -source 'cdn' source-params read-params\n\n\n\n\nDemo parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-every\n\n\nHistoric points are downsampled at the specified interval \nSee \nTime notation in Juttle\n for syntax information. \n\n\nNo; default is the same as \n-max_samples\n\n\n\n\n\n\n-max_samples\n\n\nHistoric points are downsampled at an interval calculated so that there are no more than int intervals\n\n\nNo; default is 100\n\n\n\n\n\n\n-nhosts\n\n\nThe number of hosts to simulate\n\n\nNo; the default is 1\n\n\n\n\n\n\n-pops\n\n\nA list of Point of Presence (PoP) names to use with -nhosts \nHost names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on. \n\n\nNo\n\n\n\n\n\n\n-host_names\n\n\nA list of host names to use, in which case \n-nhosts\nis ignored and the number of hosts equals the number of specified host names\n\n\nNo; when nhosts is more than 1, and -host_names is omitted, host names are randomly generated\n\n\n\n\n\n\n-service_names\n\n\nA list of service names to use, including one or more of the following:  \nsearch\nindex\nauthentication\n\n\nNo; the default is all three service names\n\n\n\n\n\n\n-host_capacities\n\n\nThe relative capacities of hosts. Daily CDN demand are divided among them by these proportions.\n\n\nNo\n\n\n\n\n\n\n-daily\n\n\nThe scale factor for daily demand wave. A value of n will max out n hosts during peak hours.\n\n\nNo\n\n\n\n\n\n\n-dos\n\n\nThe scale of DOS demand load on a host, between 0 and 1. A value of 1 will max out any host it hits.\n\n\nNo; the default is 0\n\n\n\n\n\n\n-dos_dur\n\n\nThe average time that an attack spends on a host before moving to another host \nSimulation times increase with this number; keep it to seconds or small minutes.\n\n\nNo; defaults to 15 seconds\n\n\n\n\n\n\n-dos_id\n\n\nThe customer ID of the DOS attacker\n\n\nNo; default is 13\n\n\n\n\n\n\n-dos_router\n\n\nThe method for selecting the next host for the DOS demand \nRoundrobin cycles through the host list, each for dos_dur seconds. markov is more interesting. \n\n\nNo\n\n\n\n\n\n\n-errp\n\n\nThe syslog error percentage at peak demand\n\n\nNo; defaults to 0.02\n\n\n\n\n\n\n-lpm\n\n\nThe average number of syslog lines per minute per host\n\n\nNo; defaults to 60\n\n\n\n\n\n\n-debug\n\n\n\"1\" to emit additional points with behind-the-scenes information\n\n\nNo\n\n\n\n\n\n\n\n\nRead parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-from\n\n\nStream points whose time stamps are greater than or equal to the specified moment \nSee \nTime notation in Juttle\n  for syntax information. \n\n\nRequired only if -to is present; defaults to :now:\n\n\n\n\n\n\n-to\n\n\nStream points whose time stamps are less than the specified moment, which is less than or equal to :now:  \nSee \nTime notation in Juttle\n for syntax information. \n:information_source: \nNote:\nTo stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.\n\n\nNo; defaults to forever\n\n\n\n\n\n\nfilter-expression\n\n\nA field comparison, where multiple terms are joined with AND, OR, or NOT \nSee \nFiltering\n for additional details.  \n\n\nNo\n\n\n\n\n\n\n\n\nCharacteristics of a simulated CDN\n\n\nEach host in the simulated CDN runs two services:\n\n\n\n\n\n\nAn index service. Demand for the index services is constant.\n\n\n\n\n\n\nAn auth service. Demand for the authentication service is constant.\n\n\n\n\n\n\nA search service. Search demand varies through the day (generally highest from noon\n   to six).\n\n\n\n\n\n\nThe default \"network\" is a single host experiencing the full daily\ndemand for the CDN. You can increase the number of hosts with \n-nhosts\n or \n-host_names\n. When nhosts is more than 1, host names are randomly generated, though you can provide an explicit list of names with the \n-host_names\n\noption. The daily demand is divided among these hosts in random\nproportions unless you specify an explicit list of \n-host_capacities\n. The daily demand level can be increased to keep all hosts busier with the \n-daily\n scale factor.\n\n\nIn addition to the daily demand cycle, you can enable a\nDenial-of-Service (DoS) agent that hops from host to host with blasts of\nrequests. When hosts are under high load, their services begin to\ndegrade and the frequency of error responses increases. Its demand\nfactor ranges from 0 to 1, configured with the \n-dos\n option . The default, 0, disables\nthe agent. You can also control how fast this load shifts between hosts\n(\n-dos_dur\n) and how predictable the shifts are (\n-dos_router\n).\n\n\nExample: Live and historical streams in a simulated CDN\n\n\nTo simulate live data, start the interval now and omit the \n-to\n parameter:\n\n\nread stochastic -source 'cdn' ... -from :now:\n\n\n\n\nTo simulate historical data, start the interval anytime in the past and\nspecify \n-to\n as either another past moment or \n:now:\n:\n\n\nread stochastic -source 'cdn' ... -from :past_moment: -to :now:\n\n\n\n\nTo combine historical and live data, start the interval in the past and\ndo not include the -to argument:\n\n\nread stochastic -source 'cdn' ... -from :2 days ago:\n\n\n\n\nWhen you combine historical and live data, all data from the \n-from\n moment until the present moment is delivered immediately. Then, all subsequent data is delivered in real\ntime.\n\n\nData points are produced each time the network is sampled:\n\n\n\n\nFor live data, a sample is emitted each second.\n\n\nFor historical data, 60 samples are emitted to cover the specified time range unless you provide a sampling interval with the \n-every\n parameter.\n\n\n\n\nFields in data points in a simulated CDN\n\n\nAll points include these fields:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntime :moment:\n\n\nThe point's time stamp\n\n\n\n\n\n\nhost hostname\n\n\nThe host that emitted this point\n\n\n\n\n\n\nservice index|search\n\n\nThe service type to which this point belongs; see \nCharacteristics of a simulated CDN\n\n\n\n\n\n\nsource_type metric|event\n\n\nThe data type for this point\n\n\n\n\n\n\nname string\n\n\nThe name of this data point\n\n\n\n\n\n\nvalue float\n\n\nThis data point's value\n\n\n\n\n\n\n\n\nIn metrics data streams generated by cdn source, the value of the value\nfield depends on the metric:\n\n\n\n\n\n\n\n\nMetric name\n\n\nValue field\n\n\n\n\n\n\n\n\n\n\nrequests\n\n\nThe number of HTTP requests for this service over this period\n\n\n\n\n\n\nresponses\n\n\nThe number of HTTP responses for this service \nThis metric type also includes a \ncode\nfield that is one of 200, 404, or 500. The \nvalue\nis the event count for this period.\n\n\n\n\n\n\nresponse_ms\n\n\nThe average response time for a request to this service in this period, in milliseconds\n\n\n\n\n\n\ncpu\n\n\nThe CPU load percentage for this host, 0..1\n\n\n\n\n\n\ndisk\n\n\nThe disk load percentage for this host, 0..1\n\n\n\n\n\n\n\n\nSimilarly, in events data streams generated by cdn source, the value of the\nvalue field depends on the event type:\n\n\n\n\n\n\n\n\nEvent type\n\n\nValue field\n\n\n\n\n\n\n\n\n\n\nserver_error\n\n\nFor server response events, only HTTP error events are emitted and \"value\" is the HTTP error code (500) \nAn additional \ncust_id\nfield contains the ID of the requestor.\n\n\n\n\n\n\nsyslog\n\n\nThe log message text\n\n\n\n\n\n\n\n\nExample: set up a CDN source using different options\n\n\nread stochastic -source 'cdn' -nhosts 3 -dos 0.5 -dos_dur :10 seconds:\n\n\n\n\nread stochastic -source 'cdn' -host_names [\nfenge\n, \ngeruth\n, \namled\n] -daily 3.0  name ~ '*cpu*'\n\n\n\n\nExample: historic query for a week of data on one host, with graphs\n\n\nread stochastic -source 'cdn' -from :1 week ago: -to :now: -source_type 'metric'\nname = 'requests' OR name = 'cpu' OR name = 'response_ms' OR name = 'disk'\n| (\n  filter service = 'search' AND name = 'requests'\n  | reduce requests_per_sec = avg(value) by host;\n\n  filter name = 'cpu'\n  | reduce cpu = avg(value) by host;\n\n  filter service = 'search' AND name = 'response_ms'\n  | reduce response_ms = avg(value) by host;\n\n  filter name = 'disk'\n  | reduce disk = avg(value) by host;\n)\n| join host\n| view table -title 'Host Metrics (Last Week)' -columnOrder [ 'host' ]\n\n\n\n\nExample: historic and real-time query of three hosts with some chaos from a DoS attack\n\n\nThe simulation renders the past data, then continues ticking along each second with a new sample.\n\n\nread stochastic -source 'cdn' -nhosts 3 -dos 0.5 -from :1 minute ago: -source_type 'metric'\n  name = 'requests' OR name = 'response_ms' OR name = 'responses' OR name = 'disk'\n| (\n  filter service = 'search' AND name = 'requests'\n  | reduce -every :10s: requests_per_sec = avg(value) by host;\n\n  filter name = 'cpu'\n  | reduce -every :10s: cpu = avg(value) by host;\n\n  filter service = 'search' AND name = 'response_ms'\n  | reduce -every :10s: response_ms = avg(value) by host;\n\n  filter name = 'disk'\n  | reduce -every :10s: disk = avg(value) by host;\n)\n| pass // required to make the following join work\n| join host", 
            "title": "____ CDN Source"
        }, 
        {
            "location": "/adapters/stochastic_cdn/#read-stochastic-source-cdn", 
            "text": "Simulate hosts and services in a content\ndistribution network (CDN), providing metric and event streams that vary\nwith demand throughout the day.  read stochastic -source 'cdn' source-params read-params", 
            "title": "read stochastic -source 'cdn'"
        }, 
        {
            "location": "/adapters/stochastic_cdn/#demo-parameters", 
            "text": "Parameter  Description  Required?      -every  Historic points are downsampled at the specified interval  See  Time notation in Juttle  for syntax information.   No; default is the same as  -max_samples    -max_samples  Historic points are downsampled at an interval calculated so that there are no more than int intervals  No; default is 100    -nhosts  The number of hosts to simulate  No; the default is 1    -pops  A list of Point of Presence (PoP) names to use with -nhosts  Host names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on.   No    -host_names  A list of host names to use, in which case  -nhosts is ignored and the number of hosts equals the number of specified host names  No; when nhosts is more than 1, and -host_names is omitted, host names are randomly generated    -service_names  A list of service names to use, including one or more of the following:   search index authentication  No; the default is all three service names    -host_capacities  The relative capacities of hosts. Daily CDN demand are divided among them by these proportions.  No    -daily  The scale factor for daily demand wave. A value of n will max out n hosts during peak hours.  No    -dos  The scale of DOS demand load on a host, between 0 and 1. A value of 1 will max out any host it hits.  No; the default is 0    -dos_dur  The average time that an attack spends on a host before moving to another host  Simulation times increase with this number; keep it to seconds or small minutes.  No; defaults to 15 seconds    -dos_id  The customer ID of the DOS attacker  No; default is 13    -dos_router  The method for selecting the next host for the DOS demand  Roundrobin cycles through the host list, each for dos_dur seconds. markov is more interesting.   No    -errp  The syslog error percentage at peak demand  No; defaults to 0.02    -lpm  The average number of syslog lines per minute per host  No; defaults to 60    -debug  \"1\" to emit additional points with behind-the-scenes information  No", 
            "title": "Demo parameters"
        }, 
        {
            "location": "/adapters/stochastic_cdn/#read-parameters", 
            "text": "Parameter  Description  Required?      -from  Stream points whose time stamps are greater than or equal to the specified moment  See  Time notation in Juttle   for syntax information.   Required only if -to is present; defaults to :now:    -to  Stream points whose time stamps are less than the specified moment, which is less than or equal to :now:   See  Time notation in Juttle  for syntax information.  :information_source:  Note: To stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.  No; defaults to forever    filter-expression  A field comparison, where multiple terms are joined with AND, OR, or NOT  See  Filtering  for additional details.    No", 
            "title": "Read parameters"
        }, 
        {
            "location": "/adapters/stochastic_cdn/#characteristics-of-a-simulated-cdn", 
            "text": "Each host in the simulated CDN runs two services:    An index service. Demand for the index services is constant.    An auth service. Demand for the authentication service is constant.    A search service. Search demand varies through the day (generally highest from noon\n   to six).    The default \"network\" is a single host experiencing the full daily\ndemand for the CDN. You can increase the number of hosts with  -nhosts  or  -host_names . When nhosts is more than 1, host names are randomly generated, though you can provide an explicit list of names with the  -host_names \noption. The daily demand is divided among these hosts in random\nproportions unless you specify an explicit list of  -host_capacities . The daily demand level can be increased to keep all hosts busier with the  -daily  scale factor.  In addition to the daily demand cycle, you can enable a\nDenial-of-Service (DoS) agent that hops from host to host with blasts of\nrequests. When hosts are under high load, their services begin to\ndegrade and the frequency of error responses increases. Its demand\nfactor ranges from 0 to 1, configured with the  -dos  option . The default, 0, disables\nthe agent. You can also control how fast this load shifts between hosts\n( -dos_dur ) and how predictable the shifts are ( -dos_router ).  Example: Live and historical streams in a simulated CDN  To simulate live data, start the interval now and omit the  -to  parameter:  read stochastic -source 'cdn' ... -from :now:  To simulate historical data, start the interval anytime in the past and\nspecify  -to  as either another past moment or  :now: :  read stochastic -source 'cdn' ... -from :past_moment: -to :now:  To combine historical and live data, start the interval in the past and\ndo not include the -to argument:  read stochastic -source 'cdn' ... -from :2 days ago:  When you combine historical and live data, all data from the  -from  moment until the present moment is delivered immediately. Then, all subsequent data is delivered in real\ntime.  Data points are produced each time the network is sampled:   For live data, a sample is emitted each second.  For historical data, 60 samples are emitted to cover the specified time range unless you provide a sampling interval with the  -every  parameter.", 
            "title": "Characteristics of a simulated CDN"
        }, 
        {
            "location": "/adapters/stochastic_cdn/#fields-in-data-points-in-a-simulated-cdn", 
            "text": "All points include these fields:     Field  Description      time :moment:  The point's time stamp    host hostname  The host that emitted this point    service index|search  The service type to which this point belongs; see  Characteristics of a simulated CDN    source_type metric|event  The data type for this point    name string  The name of this data point    value float  This data point's value     In metrics data streams generated by cdn source, the value of the value\nfield depends on the metric:     Metric name  Value field      requests  The number of HTTP requests for this service over this period    responses  The number of HTTP responses for this service  This metric type also includes a  code field that is one of 200, 404, or 500. The  value is the event count for this period.    response_ms  The average response time for a request to this service in this period, in milliseconds    cpu  The CPU load percentage for this host, 0..1    disk  The disk load percentage for this host, 0..1     Similarly, in events data streams generated by cdn source, the value of the\nvalue field depends on the event type:     Event type  Value field      server_error  For server response events, only HTTP error events are emitted and \"value\" is the HTTP error code (500)  An additional  cust_id field contains the ID of the requestor.    syslog  The log message text     Example: set up a CDN source using different options  read stochastic -source 'cdn' -nhosts 3 -dos 0.5 -dos_dur :10 seconds:  read stochastic -source 'cdn' -host_names [ fenge ,  geruth ,  amled ] -daily 3.0  name ~ '*cpu*'  Example: historic query for a week of data on one host, with graphs  read stochastic -source 'cdn' -from :1 week ago: -to :now: -source_type 'metric'\nname = 'requests' OR name = 'cpu' OR name = 'response_ms' OR name = 'disk'\n| (\n  filter service = 'search' AND name = 'requests'\n  | reduce requests_per_sec = avg(value) by host;\n\n  filter name = 'cpu'\n  | reduce cpu = avg(value) by host;\n\n  filter service = 'search' AND name = 'response_ms'\n  | reduce response_ms = avg(value) by host;\n\n  filter name = 'disk'\n  | reduce disk = avg(value) by host;\n)\n| join host\n| view table -title 'Host Metrics (Last Week)' -columnOrder [ 'host' ]  Example: historic and real-time query of three hosts with some chaos from a DoS attack  The simulation renders the past data, then continues ticking along each second with a new sample.  read stochastic -source 'cdn' -nhosts 3 -dos 0.5 -from :1 minute ago: -source_type 'metric'\n  name = 'requests' OR name = 'response_ms' OR name = 'responses' OR name = 'disk'\n| (\n  filter service = 'search' AND name = 'requests'\n  | reduce -every :10s: requests_per_sec = avg(value) by host;\n\n  filter name = 'cpu'\n  | reduce -every :10s: cpu = avg(value) by host;\n\n  filter service = 'search' AND name = 'response_ms'\n  | reduce -every :10s: response_ms = avg(value) by host;\n\n  filter name = 'disk'\n  | reduce -every :10s: disk = avg(value) by host;\n)\n| pass // required to make the following join work\n| join host", 
            "title": "Fields in data points in a simulated CDN"
        }, 
        {
            "location": "/adapters/stochastic_logs/", 
            "text": "read stochastic -source 'logs'\n\n\nGenerate simulated streams of logs as event\npoints from a variety of sources, emitting live or historic points using\na configurable error rate.`\n\n\nread stochastic -source 'logs' source-params read-params\n\n\n\n\nSource parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-logType\n\n\nOne of: \n \nsyslog\nA UNIX syslog generator with configurable error rate (-errp below)\n \ngit\nA stream of commit and merge messages with a configurable ratio (-mergep below)\n \nosx\nA Mac OSX system log\n \npix\nA Cisco router log\n\n\nNo; defaults to syslog\n\n\n\n\n\n\n-lpm\n\n\nThe average number of syslog lines per minute per host\n\n\nNo; defaults to 60\n\n\n\n\n\n\n-nhosts\n\n\nThe number of hosts to simulate\n\n\nNo; the default is 1\n\n\n\n\n\n\n-pops\n\n\nA list of Point of Presence (PoP) names to use with -nhosts \nHost names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on.  \n\n\nNo\n\n\n\n\n\n\n-host_names\n\n\nA list of host names to use, in which case -nhosts is ignored and the number of hosts equals the number of specified host names\n\n\nNo; when nhosts is more than 1, and -host_names is omitted, host names are randomly generated\n\n\n\n\n\n\n-errp\n\n\nThe syslog error percentage at peak demand\n\n\nNo; defaults to 0.02\n\n\n\n\n\n\n-mergep\n\n\nThe probability that a git commit message is a merge commit\n\n\nNo; defaults to 0.10\n\n\n\n\n\n\n\n\nRead parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-from\n\n\nStream points whose time stamps are greater than or equal to the specified moment \nSee \nTime notation in Juttle\n  for syntax information. \n\n\nRequired only if -to is present; defaults to :now:\n\n\n\n\n\n\n-to\n\n\nStream points whose time stamps are less than the specified moment, which is less than or equal to :now:  \nSee \nTime notation in Juttle\n for syntax information. \n:information_source: \nNote:\nTo stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.\n\n\nNo; defaults to forever\n\n\n\n\n\n\nfilter-expression\n\n\nA field comparison, where multiple terms are joined with AND, OR, or NOT \nSee \nFiltering\n for additional details.  \n\n\nNo\n\n\n\n\n\n\n\n\nExample: Different ways to use read stochastic -source 'logs'\n\n\nread stochastic -source 'logs'\n| view text\n\n\n\n\nread stochastic -source 'logs' -logType \ngit\n -mergep 0.5\n| view text\n\n\n\n\nread stochastic -source 'logs' -logType \nsyslog\n -errp 0.4\n| view text", 
            "title": "____ Logs Source"
        }, 
        {
            "location": "/adapters/stochastic_logs/#read-stochastic-source-logs", 
            "text": "Generate simulated streams of logs as event\npoints from a variety of sources, emitting live or historic points using\na configurable error rate.`  read stochastic -source 'logs' source-params read-params", 
            "title": "read stochastic -source 'logs'"
        }, 
        {
            "location": "/adapters/stochastic_logs/#source-parameters", 
            "text": "Parameter  Description  Required?      -logType  One of:    syslog A UNIX syslog generator with configurable error rate (-errp below)   git A stream of commit and merge messages with a configurable ratio (-mergep below)   osx A Mac OSX system log   pix A Cisco router log  No; defaults to syslog    -lpm  The average number of syslog lines per minute per host  No; defaults to 60    -nhosts  The number of hosts to simulate  No; the default is 1    -pops  A list of Point of Presence (PoP) names to use with -nhosts  Host names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on.    No    -host_names  A list of host names to use, in which case -nhosts is ignored and the number of hosts equals the number of specified host names  No; when nhosts is more than 1, and -host_names is omitted, host names are randomly generated    -errp  The syslog error percentage at peak demand  No; defaults to 0.02    -mergep  The probability that a git commit message is a merge commit  No; defaults to 0.10", 
            "title": "Source parameters"
        }, 
        {
            "location": "/adapters/stochastic_logs/#read-parameters", 
            "text": "Parameter  Description  Required?      -from  Stream points whose time stamps are greater than or equal to the specified moment  See  Time notation in Juttle   for syntax information.   Required only if -to is present; defaults to :now:    -to  Stream points whose time stamps are less than the specified moment, which is less than or equal to :now:   See  Time notation in Juttle  for syntax information.  :information_source:  Note: To stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.  No; defaults to forever    filter-expression  A field comparison, where multiple terms are joined with AND, OR, or NOT  See  Filtering  for additional details.    No     Example: Different ways to use read stochastic -source 'logs'  read stochastic -source 'logs'\n| view text  read stochastic -source 'logs' -logType  git  -mergep 0.5\n| view text  read stochastic -source 'logs' -logType  syslog  -errp 0.4\n| view text", 
            "title": "Read parameters"
        }, 
        {
            "location": "/adapters/stochastic_search_cluster/", 
            "text": "read stochastic -source 'srch_cluster'\n\n\nSimulate a cluster of search engine hosts.\nThis source is similar to \nread -source 'cdn'\n,\nbut with defaults that simulate multiple hosts undergoing a\ndenial-of-service attack.`\n\n\nread stochastic -source 'srch_cluster' source-params read-params\n\n\n\n\nSource parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-every\n\n\nHistoric points are downsampled at the specified interval \nSee \nTime notation in Juttle\n for syntax information. \n\n\nNo; default is the same as \n-max_samples\n\n\n\n\n\n\n-max_samples\n\n\nHistoric points are downsampled at an interval calculated so that there are no more than int intervals\n\n\nNo; default is 1000\n\n\n\n\n\n\n-nhosts\n\n\nThe number of hosts to simulate\n\n\nNo; the default is 5\n\n\n\n\n\n\n-pops\n\n\nA list of Point of Presence (PoP) names to use with -nhosts \nHost names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on. \n\n\nNo\n\n\n\n\n\n\n-host_names\n\n\nA list of host names to use, in which case -nhosts is ignored and the number of hosts equals the number of specified host names\n\n\nNo; when -nhosts is more than 1, and -host_names is omitted, then host names are randomly generated.\n\n\n\n\n\n\n-service_names\n\n\nA list of service names to use, including one or more of the following: \nsearch\nindex\nauthentication\n\n\nNo; the default is all three service names\n\n\n\n\n\n\n-host_capacities\n\n\nThe relative capacities of hosts \nDaily CDN demand are divided among them by these proportions.\n\n\nNo\n\n\n\n\n\n\n-daily\n\n\nThe scale factor for daily demand wave \nA value of n will max out n hosts during peak hours.\n\n\nNo\n\n\n\n\n\n\n-dos\n\n\nThe scale of DOS demand load on a host, between 0 and 1. A value of 1 will max out any host it hits.\n\n\nNo; the default is 0.7\n\n\n\n\n\n\n-dos_dur\n\n\nThe average time that an attack spends on a host before moving to another host \nSimulation times increase with this number; keep it to seconds or small minutes. \n\n\nNo; defaults to 15 seconds\n\n\n\n\n\n\n-dos_id\n\n\nThe customer ID of the DOS attacker\n\n\nNo; default is 13\n\n\n\n\n\n\n-dos_router\n\n\nThe method for selecting the next host for the DOS demand. Roundrobin cycles through the host list, each for dos_dur seconds. markov is more interesting.\n\n\nNo\n\n\n\n\n\n\n-errp\n\n\nThe syslog error percentage at peak demand\n\n\nNo; defaults to 0.02\n\n\n\n\n\n\n-lpm\n\n\nThe average number of syslog lines per minute per host\n\n\nNo; defaults to 60\n\n\n\n\n\n\n-debug\n\n\n\"1\" to emit additional points with behind-the-scenes information\n\n\nNo\n\n\n\n\n\n\n\n\nRead parameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n-from\n\n\nStream points whose time stamps are greater than or equal to the specified moment \nSee \nTime notation in Juttle\n  for syntax information. \n\n\nRequired only if -to is present; defaults to :now:\n\n\n\n\n\n\n-to\n\n\nStream points whose time stamps are less than the specified moment, which is less than or equal to :now:  \nSee \nTime notation in Juttle\n for syntax information. \n:information_source: \nNote:\nTo stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.\n\n\nNo; defaults to forever\n\n\n\n\n\n\nfilter-expression\n\n\nA field comparison, where multiple terms are joined with AND, OR, or NOT \nSee \nFiltering\n for additional details.  \n\n\nNo\n\n\n\n\n\n\n\n\nExample: Compare response times from the last two minutes to live streaming response times\n\n\n// Get data from 2 minutes ago and ingest live data and et response_ms metrics from our search demo data\n\nread stochastic -source 'srch_cluster' -from :2 minutes ago: name ='response_ms'\n| batch :2 seconds:   // Group into 2 second intervals\n| reduce p90 = percentile('value', .9) by service   // Calculate p90 response time\n| view timechart\n   -valueField 'p90'   // Plot data on a chart\n   -keyField 'service'   // Every service should be a line on the chart\n   -title 'Response time (ms) by Service'", 
            "title": "____ Search Cluster Source"
        }, 
        {
            "location": "/adapters/stochastic_search_cluster/#read-stochastic-source-srch_cluster", 
            "text": "Simulate a cluster of search engine hosts.\nThis source is similar to  read -source 'cdn' ,\nbut with defaults that simulate multiple hosts undergoing a\ndenial-of-service attack.`  read stochastic -source 'srch_cluster' source-params read-params", 
            "title": "read stochastic -source 'srch_cluster'"
        }, 
        {
            "location": "/adapters/stochastic_search_cluster/#source-parameters", 
            "text": "Parameter  Description  Required?      -every  Historic points are downsampled at the specified interval  See  Time notation in Juttle  for syntax information.   No; default is the same as  -max_samples    -max_samples  Historic points are downsampled at an interval calculated so that there are no more than int intervals  No; default is 1000    -nhosts  The number of hosts to simulate  No; the default is 5    -pops  A list of Point of Presence (PoP) names to use with -nhosts  Host names are generated by assigning numbers to PoPs in a round-robin fashion, such as pop1.1, pop2.2, pop1.3, pop1.4, and so on.   No    -host_names  A list of host names to use, in which case -nhosts is ignored and the number of hosts equals the number of specified host names  No; when -nhosts is more than 1, and -host_names is omitted, then host names are randomly generated.    -service_names  A list of service names to use, including one or more of the following:  search index authentication  No; the default is all three service names    -host_capacities  The relative capacities of hosts  Daily CDN demand are divided among them by these proportions.  No    -daily  The scale factor for daily demand wave  A value of n will max out n hosts during peak hours.  No    -dos  The scale of DOS demand load on a host, between 0 and 1. A value of 1 will max out any host it hits.  No; the default is 0.7    -dos_dur  The average time that an attack spends on a host before moving to another host  Simulation times increase with this number; keep it to seconds or small minutes.   No; defaults to 15 seconds    -dos_id  The customer ID of the DOS attacker  No; default is 13    -dos_router  The method for selecting the next host for the DOS demand. Roundrobin cycles through the host list, each for dos_dur seconds. markov is more interesting.  No    -errp  The syslog error percentage at peak demand  No; defaults to 0.02    -lpm  The average number of syslog lines per minute per host  No; defaults to 60    -debug  \"1\" to emit additional points with behind-the-scenes information  No", 
            "title": "Source parameters"
        }, 
        {
            "location": "/adapters/stochastic_search_cluster/#read-parameters", 
            "text": "Parameter  Description  Required?      -from  Stream points whose time stamps are greater than or equal to the specified moment  See  Time notation in Juttle   for syntax information.   Required only if -to is present; defaults to :now:    -to  Stream points whose time stamps are less than the specified moment, which is less than or equal to :now:   See  Time notation in Juttle  for syntax information.  :information_source:  Note: To stream live data only, omit -from and -to. To combine historical and live data, specify a -from value in the past and omit -to.  No; defaults to forever    filter-expression  A field comparison, where multiple terms are joined with AND, OR, or NOT  See  Filtering  for additional details.    No     Example: Compare response times from the last two minutes to live streaming response times  // Get data from 2 minutes ago and ingest live data and et response_ms metrics from our search demo data\n\nread stochastic -source 'srch_cluster' -from :2 minutes ago: name ='response_ms'\n| batch :2 seconds:   // Group into 2 second intervals\n| reduce p90 = percentile('value', .9) by service   // Calculate p90 response time\n| view timechart\n   -valueField 'p90'   // Plot data on a chart\n   -keyField 'service'   // Every service should be a line on the chart\n   -title 'Response time (ms) by Service'", 
            "title": "Read parameters"
        }, 
        {
            "location": "/reducers/", 
            "text": "Reducers\n\n\nJuttle reducers operate on batches of points, carrying\nout a running computation over values contained in the points, optionally over a \nmoving time window\n.\n\n\nReducers can be used as the right-hand side of\n\nput\n\nexpressions, in which case they compute a result over points in the\ncurrent batch as each new point arrives. They can also be used as the\nright-hand side of\n\nreduce\n\nexpressions, in which case they compute a result over points in the\ncurrent batch when the batch ends.\nFor options that govern all reducers, see\n\nput\n\nand\n\nreduce\n.\n\n\nJuttle comes with built-in reducers:\n\n\navg\n\n\nReturn the average of the values of a specified field.\n\n\ncount\n\n\nReturn the number of points in the stream, optionally filtering on a\nspecified field.\n\n\ncount_unique\n\n\nReturn the total number of unique values of a specified field throughout\nthe batch.\n\n\ndelta\n\n\nReturn the change in value of a field.\n\n\nfirst\n\n\nReturn the value of the specified field in the first point that contains it.\n\n\nlast\n\n\nReturn the value of the specified field for the last point that contains it.\n\n\nmad\n\n\nReturn the Median Absolute Deviation (MAD) of the value of the specified field.\n\n\nmax\n\n\nReturn the maximum value of the specified field from among all points containing that field.\n\n\nmin\n\n\nReturn the minimum value of the specified field from among all points\ncontaining that field.\n\n\npercentile\n\n\nReturn the p\nth\n percentile ranked value of the specified field.\n\n\npluck\n\n\nReturn an array of the values of a specified field in the batch.\n\n\nsigma\n\n\nReturn the standard deviation of the value of the specified field.\n\n\nsum\n\n\nReturn the sum of the values of the specified field throughout the batch.", 
            "title": "Reducers"
        }, 
        {
            "location": "/reducers/#reducers", 
            "text": "Juttle reducers operate on batches of points, carrying\nout a running computation over values contained in the points, optionally over a  moving time window .  Reducers can be used as the right-hand side of put \nexpressions, in which case they compute a result over points in the\ncurrent batch as each new point arrives. They can also be used as the\nright-hand side of reduce \nexpressions, in which case they compute a result over points in the\ncurrent batch when the batch ends.\nFor options that govern all reducers, see put \nand reduce .  Juttle comes with built-in reducers:  avg  Return the average of the values of a specified field.  count  Return the number of points in the stream, optionally filtering on a\nspecified field.  count_unique  Return the total number of unique values of a specified field throughout\nthe batch.  delta  Return the change in value of a field.  first  Return the value of the specified field in the first point that contains it.  last  Return the value of the specified field for the last point that contains it.  mad  Return the Median Absolute Deviation (MAD) of the value of the specified field.  max  Return the maximum value of the specified field from among all points containing that field.  min  Return the minimum value of the specified field from among all points\ncontaining that field.  percentile  Return the p th  percentile ranked value of the specified field.  pluck  Return an array of the values of a specified field in the batch.  sigma  Return the standard deviation of the value of the specified field.  sum  Return the sum of the values of the specified field throughout the batch.", 
            "title": "Reducers"
        }, 
        {
            "location": "/reducers/juttle_reducers_timewindows/", 
            "text": "Moving time windows\n\n\nReducers can operate over moving time windows.\n\n\nWith the \nreduce\n\nprocessor, three parameters are used to define a moving time window:\n\n-over\n, \n-every\n, and \n-on\n. With \nput\n,\nonly \n-over\n is used. The \n-over\n parameter is known as the window,\nbecause it specifies a moving window of points up to the moment at which\nthe reducer is run. The window may be greater or less than the interval\n\n-every\n, but is typically some multiple of \n-every\n.\n\n\nFor example, the snippet below emits a new point every hour at 15\nminutes past the hour, using all points received in the past 6 hours:\n\n\nreduce\n  -over :6 hours:\n  -every :hour:\n  -on :00:15:00: a = avg(foo)\n| ...\n\n\n\n\nIts first point will be at 15 minutes after the first hour in which data\narrives, and it will continue emitting points every hour at 15 minutes\npast the hour, each being the average of points received over the\npreceding 6 hours. It ignores any upstream batch marks, and does not\nemit any batch marks of its own.\n\n\nWhen an \n-over\n window is specified,\nThe optional parameters \n-from :moment:\n and \n-to :moment:\n\nallow you to explicitly state the range of time covered by the stream.\nFor example, if you were computing a trailing yearly average from points\nthat fall on the 15th of each month, \n-from :2013-01-01:\n and \n-to :2015-01-01:\n \nlet you specify that two full years of time is\nrepresented by these points. Without it, yearly averages involving the\nfirst month would not be produced, because the points themselves only\nspan part of a month. And if there was no data for the final months, the\n\n-to\n parameter would force results to be produced for those empty months.\n\n\nIf you want the reduce to be driven by an upstream batch processor, but\nwith a window of different size than the batch interval, specify \n\n-over\n without \n-every\n. reduce will emit a point when it\nreceives a batch mark, using points within the current window. In this\nuse, the batch triggers computation according to its interval, but the\n\n-over\n parameter controls which points are used by reduce in its computation.\n\n\nWhen using a put processor, only \n-over\n may be specified, since it runs on\neach point as it is received.", 
            "title": "____ Time Windowed Reducers"
        }, 
        {
            "location": "/reducers/juttle_reducers_timewindows/#moving-time-windows", 
            "text": "Reducers can operate over moving time windows.  With the  reduce \nprocessor, three parameters are used to define a moving time window: -over ,  -every , and  -on . With  put ,\nonly  -over  is used. The  -over  parameter is known as the window,\nbecause it specifies a moving window of points up to the moment at which\nthe reducer is run. The window may be greater or less than the interval -every , but is typically some multiple of  -every .  For example, the snippet below emits a new point every hour at 15\nminutes past the hour, using all points received in the past 6 hours:  reduce\n  -over :6 hours:\n  -every :hour:\n  -on :00:15:00: a = avg(foo)\n| ...  Its first point will be at 15 minutes after the first hour in which data\narrives, and it will continue emitting points every hour at 15 minutes\npast the hour, each being the average of points received over the\npreceding 6 hours. It ignores any upstream batch marks, and does not\nemit any batch marks of its own.  When an  -over  window is specified,\nThe optional parameters  -from :moment:  and  -to :moment: \nallow you to explicitly state the range of time covered by the stream.\nFor example, if you were computing a trailing yearly average from points\nthat fall on the 15th of each month,  -from :2013-01-01:  and  -to :2015-01-01:  \nlet you specify that two full years of time is\nrepresented by these points. Without it, yearly averages involving the\nfirst month would not be produced, because the points themselves only\nspan part of a month. And if there was no data for the final months, the -to  parameter would force results to be produced for those empty months.  If you want the reduce to be driven by an upstream batch processor, but\nwith a window of different size than the batch interval, specify  -over  without  -every . reduce will emit a point when it\nreceives a batch mark, using points within the current window. In this\nuse, the batch triggers computation according to its interval, but the -over  parameter controls which points are used by reduce in its computation.  When using a put processor, only  -over  may be specified, since it runs on\neach point as it is received.", 
            "title": "Moving time windows"
        }, 
        {
            "location": "/reducers/juttle_reducers_user-defined/", 
            "text": "User-defined reducers\n\n\nJuttle includes \nbuilt-in reducers\n,\nbut users can also define their own reducers by using the reducer keyword.\n\n\nA custom reducer is composed of two required functions and two optional\nfunctions:\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nupdate\n\n\nThis function is called once for each point in a stream.\n\n\nYes\n\n\n\n\n\n\nresult\n\n\nThis function is called at the end of a batch or stream in order to retrieve the final computed value from the reducer.\n\n\nYes\n\n\n\n\n\n\nexpire\n\n\nThis function is only needed if the reducer is to be used in a windowed reduce or put.\n\n\nNo\n\n\n\n\n\n\n\n\nIn addition, a reducer can define local variables that are used in its computation.\n\n\n\n\nTime windows with user-defined reducers\n\n\nIf a user-defined reducer is to be used with a \nmoving time window\n (in a \nreduce\n, or \nput -over\n), then it may include an optional expire function.\n\n\nAn expire function is called once for each point as it leaves the\nwindow. It is considered the \"opposite\" of the update function, and it\nshould undo what update did when the point entered the window.\n\n\nIt it not always possible to define an expire function (for example,\nthere is no way to write expire for max or min functions). An expire\nfunction should be though of as an optimization -- without it, the\ncomputation is redone for all points in the current window each time\nresult is called. The same result should be achieved whether or not\nexpire is defined.\n\n\n\n\nExample: this reducer counts the number of times a field has an odd value, and can be used with -over\n\n\nreducer count_odd(fieldname) {\n    var count = 0;\n\n    function update() {\n        if (*fieldname % 2 != 0) {\n            count = count + 1;\n        }\n    }\n    function expire() {\n        if (*fieldname % 2 != 0) {\n            count = count - 1;\n        }\n    }\n    function result() {\n        return count;\n    }\n}\n\nemit -from :-1d: -every :1m: -limit 100 | put value=count() \n| (\n  reduce -every :10m: count_odd_last10m = count_odd(value);\n  reduce -every :10m: -over :1h: count_odd_last1h = count_odd(value) \n  ) \n| join\n| view table\n\n\n\n\nExample: let's define an exponentially weighted moving average (EWMA) reducer\n\n\nreducer ewma(fieldname, alpha = 0.5) {\n var ma = 0;\n function update() {\n    ma = ma * (1 - alpha) + * fieldname * alpha;\n }\n function result() { \n    return ma; \n }\n}\nemit -limit 1 \n| put cnt = Math.random() * 10 \n| put ma_fast = ewma(cnt, 0.9), ma_slow = ewma(cnt, 0.1) \n| view table\n\n\n\n\n\n\n\n\nThe first line declares the reducer name and indicates that the\n    reducer takes two arguments: a field name (fieldname) and a\n    weighting value (alpha). The alpha argument is optional because it\n    is defined with a default value.\n\n\n\n\n\n\nThe second line defines a variable (ma) that we'll use to store the\n    running value of the moving average.\n\n\n\n\n\n\nThe update() function updates the moving average each time it is\n    invoked (for example, for each point passing through the reducer). \n\n\nSee \nField referencing\n\nfor an explanation of why we used the \n*\n operator to\nreference \n*fieldname\n.\n\n\n\n\n\n\nThe result() function simply returns that moving average.\n\n\n\n\n\n\nThen we have a flowgraph that emits a synthetic data point, sets the\n    cnt field to a random number, invokes the ewma reducer twice, and\n    outputs a point with two fields: ma_fast and ma_slow. Each is\n    computed over the values of the cnt field, with a different\n    alpha parameter.\n\n\nNote that ma_fast and ma_slow are computed independently, by\nseparate instances of the reducer. A single reducer instance is\ncreated for each assignment expression, and each reducer instance\nhas its own variables and state.\n\n\n\n\n\n\nThe alpha parameter is used by the reducer like a regular function\nargument. The fieldname parameter is used differently. Specifically, in\nthe update() function, the fieldname parameter is de-referenced to\nobtain the value of the field whose name was passed in via the fieldname\nparameter. This allows reducers to be used generally over arbitrary\nfield names, by having the user of a reducer specify which fields the\nreducer should use in its computation. In the above example, the field\nof interest is cnt, and is the first argument passed to the EWMA\nreducer.", 
            "title": "____ User-defined Reducers"
        }, 
        {
            "location": "/reducers/juttle_reducers_user-defined/#user-defined-reducers", 
            "text": "Juttle includes  built-in reducers ,\nbut users can also define their own reducers by using the reducer keyword.  A custom reducer is composed of two required functions and two optional\nfunctions:     Function  Description  Required?      update  This function is called once for each point in a stream.  Yes    result  This function is called at the end of a batch or stream in order to retrieve the final computed value from the reducer.  Yes    expire  This function is only needed if the reducer is to be used in a windowed reduce or put.  No     In addition, a reducer can define local variables that are used in its computation.", 
            "title": "User-defined reducers"
        }, 
        {
            "location": "/reducers/juttle_reducers_user-defined/#time-windows-with-user-defined-reducers", 
            "text": "If a user-defined reducer is to be used with a  moving time window  (in a  reduce , or  put -over ), then it may include an optional expire function.  An expire function is called once for each point as it leaves the\nwindow. It is considered the \"opposite\" of the update function, and it\nshould undo what update did when the point entered the window.  It it not always possible to define an expire function (for example,\nthere is no way to write expire for max or min functions). An expire\nfunction should be though of as an optimization -- without it, the\ncomputation is redone for all points in the current window each time\nresult is called. The same result should be achieved whether or not\nexpire is defined.   Example: this reducer counts the number of times a field has an odd value, and can be used with -over  reducer count_odd(fieldname) {\n    var count = 0;\n\n    function update() {\n        if (*fieldname % 2 != 0) {\n            count = count + 1;\n        }\n    }\n    function expire() {\n        if (*fieldname % 2 != 0) {\n            count = count - 1;\n        }\n    }\n    function result() {\n        return count;\n    }\n}\n\nemit -from :-1d: -every :1m: -limit 100 | put value=count() \n| (\n  reduce -every :10m: count_odd_last10m = count_odd(value);\n  reduce -every :10m: -over :1h: count_odd_last1h = count_odd(value) \n  ) \n| join\n| view table  Example: let's define an exponentially weighted moving average (EWMA) reducer  reducer ewma(fieldname, alpha = 0.5) {\n var ma = 0;\n function update() {\n    ma = ma * (1 - alpha) + * fieldname * alpha;\n }\n function result() { \n    return ma; \n }\n}\nemit -limit 1 \n| put cnt = Math.random() * 10 \n| put ma_fast = ewma(cnt, 0.9), ma_slow = ewma(cnt, 0.1) \n| view table    The first line declares the reducer name and indicates that the\n    reducer takes two arguments: a field name (fieldname) and a\n    weighting value (alpha). The alpha argument is optional because it\n    is defined with a default value.    The second line defines a variable (ma) that we'll use to store the\n    running value of the moving average.    The update() function updates the moving average each time it is\n    invoked (for example, for each point passing through the reducer).   See  Field referencing \nfor an explanation of why we used the  *  operator to\nreference  *fieldname .    The result() function simply returns that moving average.    Then we have a flowgraph that emits a synthetic data point, sets the\n    cnt field to a random number, invokes the ewma reducer twice, and\n    outputs a point with two fields: ma_fast and ma_slow. Each is\n    computed over the values of the cnt field, with a different\n    alpha parameter.  Note that ma_fast and ma_slow are computed independently, by\nseparate instances of the reducer. A single reducer instance is\ncreated for each assignment expression, and each reducer instance\nhas its own variables and state.    The alpha parameter is used by the reducer like a regular function\nargument. The fieldname parameter is used differently. Specifically, in\nthe update() function, the fieldname parameter is de-referenced to\nobtain the value of the field whose name was passed in via the fieldname\nparameter. This allows reducers to be used generally over arbitrary\nfield names, by having the user of a reducer specify which fields the\nreducer should use in its computation. In the above example, the field\nof interest is cnt, and is the first argument passed to the EWMA\nreducer.", 
            "title": "Time windows with user-defined reducers"
        }, 
        {
            "location": "/reducers/avg/", 
            "text": "avg\n\n\nReturn the average of the values of a specified field.\n\n\nput|reduce avg(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe name of the field to average\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the \nbatch\n processor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce avg(value) \n| view table \n  -title \nHistorical average\n\n  ;\nhistorical_points \n| batch 5 \n| reduce avg(value) \n| view table \n  -update \nappend\n \n  -title \nHistorical 5-second average\n\n  ;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 2 \n| reduce avg(value) \n| view table \n  -update \nappend\n \n  -title \nLive 2-second average\n\n  ;", 
            "title": "____ avg"
        }, 
        {
            "location": "/reducers/avg/#avg", 
            "text": "Return the average of the values of a specified field.  put|reduce avg(field)     Parameter  Description  Required?      field  The name of the field to average  Yes     Example  This example uses the  batch  processor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce avg(value) \n| view table \n  -title  Historical average \n  ;\nhistorical_points \n| batch 5 \n| reduce avg(value) \n| view table \n  -update  append  \n  -title  Historical 5-second average \n  ;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 2 \n| reduce avg(value) \n| view table \n  -update  append  \n  -title  Live 2-second average \n  ;", 
            "title": "avg"
        }, 
        {
            "location": "/reducers/count/", 
            "text": "count\n\n\nReturn the number of points in the stream, optionally filtering on a\nspecified field.\n\n\nput|reduce count(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field on which to filter \nIf field is specified, then count returns the number of points containing that field. If unspecified, it returns the total number of points received. \n\n\nNo\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses batching. See the\n\nbatch\n\nprocessor for details about batching.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce cnt = count() \n| view table -title \nHistorical total count\n\n;\nhistorical_points \n| batch 5 \n| reduce cnt = count(value) \n| view table \n    -update \nappend\n \n    -title \nHistorical 5-second count\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\nemit -limit 10 \n| put cnt = count(), value = Math.random() \n}\nlive_points \n| batch 2 \n| reduce cnt=count() \n| view table \n    -update 'append' \n    -title \nLive 2-second count", 
            "title": "____ count"
        }, 
        {
            "location": "/reducers/count/#count", 
            "text": "Return the number of points in the stream, optionally filtering on a\nspecified field.  put|reduce count(field)     Parameter  Description  Required?      field  The field on which to filter  If field is specified, then count returns the number of points containing that field. If unspecified, it returns the total number of points received.   No     Example  This example uses batching. See the batch \nprocessor for details about batching.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce cnt = count() \n| view table -title  Historical total count \n;\nhistorical_points \n| batch 5 \n| reduce cnt = count(value) \n| view table \n    -update  append  \n    -title  Historical 5-second count \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\nemit -limit 10 \n| put cnt = count(), value = Math.random() \n}\nlive_points \n| batch 2 \n| reduce cnt=count() \n| view table \n    -update 'append' \n    -title  Live 2-second count", 
            "title": "count"
        }, 
        {
            "location": "/reducers/count_unique/", 
            "text": "count_unique\n\n\nReturn the total number of unique values of a specified field throughout\nthe batch.\n\n\n \nNote:\n For efficiency, the back-end\nimplementation of count_unique() uses an approximation algorithm, which\nmay return imprecise values for sets with large cardinality\n\n\nput|reduce count_unique(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field for which to count unique values\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), num = Math.floor(Math.random() * 10), value = \nthing ${num}\n\n}\n\nhistorical_points \n| (\n  reduce uniq = count_unique(value) \n  | view table -title \nHistorical total count of unique values\n;\n\n  reduce cnt=count(value) by value \n  | view table -title \nHistorical list of unique values with counts\n;\n);\n\nhistorical_points \n| batch 5 \n| reduce uniq = count_unique(value) \n| view table -update \nappend\n -title \nHistorical 5-second count of unique values\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), num = Math.floor(Math.random() * 10), value = \nthing ${num}\n\n}\n\nlive_points \n| batch 2 \n| (\n  reduce uniq = count_unique(value), cnt = null, value = null;\n  reduce cnt = count(value), uniq = null by value\n)\n| view table \n  -update \nappend\n \n  -columnOrder \nuniq\n,\nvalue\n,\ncnt\n \n  -title \nLive 2-second counts of unique values\n;", 
            "title": "____ count_unique"
        }, 
        {
            "location": "/reducers/count_unique/#count_unique", 
            "text": "Return the total number of unique values of a specified field throughout\nthe batch.    Note:  For efficiency, the back-end\nimplementation of count_unique() uses an approximation algorithm, which\nmay return imprecise values for sets with large cardinality  put|reduce count_unique(field)     Parameter  Description  Required?      field  The field for which to count unique values  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put cnt = count(), num = Math.floor(Math.random() * 10), value =  thing ${num} \n}\n\nhistorical_points \n| (\n  reduce uniq = count_unique(value) \n  | view table -title  Historical total count of unique values ;\n\n  reduce cnt=count(value) by value \n  | view table -title  Historical list of unique values with counts ;\n);\n\nhistorical_points \n| batch 5 \n| reduce uniq = count_unique(value) \n| view table -update  append  -title  Historical 5-second count of unique values ;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), num = Math.floor(Math.random() * 10), value =  thing ${num} \n}\n\nlive_points \n| batch 2 \n| (\n  reduce uniq = count_unique(value), cnt = null, value = null;\n  reduce cnt = count(value), uniq = null by value\n)\n| view table \n  -update  append  \n  -columnOrder  uniq , value , cnt  \n  -title  Live 2-second counts of unique values ;", 
            "title": "count_unique"
        }, 
        {
            "location": "/reducers/delta/", 
            "text": "delta\n\n\nReturn the change in value of a field.\n\n\ndelta computes the difference between a field's current value and the\nvalue used in the previous delta. When used in a series of\n\nput\ns,\nthat is simply the point-to-point difference. When used in a series of\n\nbatch\nes,\nit is the difference between the last points of successive batches. When\nno point is present (for example, an empty batch), delta returns its\nspecial \"empty\" value, which defaults to 0.\n\n\n \nNote:\n If used in batched mode, or with\n\nreduce\n\n-every, you must specify -reset false. delta does not work at all with\n-over (in put or reduce).\n\n\nput|reduce delta(field, emptyValue, wrap)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to use as a value\n\n\nYes\n\n\n\n\n\n\nempty\n\n\nThe value to return when a current or previous value is not present (for example, no point has been observed since last call), default is 0.\n\n\nNo\n\n\n\n\n\n\nwrap\n\n\nTreat field as an incrementing counter that wraps occasionally. When wrap is specified, any field value that is less than the previous value is assumed to have wrapped. There are 3 ways to specify what should be returned: \nwrap != 0\n \nAdd wrap to delta (to make it positive). Use this for ingesting counters like a collectd bytes-in counter, which wraps when it reaches a specified maximum (for example, 2^32). \nwrap=0\n \nReturn the current (not delta) value. Use this for statsd's \"periodic counters\", which regularly reset themselves to 0 but are otherwise cumulative (and thus their first reading after a reset is in fact the delta value).  \nwrap=true\nReturn the \"empty\" value. Use this when you know you have a wrapping counter, but do not know the wrap value. \n\n\nNo\n\n\n\n\n\n\n\n\nExample: rate of change\n\n\nAt the moment, juttle has no rate-of-change reducer (for example,\nderivative). In this example delta computes rate of change the\nold-fashioned way, by dividing the change in a field by the change in\ntime. This example also shows the use of delta with a time value.\ndelta(time) is a duration, and is converted to seconds so it can divide\ndelta(position) to give a rate per second. The example also shows use of\nthe \"empty\" parameter with delta(time). When a value is not present (in\nthis example, for the very first delta), delta(time) will return a\ndefault duration of 0 seconds.\n\n\nemit -limit 100 -from Date.new(0)\n| put position = count() * count()\n| put dt = Duration.seconds(delta(time, :0s:))\n| put speed = delta(position) / dt\n| put acceleration = delta(speed) / dt\n| (\n  view timechart -valueField 'position';\n  view timechart -valueField 'speed';\n  view timechart -valueField 'acceleration';\n  )", 
            "title": "____ delta"
        }, 
        {
            "location": "/reducers/delta/#delta", 
            "text": "Return the change in value of a field.  delta computes the difference between a field's current value and the\nvalue used in the previous delta. When used in a series of put s,\nthat is simply the point-to-point difference. When used in a series of batch es,\nit is the difference between the last points of successive batches. When\nno point is present (for example, an empty batch), delta returns its\nspecial \"empty\" value, which defaults to 0.    Note:  If used in batched mode, or with reduce \n-every, you must specify -reset false. delta does not work at all with\n-over (in put or reduce).  put|reduce delta(field, emptyValue, wrap)     Parameter  Description  Required?      field  The field to use as a value  Yes    empty  The value to return when a current or previous value is not present (for example, no point has been observed since last call), default is 0.  No    wrap  Treat field as an incrementing counter that wraps occasionally. When wrap is specified, any field value that is less than the previous value is assumed to have wrapped. There are 3 ways to specify what should be returned:  wrap != 0   Add wrap to delta (to make it positive). Use this for ingesting counters like a collectd bytes-in counter, which wraps when it reaches a specified maximum (for example, 2^32).  wrap=0   Return the current (not delta) value. Use this for statsd's \"periodic counters\", which regularly reset themselves to 0 but are otherwise cumulative (and thus their first reading after a reset is in fact the delta value).   wrap=true Return the \"empty\" value. Use this when you know you have a wrapping counter, but do not know the wrap value.   No     Example: rate of change  At the moment, juttle has no rate-of-change reducer (for example,\nderivative). In this example delta computes rate of change the\nold-fashioned way, by dividing the change in a field by the change in\ntime. This example also shows the use of delta with a time value.\ndelta(time) is a duration, and is converted to seconds so it can divide\ndelta(position) to give a rate per second. The example also shows use of\nthe \"empty\" parameter with delta(time). When a value is not present (in\nthis example, for the very first delta), delta(time) will return a\ndefault duration of 0 seconds.  emit -limit 100 -from Date.new(0)\n| put position = count() * count()\n| put dt = Duration.seconds(delta(time, :0s:))\n| put speed = delta(position) / dt\n| put acceleration = delta(speed) / dt\n| (\n  view timechart -valueField 'position';\n  view timechart -valueField 'speed';\n  view timechart -valueField 'acceleration';\n  )", 
            "title": "delta"
        }, 
        {
            "location": "/reducers/first/", 
            "text": "first\n\n\nReturn the value of the specified field in the first point that contains\nit.\n\n\nput|reduce first(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to return\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce first(cnt), first_val = first(value), last(cnt), last_val = last(value)\n| view table -title \nFirst and last historical points\n\n;\nhistorical_points \n| batch 5 \n| reduce first(cnt), last(cnt) \n| view table\n    -update \nappend\n \n    -title \nFirst and last points per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce first(cnt), last(cnt) \n| view table\n    -update \nappend\n \n    -title \nFirst and last points per 3-second batch, live\n\n;", 
            "title": "____ first"
        }, 
        {
            "location": "/reducers/first/#first", 
            "text": "Return the value of the specified field in the first point that contains\nit.  put|reduce first(field)     Parameter  Description  Required?      field  The field to return  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce first(cnt), first_val = first(value), last(cnt), last_val = last(value)\n| view table -title  First and last historical points \n;\nhistorical_points \n| batch 5 \n| reduce first(cnt), last(cnt) \n| view table\n    -update  append  \n    -title  First and last points per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce first(cnt), last(cnt) \n| view table\n    -update  append  \n    -title  First and last points per 3-second batch, live \n;", 
            "title": "first"
        }, 
        {
            "location": "/reducers/last/", 
            "text": "last\n\n\nReturn the value of the specified field for the last point that contains\nit.\n\n\nput|reduce last(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to return\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce first(cnt), first_val = first(value), last(cnt), last_val = last(value)\n| view table -title \nFirst and last historical points\n\n;\nhistorical_points \n| batch 5 \n| reduce first(cnt), last(cnt) \n| view table\n    -update \nappend\n \n    -title \nFirst and last points per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce first(cnt), last(cnt) \n| view table\n    -update \nappend\n \n    -title \nFirst and last points per 3-second batch, live\n\n;", 
            "title": "____ last"
        }, 
        {
            "location": "/reducers/last/#last", 
            "text": "Return the value of the specified field for the last point that contains\nit.  put|reduce last(field)     Parameter  Description  Required?      field  The field to return  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce first(cnt), first_val = first(value), last(cnt), last_val = last(value)\n| view table -title  First and last historical points \n;\nhistorical_points \n| batch 5 \n| reduce first(cnt), last(cnt) \n| view table\n    -update  append  \n    -title  First and last points per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce first(cnt), last(cnt) \n| view table\n    -update  append  \n    -title  First and last points per 3-second batch, live \n;", 
            "title": "last"
        }, 
        {
            "location": "/reducers/mad/", 
            "text": "mad\n\n\nReturn the Median Absolute Deviation (MAD) of the value of the specified\nfield.\n\n\nput|reduce mad(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to compute\n\n\nYes\n\n\n\n\n\n\n\n\nMedian Absolute Deviation (MAD) is related to standard deviation in much\nthe way that median is related to mean. It is a robust measure of how\nspread out a set of values are around their central location (the\nmedian). \nMAD = median( | x - median| )\n\n\nExample: MAD and standard deviations\n\n\nThe example below shows how MAD and standard deviation behave on samples\nfrom a pure normal distribution and samples from a distribution\n\"contaminated\" with an outlier distribution (1% contamination). The MAD\nestimate for the contaminated distribution remains close to its value\nfor the dominant (pure) distribution, unlike the standard deviation.\n\n\nThis example uses a module from the\n\nstandard library\n.\n\n\nimport \nrandom.juttle\n as random;\nemit -limit 1000 -from Date.new(0)\n| put pure = random.normal(0, 1)\n| put mixed = (Math.random() \n .99) ? random.normal(0, 1) : random.normal(100, 0.1)\n| reduce\n   pure_mean = avg(pure),\n   pure_median = percentile(pure, 0.5),\n   pure_sd = sigma(pure),\n   pure_mad = mad(pure) * 1.48, // the 1.48 factor scales normal MAD to be same as normal SD\n   mixed_mean = avg(mixed),\n   mixed_median = percentile(mixed, 0.5),\n   mixed_sd = sigma(mixed),\n   mixed_mad = mad(mixed) * 1.48\n| view table -columnOrder ['pure_mean', 'pure_sd', 'pure_median', 'pure_mad', 'mixed_mean', 'mixed_sd', 'mixed_median', 'mixed_mad']", 
            "title": "____ mad"
        }, 
        {
            "location": "/reducers/mad/#mad", 
            "text": "Return the Median Absolute Deviation (MAD) of the value of the specified\nfield.  put|reduce mad(field)     Parameter  Description  Required?      field  The field to compute  Yes     Median Absolute Deviation (MAD) is related to standard deviation in much\nthe way that median is related to mean. It is a robust measure of how\nspread out a set of values are around their central location (the\nmedian).  MAD = median( | x - median| )  Example: MAD and standard deviations  The example below shows how MAD and standard deviation behave on samples\nfrom a pure normal distribution and samples from a distribution\n\"contaminated\" with an outlier distribution (1% contamination). The MAD\nestimate for the contaminated distribution remains close to its value\nfor the dominant (pure) distribution, unlike the standard deviation.  This example uses a module from the standard library .  import  random.juttle  as random;\nemit -limit 1000 -from Date.new(0)\n| put pure = random.normal(0, 1)\n| put mixed = (Math.random()   .99) ? random.normal(0, 1) : random.normal(100, 0.1)\n| reduce\n   pure_mean = avg(pure),\n   pure_median = percentile(pure, 0.5),\n   pure_sd = sigma(pure),\n   pure_mad = mad(pure) * 1.48, // the 1.48 factor scales normal MAD to be same as normal SD\n   mixed_mean = avg(mixed),\n   mixed_median = percentile(mixed, 0.5),\n   mixed_sd = sigma(mixed),\n   mixed_mad = mad(mixed) * 1.48\n| view table -columnOrder ['pure_mean', 'pure_sd', 'pure_median', 'pure_mad', 'mixed_mean', 'mixed_sd', 'mixed_median', 'mixed_mad']", 
            "title": "mad"
        }, 
        {
            "location": "/reducers/max/", 
            "text": "max\n\n\nReturn the maximum value of the specified field from among all points\ncontaining that field.\n\n\nput|reduce max(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to analyze for the maximum value\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce min(cnt), min_val = min(value), max(cnt), max_val = max(value)\n| view table \n    -columnOrder \nmin\n,\nmin_val\n,\nmax\n,\nmax_val\n\n    -title \nMin and max of historical points\n\n  ;\nhistorical_points \n| batch 5 \n| reduce min(cnt), max(cnt) \n| view table\n    -columnOrder \nmin\n,\nmax\n \n    -update \nappend\n \n    -title \nMin and max per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce min(cnt), max(cnt) \n| view table \n  -columnOrder \nmin\n,\nmax\n -update \nappend\n\n  -title \nMin and max per 3-second batch, live\n;", 
            "title": "____ max"
        }, 
        {
            "location": "/reducers/max/#max", 
            "text": "Return the maximum value of the specified field from among all points\ncontaining that field.  put|reduce max(field)     Parameter  Description  Required?      field  The field to analyze for the maximum value  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce min(cnt), min_val = min(value), max(cnt), max_val = max(value)\n| view table \n    -columnOrder  min , min_val , max , max_val \n    -title  Min and max of historical points \n  ;\nhistorical_points \n| batch 5 \n| reduce min(cnt), max(cnt) \n| view table\n    -columnOrder  min , max  \n    -update  append  \n    -title  Min and max per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce min(cnt), max(cnt) \n| view table \n  -columnOrder  min , max  -update  append \n  -title  Min and max per 3-second batch, live ;", 
            "title": "max"
        }, 
        {
            "location": "/reducers/min/", 
            "text": "min\n\n\nReturn the minimum value of the specified field from among all points\ncontaining that field.\n\n\nput|reduce min(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to analyze for the minimum value\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce min(cnt), min_val = min(value), max(cnt), max_val = max(value)\n| view table \n    -columnOrder \nmin\n,\nmin_val\n,\nmax\n,\nmax_val\n\n    -title \nMin and max of historical points\n\n  ;\nhistorical_points \n| batch 5 \n| reduce min(cnt), max(cnt) \n| view table\n    -columnOrder \nmin\n,\nmax\n \n    -update \nappend\n \n    -title \nMin and max per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce min(cnt), max(cnt) \n| view table \n  -columnOrder \nmin\n,\nmax\n -update \nappend\n\n  -title \nMin and max per 3-second batch, live\n;", 
            "title": "____ min"
        }, 
        {
            "location": "/reducers/min/#min", 
            "text": "Return the minimum value of the specified field from among all points\ncontaining that field.  put|reduce min(field)     Parameter  Description  Required?      field  The field to analyze for the minimum value  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 | put cnt = count(), value = Math.random()\n}\nhistorical_points \n| reduce min(cnt), min_val = min(value), max(cnt), max_val = max(value)\n| view table \n    -columnOrder  min , min_val , max , max_val \n    -title  Min and max of historical points \n  ;\nhistorical_points \n| batch 5 \n| reduce min(cnt), max(cnt) \n| view table\n    -columnOrder  min , max  \n    -update  append  \n    -title  Min and max per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 | put cnt = count(), value = Math.random()\n}\nlive_points \n| batch 3 \n| reduce min(cnt), max(cnt) \n| view table \n  -columnOrder  min , max  -update  append \n  -title  Min and max per 3-second batch, live ;", 
            "title": "min"
        }, 
        {
            "location": "/reducers/percentile/", 
            "text": "percentile\n\n\nReturn the p^th^ percentile ranked value of the specified field (or a\nlist of percentile values if given a list of percentile ranks). When\npercentile is applied to large numbers of distinct values (such as from\na continuous metric) the returned value is approximate rather than\nexact.\n\n\nput|reduce percentile(field, p)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to use as a value\n\n\nYes\n\n\n\n\n\n\np\n\n\nThe percentile rank to select, in range 0...1 (or a list of percentile ranks). A value (or list of values) will be returned whose rank is equal to or greater than \np * number of points\nwhen the points are sorted by the value of the field.\n\n\nYes\n\n\n\n\n\n\n\n\nExample: 10th and 90th percentiles\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() { \n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nhistorical_points \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9)\n| view table -title \n10th and 90th percentile of historical points\n\n;\nhistorical_points \n| batch 5 \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9) \n| view table \n    -update \nappend\n \n    -title \n10th and 90th percentile per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() { \n  emit -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9)\n| view table \n    -update \nappend\n \n    -title \n10th and 90th percentile per 3-second batch, live\n\n\n\n\n\nExample: quartiles\n \n\n\nThis example uses a list of percentile ranks to retrieve the quartile\nvalues of the data (minimum, 25%, median, 75%, and maximum). Because of\nthe large number of x values, approximate quartile values are returned,\nthough min and max will always be exact.\n\n\nconst QUARTILES = [0, 0.25, 0.5, 0.75, 1];\nemit -limit 10001 -from Date.new(0)\n| put x = count() - 1 // our data value ranges from 0 to 10000\n| reduce Q = percentile(x, QUARTILES)\n| put min = Q[0], Q1 = Q[1], Q2 = Q[2], Q3 = Q[3], max = Q[4]\n| remove Q\n| view table", 
            "title": "____ percentile"
        }, 
        {
            "location": "/reducers/percentile/#percentile", 
            "text": "Return the p^th^ percentile ranked value of the specified field (or a\nlist of percentile values if given a list of percentile ranks). When\npercentile is applied to large numbers of distinct values (such as from\na continuous metric) the returned value is approximate rather than\nexact.  put|reduce percentile(field, p)     Parameter  Description  Required?      field  The field to use as a value  Yes    p  The percentile rank to select, in range 0...1 (or a list of percentile ranks). A value (or list of values) will be returned whose rank is equal to or greater than  p * number of points when the points are sorted by the value of the field.  Yes     Example: 10th and 90th percentiles  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() { \n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nhistorical_points \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9)\n| view table -title  10th and 90th percentile of historical points \n;\nhistorical_points \n| batch 5 \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9) \n| view table \n    -update  append  \n    -title  10th and 90th percentile per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() { \n  emit -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce pct10 = percentile(value, 0.1), pct90 = percentile(value, 0.9)\n| view table \n    -update  append  \n    -title  10th and 90th percentile per 3-second batch, live   Example: quartiles    This example uses a list of percentile ranks to retrieve the quartile\nvalues of the data (minimum, 25%, median, 75%, and maximum). Because of\nthe large number of x values, approximate quartile values are returned,\nthough min and max will always be exact.  const QUARTILES = [0, 0.25, 0.5, 0.75, 1];\nemit -limit 10001 -from Date.new(0)\n| put x = count() - 1 // our data value ranges from 0 to 10000\n| reduce Q = percentile(x, QUARTILES)\n| put min = Q[0], Q1 = Q[1], Q2 = Q[2], Q3 = Q[3], max = Q[4]\n| remove Q\n| view table", 
            "title": "percentile"
        }, 
        {
            "location": "/reducers/pluck/", 
            "text": "pluck\n\n\nReturn an array of the values of a specified field in the batch. The\nresulting array is stored in a field called \"pluck\".\n\n\nput|reduce pluck(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to pluck\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n} \nhistorical_points \n| reduce values = pluck(value)\n| view table -title \nList values of all historical points\n\n;\nhistorical_points \n| batch 5 \n| reduce values = pluck(value) \n| view table\n    -update \nappend\n \n    -title \nList values of points per 5-second batch, historical\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\nemit -limit 10 \n| put value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce values = pluck(value)\n| view table \n    -update \nappend\n \n    -title \nList values of points per 3-second batch, live\n\n;", 
            "title": "____ pluck"
        }, 
        {
            "location": "/reducers/pluck/#pluck", 
            "text": "Return an array of the values of a specified field in the batch. The\nresulting array is stored in a field called \"pluck\".  put|reduce pluck(field)     Parameter  Description  Required?      field  The field to pluck  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n} \nhistorical_points \n| reduce values = pluck(value)\n| view table -title  List values of all historical points \n;\nhistorical_points \n| batch 5 \n| reduce values = pluck(value) \n| view table\n    -update  append  \n    -title  List values of points per 5-second batch, historical \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\nemit -limit 10 \n| put value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce values = pluck(value)\n| view table \n    -update  append  \n    -title  List values of points per 3-second batch, live \n;", 
            "title": "pluck"
        }, 
        {
            "location": "/reducers/sigma/", 
            "text": "sigma\n\n\nReturn the standard deviation of the value of the specified field.\n\n\nput|reduce sigma(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to compute\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() { \n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nhistorical_points \n| reduce avg(value), stdev = sigma(value)\n| view table -title \nHistorical average and standard deviation\n\n;\nhistorical_points \n| batch 5 \n| reduce avg(value), stdev = sigma(value) \n| view table\n    -update \nappend\n \n    -title \nHistorical 5-second average and standard deviation\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce avg(value), stdev = sigma(value)\n| view table \n    -update \nappend\n \n    -title \nLive 3-second average and standard deviation\n\n;", 
            "title": "____ sigma"
        }, 
        {
            "location": "/reducers/sigma/#sigma", 
            "text": "Return the standard deviation of the value of the specified field.  put|reduce sigma(field)     Parameter  Description  Required?      field  The field to compute  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() { \n  emit -from :0: -limit 10 \n  | put value = Math.floor(Math.random() * 100)\n}\nhistorical_points \n| reduce avg(value), stdev = sigma(value)\n| view table -title  Historical average and standard deviation \n;\nhistorical_points \n| batch 5 \n| reduce avg(value), stdev = sigma(value) \n| view table\n    -update  append  \n    -title  Historical 5-second average and standard deviation \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put cnt = count(), value = Math.floor(Math.random() * 100)\n}\nlive_points \n| batch 3 \n| reduce avg(value), stdev = sigma(value)\n| view table \n    -update  append  \n    -title  Live 3-second average and standard deviation \n;", 
            "title": "sigma"
        }, 
        {
            "location": "/reducers/sum/", 
            "text": "sum\n\n\nReturn the sum of the values of the specified field throughout the\nbatch.\n\n\nput|reduce sum(field)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfield\n\n\nThe field to sum\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nThis example uses the\n\nbatch\n\nprocessor.\n\n\n// On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put value = count()\n}\nhistorical_points \n| reduce sum(value) \n| view table -title \nHistorical sum total\n\n;\nhistorical_points \n| batch 5 \n| reduce sum(value) \n| view table\n    -update \nappend\n \n    -title \nHistorical 5-second subtotals\n\n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put value = count()\n}\nlive_points \n| batch 3 \n| reduce sum(value)\n| view table \n    -update \nappend\n \n    -title \nLive 3-second subtotals\n\n;", 
            "title": "____ sum"
        }, 
        {
            "location": "/reducers/sum/#sum", 
            "text": "Return the sum of the values of the specified field throughout the\nbatch.  put|reduce sum(field)     Parameter  Description  Required?      field  The field to sum  Yes     Example  This example uses the batch \nprocessor.  // On historical data, you can apply reducer to get a single computation,\n// or batch your historical data by time, then reduce per batch period.\n\nsub historical_points() {\n  emit -from :0: -limit 10 \n  | put value = count()\n}\nhistorical_points \n| reduce sum(value) \n| view table -title  Historical sum total \n;\nhistorical_points \n| batch 5 \n| reduce sum(value) \n| view table\n    -update  append  \n    -title  Historical 5-second subtotals \n;\n\n// On live streaming data, you must batch by time, then reduce per batch period.\n\nsub live_points() {\n  emit -limit 10 \n  | put value = count()\n}\nlive_points \n| batch 3 \n| reduce sum(value)\n| view table \n    -update  append  \n    -title  Live 3-second subtotals \n;", 
            "title": "sum"
        }, 
        {
            "location": "/modules/", 
            "text": "Modules\n\n\nJuttle comes with built-in modules that provide functions for different data types:\n\n\n\n\nArray\n\n\nDate\n\n\nDuration\n\n\nMath\n\n\nNumber\n\n\nObject\n\n\nString\n\n\n\n\nThe constants and functions exported by these modules can be used in Juttle programs directly, without an import statement.", 
            "title": "Modules"
        }, 
        {
            "location": "/modules/#modules", 
            "text": "Juttle comes with built-in modules that provide functions for different data types:   Array  Date  Duration  Math  Number  Object  String   The constants and functions exported by these modules can be used in Juttle programs directly, without an import statement.", 
            "title": "Modules"
        }, 
        {
            "location": "/modules/array/", 
            "text": "Array\n\n\n\n\n\n\nArray\n\n\nArray.length\n\n\nArray.indexOf\n\n\n\n\n\n\n\n\n\n\nArrays can be declared as constants in a flowgraph or variables in\n\nuser defined functions\n. To create an array from your data in a Juttle pipeline, use the \npluck\n reducer.\n\n\nThe Array module exposes a limited set of operations on array variables.\n\n\nArray.length\n\n\nReturn the number of items in an array.\n\n\nArray.length(array)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\narray\n\n\nThe array to measure\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Create an array containing a random number of values, then count them\n\n\nconst random_length=(Math.round(Math.random()*10));\n;\nemit -limit random_length -every :.1 second:\n| put values=(Math.round(Math.random()*10))\n| reduce pluck(values)\n| put length=Array.length(pluck)\n\n\n\n\nArray.indexOf\n\n\nSearch an array and return the zero-based index of the first match, or\n-1 if there is no match.\n\n\nArray.indexOf(array, searchvalue)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\narray\n\n\nThe array to search\n\n\nYes\n\n\n\n\n\n\nsearchvalue\n\n\nThe value for which to search\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Search for a random integer in an array of random integers\n\n\nemit -limit 10 -every :.1 second:\n| put values=(Math.round(Math.random()*10))\n| reduce pluck(values)\n| put searchvalue=(Math.round(Math.random()*10))\n| put index=Array.indexOf(pluck, searchvalue)", 
            "title": "____ Array"
        }, 
        {
            "location": "/modules/array/#array", 
            "text": "Array  Array.length  Array.indexOf      Arrays can be declared as constants in a flowgraph or variables in user defined functions . To create an array from your data in a Juttle pipeline, use the  pluck  reducer.  The Array module exposes a limited set of operations on array variables.", 
            "title": "Array"
        }, 
        {
            "location": "/modules/array/#arraylength", 
            "text": "Return the number of items in an array.  Array.length(array)     Parameter  Description  Required?      array  The array to measure  Yes     Example: Create an array containing a random number of values, then count them  const random_length=(Math.round(Math.random()*10));\n;\nemit -limit random_length -every :.1 second:\n| put values=(Math.round(Math.random()*10))\n| reduce pluck(values)\n| put length=Array.length(pluck)", 
            "title": "Array.length"
        }, 
        {
            "location": "/modules/array/#arrayindexof", 
            "text": "Search an array and return the zero-based index of the first match, or\n-1 if there is no match.  Array.indexOf(array, searchvalue)     Parameter  Description  Required?      array  The array to search  Yes    searchvalue  The value for which to search  Yes     Example: Search for a random integer in an array of random integers  emit -limit 10 -every :.1 second:\n| put values=(Math.round(Math.random()*10))\n| reduce pluck(values)\n| put searchvalue=(Math.round(Math.random()*10))\n| put index=Array.indexOf(pluck, searchvalue)", 
            "title": "Array.indexOf"
        }, 
        {
            "location": "/modules/date/", 
            "text": "Date\n\n\nDates are represented in a Juttle flowgraph as Juttle moments. The Date module enables conversion between Unix timestamps, string representation of date/time, and moments.\n\n\n\n\n\n\nDate\n\n\nDate.elapsed\n\n\nDate.endOf\n\n\nDate.format\n\n\nDate.formatTz\n\n\nDate.get\n\n\nDate.new\n\n\nDate.parse\n\n\nFormat\n\n\nTimezone support\n\n\n\n\n\n\nDate.quantize\n\n\nDate.startOf\n\n\nDate.time\n\n\nDate.toString()\n\n\nDate.unix\n\n\nDate.unixms\n\n\n\n\n\n\n\n\n\n\n\n\nDate.elapsed\n\n\nReturn floating-point seconds since the moment.\n\n\nDate.elapsed(moment)\n\n\n\n\n \nNote:\n This is equivalent to\n\nDuration.seconds(Date.now() - moment)\n.\n\n\nemit -limit 1 | put hours_wasted_today = Date.elapsed(:today:) / 3600;\n\n\n\n\n\n\nDate.endOf\n\n\nReturn a new moment, equal to the supplied moment set to the end of its\ncurrent time unit.\n\n\nDate.endOf(moment, timeunit)\n\n\n\n\n \nNote:\n The new moment is the end of the\nunit in the time zone in which moment was recorded, not the time zone in\nwhich it is displayed.\n\n\nExample: Find the last day of this month\n\n\nemit -limit 1 | put eom = Date.endOf(time, \nmonth\n)\n\n\n\n\n\n\nDate.format\n\n\nFormat a moment as a string in the specified format, optionally adding a\ntime zone. The input is assumed to be in UTC.\n\n\nDate.format(moment)\nDate.format(moment, format_string, timezone)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nmoment\n\n\nThe moment to reformat\n\n\nYes\n\n\n\n\n\n\nformat_string\n\n\nThe format configuration, as documented in \nmomentjs's format() function\n\n\nNo; if not specified, the function returns the ISO-8601-formatted date.\n\n\n\n\n\n\ntimezone\n\n\nA time zone to include in the formatted string, as documented in \nmomentjs's moment-timezone package\n \nAt the link above, you'll find a map you can hover over to get valid values for specific locations. We've also supplemented the built-in values, see list below.\n\n\nNo\n\n\n\n\n\n\n\n\n Adjustments for Daylight Savings Time/Standard Time are always applied automatically, so for example you'll still get an accurate result if you specify 'PST' for a date that falls within PDT, or 'PDT' for a date that falls within PST.\n\n\nAdditional timezone values we support:\n\n\n\n\narizona:\"US/Arizona\"   \n\n\naz:\"US/Arizona\"   \n\n\ncentral:\"US/Central\"   \n\n\ncdt:\"US/Central\"   \n\n\ncst:\"US/Central\"   \n\n\neastern:\"US/Eastern\"   \n\n\nedt:\"US/Eastern\"   \n\n\nest:\"US/Eastern\"   \n\n\nmountain:\"US/Mountain\"   \n\n\nmdt:\"US/Mountain\"   \n\n\nmst:\"US/Mountain\"   \n\n\npacific:\"US/Pacific\"   \n\n\npst:\"US/Pacific\"   \n\n\npdt:\"US/Pacific\"    \n\n\n\n\nExample\n\n\nemit -from :0: -limit 1\n| put pacific = Date.format(:2015-01-01T00:00:00:,null,\nPDT\n)\n| put mountain = Date.format(:2015-01-01T00:00:00:,\nHH-z\n)\n| put central = Date.format(:2015-01-01T00:00:00:,\nHH Z\n,\nCDT\n)\n| put eastern = Date.format(:2015-01-01T00:00:00:,null,\nEastern\n)\n\n\n\n\n\n\nDate.formatTz\n\n\nAdd a time zone to a UTC moment.\n\n\nDate.formatTz(moment, timezone)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nmoment\n\n\nThe moment to reformat\n\n\nYes\n\n\n\n\n\n\ntimezone\n\n\nA time zone to include in the formatted string, as documented in \nmomentjs's moment-timezone package\n \nAt the link above, you'll find a map you can hover over to get valid values for specific locations. We've also supplemented the built-in values, see list below.\n\n\nNo\n\n\n\n\n\n\n\n\n Adjustments for Daylight Savings Time/Standard Time are always applied automatically, so for example you'll still get an accurate result if you specify 'PST' for a date that falls within PDT, or 'PDT' for a date that falls within PST.\n\n\nAdditional timezone values we support:\n\n\n\n\narizona:\"US/Arizona\"   \n\n\naz:\"US/Arizona\"   \n\n\ncentral:\"US/Central\"   \n\n\ncdt:\"US/Central\"   \n\n\ncst:\"US/Central\"   \n\n\neastern:\"US/Eastern\"   \n\n\nedt:\"US/Eastern\"   \n\n\nest:\"US/Eastern\"   \n\n\nmountain:\"US/Mountain\"   \n\n\nmdt:\"US/Mountain\"   \n\n\nmst:\"US/Mountain\"   \n\n\npacific:\"US/Pacific\"   \n\n\npst:\"US/Pacific\"   \n\n\npdt:\"US/Pacific\"   \n\n\n\n\nExample\n\n\nemit -from :0: -limit 1\n| put pacific = Date.formatTz(:2015-01-01T00:00:00:,\nPDT\n)\n| put mountain = Date.formatTz(:2015-01-01T00:00:00:,\nMST\n)\n| put central = Date.formatTz(:2015-01-01T00:00:00:,\nCDT\n)\n| put eastern = Date.formatTz(:2015-01-01T00:00:00:,\nEastern\n)\n\n\n\n\n\n\nDate.get\n\n\nReturn the numeric value of the time unit for a moment as an integer.\n\n\nDate.get(moment, timeunit)\n\n\n\n\nExample: get the number of current month\n\n\nemit -limit 1 | put month_num = Date.get(time, \nmonth\n)\n\n\n\n\n\n\nDate.new\n\n\nReturn a moment.\n\n\nDate.new(\nYYYY-MM-DDTHH:MM:SS.msec+TZ\n|seconds)\n\n\n\n\nThere are two ways to specify a moment:\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"YYYY-MM-DDTHH:MM:SS.msec+TZ\"\n\n\nAn ISO-8601 string. The time zone (TZ) is a numeric offset from UTC0. UTC0 is the default.\n\n\n\n\n\n\nseconds\n\n\nSeconds since the UNIX epoch\n\n\n\n\n\n\n\n\nExample: Some valid dates\n\n\nemit\n  | remove time\n  | put fromunix = Date.new(1411426250)\n  | put fromunixstr = Date.new(\n1411426250\n)\n  | put infinity = Date.new(Infinity)\n  | put negInfinity = Date.new(-Infinity)\n  | put fromday =  Date.new(\n2014-09-22\n)\n  | put fromISO =  Date.new(\n2014-09-22T11:39:17.993\n)\n  | put ISOwithTZ =  Date.new(\n2014-09-22T11:39:17.993-08:00\n)\n  | split\n\n\n\n\n\n\nDate.parse\n\n\nParse a moment from a string using the specified format.\n\n\nDate.parse(moment_string, format_string)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nmoment_string\n\n\nThe string to convert to a moment\n\n\nYes\n\n\n\n\n\n\nformat_string\n\n\nThe format configuration, as documented in \nmomentjs's String+Format constructor\n\n\nNo; if not specified, the function expects ISO-8601-formatted date as input.\n\n\n\n\n\n\n\n\nExample: Parse a JavaScript-formatted date\n\n\nemit\n| put date_in = \nThu Oct 29 2015 16:46:35 -0700\n,\n      date_format = \nddd MMM DD YYYY HH:mm:ss Z\n,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in = \nThu Oct 29 2015 16:46:35-0700\n,\n      date_format = \nddd MMM DD YYYY HH:mm:ssZ\n,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in = \nThu Oct 29 2015 16:46:35 +07:00\n,\n      date_format = \nddd MMM DD YYYY HH:mm:ss Z\n,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in = \n10/29/15 16:46:35.000Z\n,\n      date_format = \nMM/DD/YY HH:mm:ss.SSSZ\n,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in = \n2015-10-29 16-0700\n,\n      date_format = \nYYYY-MM-DD HHZ\n,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in = \n2015-10-29 16:46:35-0700\n,\n      date_format = \n//default: ISO8601\n,\n      date_out = Date.parse(date_in);\nemit\n| put date_in = \n2015-10-29T16:46:35.000Z\n,\n      date_format = \n//default: ISO8601\n,\n      date_out = Date.parse(date_in);\nemit\n| put date_in = \n2014-09-22Z+08:00\n,\n      date_format = \n//default: Date fallback with timezone\n,\n      date_out = Date.parse(date_in);\nemit\n| put date_in = \nWed Dec 2 18:56:48 2015 UTC\n,\n      date_format = \n//default: acceptable non-ISO8601\n,\n      date_out = Date.parse(date_in);\n\n\n\n\nFormat\n\n\nWhitespace in the format string should match whitespace in the incoming string.\n\n\nDifferent separators such as \n-\n vs \n/\n for date components, \n.\n vs \n,\n before milliseconds, are supported. However, if the timestamp in your data is enclosed in brackets, such as \n[2015/10/29 16:46:35]\n, Date.parse will only handle the enclosed string and not the brackets.\n\n\nWhen Date.parse is called with a single parameter of input string, without a custom format specification, it will parse several variants of ISO-8601 dates.\n\n\nTimezone support\n\n\nJuttle follows momentjs format for timezone specification, namely, as offset from UTC\n\n+HH:mm\n or \n-HH:mm\n, \n+HHmm\n or \n-HHmm\n, or the literal \nZ\n to stand for UTC (GMT) time zone.\n\n\nDate.parse does not handle acronyms such as \"PDT\", or longform such as \"GMT-0700 (PDT)\".\n\n\n\n\nDate.quantize\n\n\nReturn a new moment, equal to the supplied moment rounded down to an\neven number of durations since the epoch.\n\n\nDate.quantize(moment, duration)\n\n\n\n\nExample: get year and month when Pluto lost planet status\n\n\nemit -limit 1 \n| put oh_pluto = Date.new(\n2006-09-13\n)\n| put that_month = Date.quantize(oh_pluto, :M:), that_year = Date.quantize(oh_pluto, :y:) \n| keep oh_pluto, that_month, that_year\n\n\n\n\n\n\nDate.startOf\n\n\nReturn a new moment, equal to the supplied moment set to the start of\nits current time unit.\n\n\nDate.startOf(moment, timeunit)\n\n\n\n\n \nNote:\n The new moment is the start of the unit in the time zone in which moment was recorded, not the time zone in which it is displayed.\n\n\nExample: beginning of a given day\n\n\nemit -limit 1 | put today = Date.startOf(time, \nday\n) \n\n\n\n\n\n\nDate.time\n\n\nReturn the current moment, at millisecond resolution.\n\n\nDate.time()\n\n\n\n\nExample: Show the difference between a time stamp and real time\n\n\nconst start = Date.time();\nemit -from start\n| put elapsed = time - start, clock = Date.time(), skew = clock - time \n| view table\n\n\n\n\n\n\nDate.toString()\n\n\nConvert a value of type Date to type String. This is necessary to perform string operations such as concatenation.\n\n\nDate.toString(date)\n\n\n\n\nThe returned String is in ISO-8601 format:\n\n\nYYYY-MM-DDTHH:mm:ss.SSSZ\n\n\n\n\nExample: print current time\n\n\nemit\n| put now_moment = :now:, now_string = Date.toString(:now:)\n| put time_string = \nCurrent time: \n + now_string\n\n\n\n\n\n\nDate.unix\n\n\nReturn the UNIX time stamp for this moment, as seconds since the epoch.\n\n\nDate.unix(moment)\n\n\n\n\n \nNote:\n This is equivalent to\n\nDuration.seconds(moment - Date.new(0))\n.\n\n\nExample: Compute the number of seconds in a minute, the hard way\n\n\nconst sixty = Date.unix(Date.new(\n2014-09-22T00:01:00\n)) - Date.unix(Date.new(\n2014-09-22\n));\nemit -limit 1 | put seconds_in_minute = sixty\n\n\n\n\n\n\nDate.unixms\n\n\nReturn the UNIX time stamp for this moment, as milliseconds since the\nepoch.\n\n\nDate.unixms(moment)\n\n\n\n\n \nNote:\n This is equivalent to\n\nDuration.milliseconds(moment - Date.new(0))\n.\n\n\nExample: Compute the number of milliseconds in a minute, the hard way\n\n\nconst sixtyaught = Date.unixms(Date.new(\n2014-09-22T00:01:00\n)) - Date.unixms(Date.new(\n2014-09-22\n));\nemit -limit 1 | put ms_in_minute = sixtyaught", 
            "title": "____ Date"
        }, 
        {
            "location": "/modules/date/#date", 
            "text": "Dates are represented in a Juttle flowgraph as Juttle moments. The Date module enables conversion between Unix timestamps, string representation of date/time, and moments.    Date  Date.elapsed  Date.endOf  Date.format  Date.formatTz  Date.get  Date.new  Date.parse  Format  Timezone support    Date.quantize  Date.startOf  Date.time  Date.toString()  Date.unix  Date.unixms", 
            "title": "Date"
        }, 
        {
            "location": "/modules/date/#dateelapsed", 
            "text": "Return floating-point seconds since the moment.  Date.elapsed(moment)    Note:  This is equivalent to Duration.seconds(Date.now() - moment) .  emit -limit 1 | put hours_wasted_today = Date.elapsed(:today:) / 3600;", 
            "title": "Date.elapsed"
        }, 
        {
            "location": "/modules/date/#dateendof", 
            "text": "Return a new moment, equal to the supplied moment set to the end of its\ncurrent time unit.  Date.endOf(moment, timeunit)    Note:  The new moment is the end of the\nunit in the time zone in which moment was recorded, not the time zone in\nwhich it is displayed.  Example: Find the last day of this month  emit -limit 1 | put eom = Date.endOf(time,  month )", 
            "title": "Date.endOf"
        }, 
        {
            "location": "/modules/date/#dateformat", 
            "text": "Format a moment as a string in the specified format, optionally adding a\ntime zone. The input is assumed to be in UTC.  Date.format(moment)\nDate.format(moment, format_string, timezone)     Parameter  Description  Required?      moment  The moment to reformat  Yes    format_string  The format configuration, as documented in  momentjs's format() function  No; if not specified, the function returns the ISO-8601-formatted date.    timezone  A time zone to include in the formatted string, as documented in  momentjs's moment-timezone package   At the link above, you'll find a map you can hover over to get valid values for specific locations. We've also supplemented the built-in values, see list below.  No      Adjustments for Daylight Savings Time/Standard Time are always applied automatically, so for example you'll still get an accurate result if you specify 'PST' for a date that falls within PDT, or 'PDT' for a date that falls within PST.  Additional timezone values we support:   arizona:\"US/Arizona\"     az:\"US/Arizona\"     central:\"US/Central\"     cdt:\"US/Central\"     cst:\"US/Central\"     eastern:\"US/Eastern\"     edt:\"US/Eastern\"     est:\"US/Eastern\"     mountain:\"US/Mountain\"     mdt:\"US/Mountain\"     mst:\"US/Mountain\"     pacific:\"US/Pacific\"     pst:\"US/Pacific\"     pdt:\"US/Pacific\"       Example  emit -from :0: -limit 1\n| put pacific = Date.format(:2015-01-01T00:00:00:,null, PDT )\n| put mountain = Date.format(:2015-01-01T00:00:00:, HH-z )\n| put central = Date.format(:2015-01-01T00:00:00:, HH Z , CDT )\n| put eastern = Date.format(:2015-01-01T00:00:00:,null, Eastern )", 
            "title": "Date.format"
        }, 
        {
            "location": "/modules/date/#dateformattz", 
            "text": "Add a time zone to a UTC moment.  Date.formatTz(moment, timezone)     Parameter  Description  Required?      moment  The moment to reformat  Yes    timezone  A time zone to include in the formatted string, as documented in  momentjs's moment-timezone package   At the link above, you'll find a map you can hover over to get valid values for specific locations. We've also supplemented the built-in values, see list below.  No      Adjustments for Daylight Savings Time/Standard Time are always applied automatically, so for example you'll still get an accurate result if you specify 'PST' for a date that falls within PDT, or 'PDT' for a date that falls within PST.  Additional timezone values we support:   arizona:\"US/Arizona\"     az:\"US/Arizona\"     central:\"US/Central\"     cdt:\"US/Central\"     cst:\"US/Central\"     eastern:\"US/Eastern\"     edt:\"US/Eastern\"     est:\"US/Eastern\"     mountain:\"US/Mountain\"     mdt:\"US/Mountain\"     mst:\"US/Mountain\"     pacific:\"US/Pacific\"     pst:\"US/Pacific\"     pdt:\"US/Pacific\"      Example  emit -from :0: -limit 1\n| put pacific = Date.formatTz(:2015-01-01T00:00:00:, PDT )\n| put mountain = Date.formatTz(:2015-01-01T00:00:00:, MST )\n| put central = Date.formatTz(:2015-01-01T00:00:00:, CDT )\n| put eastern = Date.formatTz(:2015-01-01T00:00:00:, Eastern )", 
            "title": "Date.formatTz"
        }, 
        {
            "location": "/modules/date/#dateget", 
            "text": "Return the numeric value of the time unit for a moment as an integer.  Date.get(moment, timeunit)  Example: get the number of current month  emit -limit 1 | put month_num = Date.get(time,  month )", 
            "title": "Date.get"
        }, 
        {
            "location": "/modules/date/#datenew", 
            "text": "Return a moment.  Date.new( YYYY-MM-DDTHH:MM:SS.msec+TZ |seconds)  There are two ways to specify a moment:     Value  Description      \"YYYY-MM-DDTHH:MM:SS.msec+TZ\"  An ISO-8601 string. The time zone (TZ) is a numeric offset from UTC0. UTC0 is the default.    seconds  Seconds since the UNIX epoch     Example: Some valid dates  emit\n  | remove time\n  | put fromunix = Date.new(1411426250)\n  | put fromunixstr = Date.new( 1411426250 )\n  | put infinity = Date.new(Infinity)\n  | put negInfinity = Date.new(-Infinity)\n  | put fromday =  Date.new( 2014-09-22 )\n  | put fromISO =  Date.new( 2014-09-22T11:39:17.993 )\n  | put ISOwithTZ =  Date.new( 2014-09-22T11:39:17.993-08:00 )\n  | split", 
            "title": "Date.new"
        }, 
        {
            "location": "/modules/date/#dateparse", 
            "text": "Parse a moment from a string using the specified format.  Date.parse(moment_string, format_string)     Parameter  Description  Required?      moment_string  The string to convert to a moment  Yes    format_string  The format configuration, as documented in  momentjs's String+Format constructor  No; if not specified, the function expects ISO-8601-formatted date as input.     Example: Parse a JavaScript-formatted date  emit\n| put date_in =  Thu Oct 29 2015 16:46:35 -0700 ,\n      date_format =  ddd MMM DD YYYY HH:mm:ss Z ,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in =  Thu Oct 29 2015 16:46:35-0700 ,\n      date_format =  ddd MMM DD YYYY HH:mm:ssZ ,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in =  Thu Oct 29 2015 16:46:35 +07:00 ,\n      date_format =  ddd MMM DD YYYY HH:mm:ss Z ,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in =  10/29/15 16:46:35.000Z ,\n      date_format =  MM/DD/YY HH:mm:ss.SSSZ ,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in =  2015-10-29 16-0700 ,\n      date_format =  YYYY-MM-DD HHZ ,\n      date_out = Date.parse(date_in, date_format);\nemit\n| put date_in =  2015-10-29 16:46:35-0700 ,\n      date_format =  //default: ISO8601 ,\n      date_out = Date.parse(date_in);\nemit\n| put date_in =  2015-10-29T16:46:35.000Z ,\n      date_format =  //default: ISO8601 ,\n      date_out = Date.parse(date_in);\nemit\n| put date_in =  2014-09-22Z+08:00 ,\n      date_format =  //default: Date fallback with timezone ,\n      date_out = Date.parse(date_in);\nemit\n| put date_in =  Wed Dec 2 18:56:48 2015 UTC ,\n      date_format =  //default: acceptable non-ISO8601 ,\n      date_out = Date.parse(date_in);", 
            "title": "Date.parse"
        }, 
        {
            "location": "/modules/date/#format", 
            "text": "Whitespace in the format string should match whitespace in the incoming string.  Different separators such as  -  vs  /  for date components,  .  vs  ,  before milliseconds, are supported. However, if the timestamp in your data is enclosed in brackets, such as  [2015/10/29 16:46:35] , Date.parse will only handle the enclosed string and not the brackets.  When Date.parse is called with a single parameter of input string, without a custom format specification, it will parse several variants of ISO-8601 dates.", 
            "title": "Format"
        }, 
        {
            "location": "/modules/date/#timezone-support", 
            "text": "Juttle follows momentjs format for timezone specification, namely, as offset from UTC +HH:mm  or  -HH:mm ,  +HHmm  or  -HHmm , or the literal  Z  to stand for UTC (GMT) time zone.  Date.parse does not handle acronyms such as \"PDT\", or longform such as \"GMT-0700 (PDT)\".", 
            "title": "Timezone support"
        }, 
        {
            "location": "/modules/date/#datequantize", 
            "text": "Return a new moment, equal to the supplied moment rounded down to an\neven number of durations since the epoch.  Date.quantize(moment, duration)  Example: get year and month when Pluto lost planet status  emit -limit 1 \n| put oh_pluto = Date.new( 2006-09-13 )\n| put that_month = Date.quantize(oh_pluto, :M:), that_year = Date.quantize(oh_pluto, :y:) \n| keep oh_pluto, that_month, that_year", 
            "title": "Date.quantize"
        }, 
        {
            "location": "/modules/date/#datestartof", 
            "text": "Return a new moment, equal to the supplied moment set to the start of\nits current time unit.  Date.startOf(moment, timeunit)    Note:  The new moment is the start of the unit in the time zone in which moment was recorded, not the time zone in which it is displayed.  Example: beginning of a given day  emit -limit 1 | put today = Date.startOf(time,  day )", 
            "title": "Date.startOf"
        }, 
        {
            "location": "/modules/date/#datetime", 
            "text": "Return the current moment, at millisecond resolution.  Date.time()  Example: Show the difference between a time stamp and real time  const start = Date.time();\nemit -from start\n| put elapsed = time - start, clock = Date.time(), skew = clock - time \n| view table", 
            "title": "Date.time"
        }, 
        {
            "location": "/modules/date/#datetostring", 
            "text": "Convert a value of type Date to type String. This is necessary to perform string operations such as concatenation.  Date.toString(date)  The returned String is in ISO-8601 format:  YYYY-MM-DDTHH:mm:ss.SSSZ  Example: print current time  emit\n| put now_moment = :now:, now_string = Date.toString(:now:)\n| put time_string =  Current time:   + now_string", 
            "title": "Date.toString()"
        }, 
        {
            "location": "/modules/date/#dateunix", 
            "text": "Return the UNIX time stamp for this moment, as seconds since the epoch.  Date.unix(moment)    Note:  This is equivalent to Duration.seconds(moment - Date.new(0)) .  Example: Compute the number of seconds in a minute, the hard way  const sixty = Date.unix(Date.new( 2014-09-22T00:01:00 )) - Date.unix(Date.new( 2014-09-22 ));\nemit -limit 1 | put seconds_in_minute = sixty", 
            "title": "Date.unix"
        }, 
        {
            "location": "/modules/date/#dateunixms", 
            "text": "Return the UNIX time stamp for this moment, as milliseconds since the\nepoch.  Date.unixms(moment)    Note:  This is equivalent to Duration.milliseconds(moment - Date.new(0)) .  Example: Compute the number of milliseconds in a minute, the hard way  const sixtyaught = Date.unixms(Date.new( 2014-09-22T00:01:00 )) - Date.unixms(Date.new( 2014-09-22 ));\nemit -limit 1 | put ms_in_minute = sixtyaught", 
            "title": "Date.unixms"
        }, 
        {
            "location": "/modules/duration/", 
            "text": "Duration\n\n\nDuration is the representation of a time interval in Juttle, defined as the time span between two Juttle moments; refer to the \nTime module\n for more. The Duration module provides functions for working with durations, including conversion from/to seconds and strings.\n\n\n\n\n\n\nDuration\n\n\nDuration.as\n\n\nDuration.format\n\n\nDuration.get\n\n\nDuration.milliseconds\n\n\nDuration.new\n\n\nDuration.seconds\n\n\nDuration.toString()\n\n\n\n\n\n\n\n\n\n\n\n\nDuration.as\n\n\nReturn the value of duration as a floating-point number in units of\ntimeunit.\n\n\nDuration.as(duration, timeunit)\n\n\n\n\nIf you were to multiply this number by Duration(1, timeunit), you would\nget back the original duration.\n\n\nExample: output elapsed time as seconds\n\n\nconst millenium = Date.new(\n2001-01-01\n);\nemit -limit 1 | put elapsed_seconds_third_millenium = Duration.as(:now: - millenium, \nseconds\n)\n\n\n\n\n\n\nDuration.format\n\n\nFormat the duration as a string.\n\n\n   Duration.format(duration)\n   Duration.format(duration, format_string)\n\n\n\n\n\nThe optional \nformat_string\n is as described in the \nnpm moment-duration-format package\n. When no format is given, an approximate format will be built based on the magnitude of the duration.\n\n\nExample\n\n\nemit -limit 1\n| put duration = :2 months and 2 weeks and 2 days and 1 hour and 3 minutes:\n| put default=Duration.format(duration)              // approximate, depends on duration magnitude\n| put asdays=Duration.format(duration, \nDD [days]\n)  // rounded to nearest whole day\n| put dotnet=Duration.format(duration,\ndd.hh:mm:ss\n) // in .NET format, to nearest second.\n| remove time, duration // duration would display precisely, in a nonstandard mixed calendar/time format\n| view table\n\n\n\n\n\n\nDuration.get\n\n\nReturn the value of a given time unit for a duration (such as, \"days\") as an integer.\n\n\nDuration.get(duration, timeunit)\n\n\n\n\nExample: split duration into its time unit components\n\n\nconst millenial = :now: - Date.new(\n2001-01-01\n);\nemit -limit 1 \n| put y = Duration.get(millenial, \nyears\n), \n      M = Duration.get(millenial, \nmonths\n), \n      d = Duration.get(millenial, \ndays\n),\n      h = Duration.get(millenial, \nhours\n),\n      m = Duration.get(millenial, \nminutes\n),\n      s = Duration.get(millenial, \nseconds\n)\n| put ticker = \nTime lived in this millenium: ${y}y ${M}m ${d}d ${h}h:${m}m:${s}s\n \n| keep ticker\n\n\n\n\n\n\nDuration.milliseconds\n\n\nReturn the value of duration as a floating-point number in milliseconds.\n\n\nDuration.milliseconds(duration)\n\n\n\n\n \nNote:\n Equivalent to \nDuration.as(duration, \"ms\")\n.\n\n\nExample: convert duration of Hadron Epoch to milliseconds\n\n\nemit -limit 1 | put hadron_epoch_ms = Duration.milliseconds(:1s:)\n\n\n\n\n\n\nDuration.new\n\n\nReturn a duration.\n\n\nDuration.new(seconds)\n\n\n\n\nExample: Some ways to represent a minute\n\n\nemit -limit 1 \n| put this_minute = Duration.new(60), that_minute = Duration.new(\n00:01:00\n)\n| put diff = that_minute - this_minute\n\n\n\n\n\n\nDuration.seconds\n\n\nReturn the value of duration as a floating-point number in units of seconds.\n\n\nDuration.seconds(duration)\n\n\n\n\n \nNote:\n Equivalent to \nDuration.as(duration, \"seconds\")\n.\n\n\nExample: how many seconds in a year\n\n\nemit -limit 1 \n| put normal_year_seconds = Duration.seconds(:365 days:), \n      leap_year_seconds = Duration.seconds(:366 days:)\n\n\n\n\n\n\nDuration.toString()\n\n\nConvert a value of type Duration to type String.\n\n\nDuration.toString(duration)\n\n\n\n\nExample: print duration of this day so far\n\n\nemit -limit 1 \n| put today_so_far = :now: - Date.startOf(:now:, \nday\n)\n| put message = \nThis day has lasted \n + Duration.toString(today_so_far)\n| keep message", 
            "title": "____ Duration"
        }, 
        {
            "location": "/modules/duration/#duration", 
            "text": "Duration is the representation of a time interval in Juttle, defined as the time span between two Juttle moments; refer to the  Time module  for more. The Duration module provides functions for working with durations, including conversion from/to seconds and strings.    Duration  Duration.as  Duration.format  Duration.get  Duration.milliseconds  Duration.new  Duration.seconds  Duration.toString()", 
            "title": "Duration"
        }, 
        {
            "location": "/modules/duration/#durationas", 
            "text": "Return the value of duration as a floating-point number in units of\ntimeunit.  Duration.as(duration, timeunit)  If you were to multiply this number by Duration(1, timeunit), you would\nget back the original duration.  Example: output elapsed time as seconds  const millenium = Date.new( 2001-01-01 );\nemit -limit 1 | put elapsed_seconds_third_millenium = Duration.as(:now: - millenium,  seconds )", 
            "title": "Duration.as"
        }, 
        {
            "location": "/modules/duration/#durationformat", 
            "text": "Format the duration as a string.     Duration.format(duration)\n   Duration.format(duration, format_string)  The optional  format_string  is as described in the  npm moment-duration-format package . When no format is given, an approximate format will be built based on the magnitude of the duration.  Example  emit -limit 1\n| put duration = :2 months and 2 weeks and 2 days and 1 hour and 3 minutes:\n| put default=Duration.format(duration)              // approximate, depends on duration magnitude\n| put asdays=Duration.format(duration,  DD [days] )  // rounded to nearest whole day\n| put dotnet=Duration.format(duration, dd.hh:mm:ss ) // in .NET format, to nearest second.\n| remove time, duration // duration would display precisely, in a nonstandard mixed calendar/time format\n| view table", 
            "title": "Duration.format"
        }, 
        {
            "location": "/modules/duration/#durationget", 
            "text": "Return the value of a given time unit for a duration (such as, \"days\") as an integer.  Duration.get(duration, timeunit)  Example: split duration into its time unit components  const millenial = :now: - Date.new( 2001-01-01 );\nemit -limit 1 \n| put y = Duration.get(millenial,  years ), \n      M = Duration.get(millenial,  months ), \n      d = Duration.get(millenial,  days ),\n      h = Duration.get(millenial,  hours ),\n      m = Duration.get(millenial,  minutes ),\n      s = Duration.get(millenial,  seconds )\n| put ticker =  Time lived in this millenium: ${y}y ${M}m ${d}d ${h}h:${m}m:${s}s  \n| keep ticker", 
            "title": "Duration.get"
        }, 
        {
            "location": "/modules/duration/#durationmilliseconds", 
            "text": "Return the value of duration as a floating-point number in milliseconds.  Duration.milliseconds(duration)    Note:  Equivalent to  Duration.as(duration, \"ms\") .  Example: convert duration of Hadron Epoch to milliseconds  emit -limit 1 | put hadron_epoch_ms = Duration.milliseconds(:1s:)", 
            "title": "Duration.milliseconds"
        }, 
        {
            "location": "/modules/duration/#durationnew", 
            "text": "Return a duration.  Duration.new(seconds)  Example: Some ways to represent a minute  emit -limit 1 \n| put this_minute = Duration.new(60), that_minute = Duration.new( 00:01:00 )\n| put diff = that_minute - this_minute", 
            "title": "Duration.new"
        }, 
        {
            "location": "/modules/duration/#durationseconds", 
            "text": "Return the value of duration as a floating-point number in units of seconds.  Duration.seconds(duration)    Note:  Equivalent to  Duration.as(duration, \"seconds\") .  Example: how many seconds in a year  emit -limit 1 \n| put normal_year_seconds = Duration.seconds(:365 days:), \n      leap_year_seconds = Duration.seconds(:366 days:)", 
            "title": "Duration.seconds"
        }, 
        {
            "location": "/modules/duration/#durationtostring", 
            "text": "Convert a value of type Duration to type String.  Duration.toString(duration)  Example: print duration of this day so far  emit -limit 1 \n| put today_so_far = :now: - Date.startOf(:now:,  day )\n| put message =  This day has lasted   + Duration.toString(today_so_far)\n| keep message", 
            "title": "Duration.toString()"
        }, 
        {
            "location": "/modules/math/", 
            "text": "Math\n\n\nJuttle Math module provides functions and constants for mathematical operations.\n\n\n\n\n\n\nMath\n\n\nMath constants\n\n\nMath.abs\n\n\nMath.acos\n\n\nMath.asin\n\n\nMath.atan\n\n\nMath.atan2\n\n\nMath.ceil\n\n\nMath.cos\n\n\nMath.exp\n\n\nMath.floor\n\n\nMath.log\n\n\nMath.max\n\n\nMath.min\n\n\nMath.random\n\n\nMath.pow\n\n\nMath.round\n\n\nMath.seed\n\n\nMath.sin\n\n\nMath.sqrt\n\n\nMath.tan\n\n\n\n\n\n\n\n\n\n\n\n\nMath constants\n\n\nJuttle supports these Math constants as well as \nNumber constants\n. As all exports from Math module, they can be referenced without an import directive in a Juttle program.\n\n\n\n\n\n\n\n\nConstant\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMath.E\n\n\nEuler's constant, the base of natural logarithms, approximately 2.718\n\n\n\n\n\n\nMath.LN10\n\n\nThe natural logarithm of 10, approximately 2.302\n\n\n\n\n\n\nMath.LN2\n\n\nThe natural logarithm of 2, approximately 0.693\n\n\n\n\n\n\nMath.LOG2E\n\n\nThe base 2 logarithm of Euler's constant, approximately 1.442\n\n\n\n\n\n\nMath.Log10E\n\n\nThe base 10 logarithm of Euler's constant, approximately 0.434\n\n\n\n\n\n\nMath.PI\n\n\nPi, the ratio of the circumference of a circle to its diameter, approximately 3.14159\n\n\n\n\n\n\nMath.SQRT1_2\n\n\nThe square root of 1/2, approximately 0.707\n\n\n\n\n\n\nMath.SQRT2\n\n\nThe square root of 2, approximately 1.414\n\n\n\n\n\n\n\n\nExample\n\n\nemit\n| put radius = 7\n| put area = Math.round(Math.PI * Math.pow(radius, 2))\n\n\n\n\n\n\nMath.abs\n\n\nReturn the absolute value of a number.\n\n\nMath.abs(number)\n\n\n\n\nExample\n\n\nemit -limit 1 \n| put step1 = 3.1415, step2 = 0.5772\n| put delta = Math.abs(step2 - step1)\n\n\n\n\n\n\nMath.acos\n\n\nReturn the arccosine of a specified number, in radians.\n\n\nMath.acos(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number between -1 and 1\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.asin\n\n\nReturn the arcsine of a specified number, in radians.\n\n\nMath.asin(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number between -1 and 1\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.atan\n\n\nReturn the arctangent of a specified number, in radians.\n\n\nMath.atan(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.atan2\n\n\nReturn the arctangent of the quotient of two numbers, in radians.\n\n\nMath.atan2(y,x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\ny\n\n\nA number representing the Y coordinate of a point\n\n\nYes\n\n\n\n\n\n\nx\n\n\nA number representing the X coordinate of a point\n\n\nYes\n\n\n\n\n\n\n\n\n \nNote:\n The Y coordinate must be specified before the X coordinate.\n\n\n\n\nMath.ceil\n\n\nReturn the smallest integer greater than or equal to a specified number.\n\n\nMath.ceil(number)\n\n\n\n\nExample\n\n\nfunction get_integer(x) { \n  return (x \n= 0) ? Math.floor(x) : Math.ceil(x); \n}\nemit -limit 1 \n| put a = get_integer(3.1415), b = get_integer(-0.5772)\n| view table\n\n\n\n\n\n\nMath.cos\n\n\nReturn the cosine of a specified number.\n\n\nMath.cos(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number, in radians\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.exp\n\n\nReturn e\nx\n, where x is a specified number and \ne\n is Euler's constant,\nthe base of the natural logarithms.\n\n\nMath.exp(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.floor\n\n\nReturn the largest integer less than or equal to a number.\n\n\nMath.floor(number)\n\n\n\n\nExample\n\n\nfunction get_integer(x) { \n  return (x \n= 0) ? Math.floor(x) : Math.ceil(x); \n}\nemit -limit 1 \n| put a = get_integer(3.1415), b = get_integer(-0.5772)\n| view table\n\n\n\n\n\n\nMath.log\n\n\nReturn the natural logarithm of the specified number.\n\n\nMath.log(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.max\n\n\nReturn the largest of one or more numbers.\n\n\nMath.max(x[, y[, .. ]])\n\n\n\n\nExample\n\n\nconst max = Math.max(Math.random(), Math.random(), Math.random());\nemit -limit 1 \n| put max = max \n| view table\n\n\n\n\n\n\n\nMath.min\n\n\nReturn the smallest of one or more numbers.\n\n\nMath.min(x[, y[, .. ]])\n\n\n\n\nExample\n\n\nemit -limit 1 \n| put a = 1, b = 2, c = 3, d = Math.min(a, b, c) \n| view table\n\n\n\n\n\n\nMath.random\n\n\nReturn a floating-point, pseudo-random number in the range [0, 1); that\nis, from 0 (inclusive) up to but not including 1 (exclusive), which you\ncan then scale to your desired range.\n\n\nMath.random()\n\n\n\n\n \nNote:\n The implementation selects the initial seed to the random number generation algorithm. Use \nMath.seed\n to choose your own.\n\n\nExample: Return a random integer between 1 and max\n\n\nfunction randomInt(max) {\n    return Math.ceil(Math.random() * max);\n}\n\nemit -limit 10 \n| put r = randomInt(10) \n| view table\n\n\n\n\n\nExample: Throw rock, paper, or scissors at random\n\n\nconst paper = 'PAPER';\nconst rock = 'ROCK';\nconst scissors = 'SCISSORS';\n\nfunction compute_result(you_threw, i_threw){\n    if(you_threw == i_threw){\n        return \nA DRAW!!\n;\n    }\n    if(you_threw == paper){\n        if(i_threw == rock){\n            return \nYou have defeated me.\n;\n        }\n        else{\n            return \nThe victory is mine!\n;\n        }\n    }\n    if(you_threw == rock){\n        if(i_threw == paper){\n            return \nI win again!\n;\n        }\n        else{\n            return \nYou have bested me once more.\n;\n        }\n    }\n    if(you_threw == scissors){\n        if(i_threw == paper){\n            return \nYour skill is greatest.\n;\n        }\n        else{\n            return \nMy superior talent has given me the victory.\n;\n        }\n    }\n    // If we get here without returning, then we didn't have a pair of arguments\n    // that we recognize.\n    return \nI'm not sure what happened there.\n;\n}\n\nfunction what_did_i_throw(){\n    var rando = Math.random();\n    if(rando \n .33){\n        return rock;\n    }\n    if(rando \n= .33 \n rando \n .67){\n        return paper;\n    }\n    if(rando \n= .67){\n        return scissors;\n    }\n}\n\ninput selection :dropdown\n    -items [rock, paper, scissors]\n    -label \nChoose your weapon\n\n    -default rock;\n\nemit -limit 1\n| put you_threw = selection,\n    i_threw = what_did_i_throw(),\n    result = compute_result(you_threw, i_threw)\n| view table\n    -title \nThe duel\n\n    -columnOrder \ntime\n, \nyou_threw\n, \ni_threw\n, \nresult\n\n\n\n\n\n\n\nMath.pow\n\n\nReturn base to the exponent power, that is, base\nexponent\n.\n\n\nMath.pow(base, exponent)\n\n\n\n\nExample\n\n\nemit -limit 5 \n| put c = count(), power = Math.pow(2, c) \n| keep power \n| view table -title \nPowers of two\n\n\n\n\n\n\n\nMath.round\n\n\nReturn the value of a number rounded to the nearest integer.\n\n\nMath.round(number)\n\n\n\n\nExample\n\n\nemit -limit 1\n| put a = Math.round(3.1415), b = Math.round(-0.5772), c = Math.round(1.5), d = Math.round(42)\n| view table\n\n\n\n\n\n\nMath.seed\n\n\nInitialize the random number generator to a known value. Subsequent\ncalls to\n\nMath.random\n\nwill always generate the same pseudo-random sequence for a given seed\nvalue\n\n\nMath.seed(seed)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nseed\n\n\nRandom number generator seed value. A number.\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Do something predictably random\n\n\nconst ignored = Math.seed(42);\nemit -limit 1\n| put pick_a_number = Math.random()\n| put lucky_guess = 0.0016341939679719736\n| put amazed = (pick_a_number == lucky_guess)\n\n\n\n\n\n\nMath.sin\n\n\nReturn the sine of the specified number.\n\n\nMath.sin(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number, in radians\n\n\nYes\n\n\n\n\n\n\n\n\n\n\nMath.sqrt\n\n\nReturn the positive square root of a number.\n\n\nMath.sqrt(number)\n\n\n\n\nExample: find numbers with whole square roots\n\n\nemit -from :0: -limit 100 \n| put a = count() \n| put b = Math.floor(Math.sqrt(a)) \n| uniq b\n\n\n\n\n\n\nMath.tan\n\n\nReturn the tangent of the specified number.\n\n\nMath.tan(x)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nx\n\n\nA number, in radians\n\n\nYes", 
            "title": "____ Math"
        }, 
        {
            "location": "/modules/math/#math", 
            "text": "Juttle Math module provides functions and constants for mathematical operations.    Math  Math constants  Math.abs  Math.acos  Math.asin  Math.atan  Math.atan2  Math.ceil  Math.cos  Math.exp  Math.floor  Math.log  Math.max  Math.min  Math.random  Math.pow  Math.round  Math.seed  Math.sin  Math.sqrt  Math.tan", 
            "title": "Math"
        }, 
        {
            "location": "/modules/math/#math-constants", 
            "text": "Juttle supports these Math constants as well as  Number constants . As all exports from Math module, they can be referenced without an import directive in a Juttle program.     Constant  Description      Math.E  Euler's constant, the base of natural logarithms, approximately 2.718    Math.LN10  The natural logarithm of 10, approximately 2.302    Math.LN2  The natural logarithm of 2, approximately 0.693    Math.LOG2E  The base 2 logarithm of Euler's constant, approximately 1.442    Math.Log10E  The base 10 logarithm of Euler's constant, approximately 0.434    Math.PI  Pi, the ratio of the circumference of a circle to its diameter, approximately 3.14159    Math.SQRT1_2  The square root of 1/2, approximately 0.707    Math.SQRT2  The square root of 2, approximately 1.414     Example  emit\n| put radius = 7\n| put area = Math.round(Math.PI * Math.pow(radius, 2))", 
            "title": "Math constants"
        }, 
        {
            "location": "/modules/math/#mathabs", 
            "text": "Return the absolute value of a number.  Math.abs(number)  Example  emit -limit 1 \n| put step1 = 3.1415, step2 = 0.5772\n| put delta = Math.abs(step2 - step1)", 
            "title": "Math.abs"
        }, 
        {
            "location": "/modules/math/#mathacos", 
            "text": "Return the arccosine of a specified number, in radians.  Math.acos(x)     Parameter  Description  Required?      x  A number between -1 and 1  Yes", 
            "title": "Math.acos"
        }, 
        {
            "location": "/modules/math/#mathasin", 
            "text": "Return the arcsine of a specified number, in radians.  Math.asin(x)     Parameter  Description  Required?      x  A number between -1 and 1  Yes", 
            "title": "Math.asin"
        }, 
        {
            "location": "/modules/math/#mathatan", 
            "text": "Return the arctangent of a specified number, in radians.  Math.atan(x)     Parameter  Description  Required?      x  A number  Yes", 
            "title": "Math.atan"
        }, 
        {
            "location": "/modules/math/#mathatan2", 
            "text": "Return the arctangent of the quotient of two numbers, in radians.  Math.atan2(y,x)     Parameter  Description  Required?      y  A number representing the Y coordinate of a point  Yes    x  A number representing the X coordinate of a point  Yes       Note:  The Y coordinate must be specified before the X coordinate.", 
            "title": "Math.atan2"
        }, 
        {
            "location": "/modules/math/#mathceil", 
            "text": "Return the smallest integer greater than or equal to a specified number.  Math.ceil(number)  Example  function get_integer(x) { \n  return (x  = 0) ? Math.floor(x) : Math.ceil(x); \n}\nemit -limit 1 \n| put a = get_integer(3.1415), b = get_integer(-0.5772)\n| view table", 
            "title": "Math.ceil"
        }, 
        {
            "location": "/modules/math/#mathcos", 
            "text": "Return the cosine of a specified number.  Math.cos(x)     Parameter  Description  Required?      x  A number, in radians  Yes", 
            "title": "Math.cos"
        }, 
        {
            "location": "/modules/math/#mathexp", 
            "text": "Return e x , where x is a specified number and  e  is Euler's constant,\nthe base of the natural logarithms.  Math.exp(x)     Parameter  Description  Required?      x  A number  Yes", 
            "title": "Math.exp"
        }, 
        {
            "location": "/modules/math/#mathfloor", 
            "text": "Return the largest integer less than or equal to a number.  Math.floor(number)  Example  function get_integer(x) { \n  return (x  = 0) ? Math.floor(x) : Math.ceil(x); \n}\nemit -limit 1 \n| put a = get_integer(3.1415), b = get_integer(-0.5772)\n| view table", 
            "title": "Math.floor"
        }, 
        {
            "location": "/modules/math/#mathlog", 
            "text": "Return the natural logarithm of the specified number.  Math.log(x)     Parameter  Description  Required?      x  A number  Yes", 
            "title": "Math.log"
        }, 
        {
            "location": "/modules/math/#mathmax", 
            "text": "Return the largest of one or more numbers.  Math.max(x[, y[, .. ]])  Example  const max = Math.max(Math.random(), Math.random(), Math.random());\nemit -limit 1 \n| put max = max \n| view table", 
            "title": "Math.max"
        }, 
        {
            "location": "/modules/math/#mathmin", 
            "text": "Return the smallest of one or more numbers.  Math.min(x[, y[, .. ]])  Example  emit -limit 1 \n| put a = 1, b = 2, c = 3, d = Math.min(a, b, c) \n| view table", 
            "title": "Math.min"
        }, 
        {
            "location": "/modules/math/#mathrandom", 
            "text": "Return a floating-point, pseudo-random number in the range [0, 1); that\nis, from 0 (inclusive) up to but not including 1 (exclusive), which you\ncan then scale to your desired range.  Math.random()    Note:  The implementation selects the initial seed to the random number generation algorithm. Use  Math.seed  to choose your own.  Example: Return a random integer between 1 and max  function randomInt(max) {\n    return Math.ceil(Math.random() * max);\n}\n\nemit -limit 10 \n| put r = randomInt(10) \n| view table  Example: Throw rock, paper, or scissors at random  const paper = 'PAPER';\nconst rock = 'ROCK';\nconst scissors = 'SCISSORS';\n\nfunction compute_result(you_threw, i_threw){\n    if(you_threw == i_threw){\n        return  A DRAW!! ;\n    }\n    if(you_threw == paper){\n        if(i_threw == rock){\n            return  You have defeated me. ;\n        }\n        else{\n            return  The victory is mine! ;\n        }\n    }\n    if(you_threw == rock){\n        if(i_threw == paper){\n            return  I win again! ;\n        }\n        else{\n            return  You have bested me once more. ;\n        }\n    }\n    if(you_threw == scissors){\n        if(i_threw == paper){\n            return  Your skill is greatest. ;\n        }\n        else{\n            return  My superior talent has given me the victory. ;\n        }\n    }\n    // If we get here without returning, then we didn't have a pair of arguments\n    // that we recognize.\n    return  I'm not sure what happened there. ;\n}\n\nfunction what_did_i_throw(){\n    var rando = Math.random();\n    if(rando   .33){\n        return rock;\n    }\n    if(rando  = .33   rando   .67){\n        return paper;\n    }\n    if(rando  = .67){\n        return scissors;\n    }\n}\n\ninput selection :dropdown\n    -items [rock, paper, scissors]\n    -label  Choose your weapon \n    -default rock;\n\nemit -limit 1\n| put you_threw = selection,\n    i_threw = what_did_i_throw(),\n    result = compute_result(you_threw, i_threw)\n| view table\n    -title  The duel \n    -columnOrder  time ,  you_threw ,  i_threw ,  result", 
            "title": "Math.random"
        }, 
        {
            "location": "/modules/math/#mathpow", 
            "text": "Return base to the exponent power, that is, base exponent .  Math.pow(base, exponent)  Example  emit -limit 5 \n| put c = count(), power = Math.pow(2, c) \n| keep power \n| view table -title  Powers of two", 
            "title": "Math.pow"
        }, 
        {
            "location": "/modules/math/#mathround", 
            "text": "Return the value of a number rounded to the nearest integer.  Math.round(number)  Example  emit -limit 1\n| put a = Math.round(3.1415), b = Math.round(-0.5772), c = Math.round(1.5), d = Math.round(42)\n| view table", 
            "title": "Math.round"
        }, 
        {
            "location": "/modules/math/#mathseed", 
            "text": "Initialize the random number generator to a known value. Subsequent\ncalls to Math.random \nwill always generate the same pseudo-random sequence for a given seed\nvalue  Math.seed(seed)     Parameter  Description  Required?      seed  Random number generator seed value. A number.  Yes     Example: Do something predictably random  const ignored = Math.seed(42);\nemit -limit 1\n| put pick_a_number = Math.random()\n| put lucky_guess = 0.0016341939679719736\n| put amazed = (pick_a_number == lucky_guess)", 
            "title": "Math.seed"
        }, 
        {
            "location": "/modules/math/#mathsin", 
            "text": "Return the sine of the specified number.  Math.sin(x)     Parameter  Description  Required?      x  A number, in radians  Yes", 
            "title": "Math.sin"
        }, 
        {
            "location": "/modules/math/#mathsqrt", 
            "text": "Return the positive square root of a number.  Math.sqrt(number)  Example: find numbers with whole square roots  emit -from :0: -limit 100 \n| put a = count() \n| put b = Math.floor(Math.sqrt(a)) \n| uniq b", 
            "title": "Math.sqrt"
        }, 
        {
            "location": "/modules/math/#mathtan", 
            "text": "Return the tangent of the specified number.  Math.tan(x)     Parameter  Description  Required?      x  A number, in radians  Yes", 
            "title": "Math.tan"
        }, 
        {
            "location": "/modules/number/", 
            "text": "Number\n\n\nJuttle Number module provides constants for numeric operations, and functions for number/string conversion. Refer to the \nMath module\n for other mathematical constants and functions.\n\n\n\n\n\n\nNumber\n\n\nNumber constants\n\n\nNumber.fromString()\n\n\nNumber.toString()\n\n\n\n\n\n\n\n\n\n\n\n\nNumber constants\n\n\nJuttle supports these Number constants as well as \nMath constants\n. They can be referenced without an import directive in a Juttle program, but are more commonly found in the output of certain mathematical operations than used explicitly.\n\n\n\n\n\n\n\n\nConstant\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNumber.MAX_VALUE\n\n\nLargest positive numeric value, approximately \n1.7976931348623157e+308\n \nLarger values are converted to \nInfinity\n\n\n\n\n\n\nNumber.MIN_VALUE\n\n\nSmallest positive numeric value (closest to zero), approximately \n5e-324\n \nSmaller values are converted to \n0\nNote that the most negative number is \n-Number.MAX_VALUE\n\n\n\n\n\n\nNumber.NaN\n\n\nNot-A-Number value, returned from illegal mathematical operations; always compares unequal to any other value including \nNaN\n \nThe floating point literal \nNaN\ncan be used for shorter notation.\n\n\n\n\n\n\nNumber.NEGATIVE_INFINITY\n\n\n-Infinity\nvalue which is less than \n-Number.MAX_VALUE\n \nThe floating point literal \n-Infinity\ncan be used for shorter notation.\n\n\n\n\n\n\nNumber.POSITIVE_INFINITY\n\n\nInfinity\nvalue which is greater than \nNumber.MAX_VALUE\n \nThe floating point literal \nInfinitiy\ncan be used for shorter notation.\n\n\n\n\n\n\n\n\nExample\n\n\nconst pos_inf = Number.POSITIVE_INFINITY;\nemit \n| put div_by_zero = 1 / 0, inf = pos_inf\n| filter div_by_zero = pos_inf\n| filter inf \n Number.MAX_VALUE\n| put nan = inf / inf\n\n\n\n\n\n\nNumber.fromString()\n\n\nConvert a value of type String to type Number.\n\n\nNumber.fromString(string)\n\n\n\n\nExample: Parse out HTTP status code from apache access log line, and convert to a number for mathematical comparison\n\n\nemit -limit 1\n| put message = '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \nGET /apache_pb.gif HTTP/1.0\n 404 347'\n| put http_status = Number.fromString(String.split(message, ' ')[8]) \n| filter http_status \n 200\n\n\n\n\n\n\nNumber.toString()\n\n\nConvert a value of type Number to type String.\n\n\nNumber.toString(number)\n\n\n\n\nExample: print out numeric values\n\n\n Note that multiple ways of printing numbers as part of a string are available. This example shows both Number.toString() conversion, and \nstring interpolation\n without explicit conversion.\n\n\nemit -from :0: -limit 10\n| put value = count(), even = value%2\n| reduce evens = count(even)\n| put msg1 = \nCount of even values: \n + Number.toString(evens),\n      msg2 = \nThere are ${evens} even values", 
            "title": "____ Number"
        }, 
        {
            "location": "/modules/number/#number", 
            "text": "Juttle Number module provides constants for numeric operations, and functions for number/string conversion. Refer to the  Math module  for other mathematical constants and functions.    Number  Number constants  Number.fromString()  Number.toString()", 
            "title": "Number"
        }, 
        {
            "location": "/modules/number/#number-constants", 
            "text": "Juttle supports these Number constants as well as  Math constants . They can be referenced without an import directive in a Juttle program, but are more commonly found in the output of certain mathematical operations than used explicitly.     Constant  Description      Number.MAX_VALUE  Largest positive numeric value, approximately  1.7976931348623157e+308   Larger values are converted to  Infinity    Number.MIN_VALUE  Smallest positive numeric value (closest to zero), approximately  5e-324   Smaller values are converted to  0 Note that the most negative number is  -Number.MAX_VALUE    Number.NaN  Not-A-Number value, returned from illegal mathematical operations; always compares unequal to any other value including  NaN   The floating point literal  NaN can be used for shorter notation.    Number.NEGATIVE_INFINITY  -Infinity value which is less than  -Number.MAX_VALUE   The floating point literal  -Infinity can be used for shorter notation.    Number.POSITIVE_INFINITY  Infinity value which is greater than  Number.MAX_VALUE   The floating point literal  Infinitiy can be used for shorter notation.     Example  const pos_inf = Number.POSITIVE_INFINITY;\nemit \n| put div_by_zero = 1 / 0, inf = pos_inf\n| filter div_by_zero = pos_inf\n| filter inf   Number.MAX_VALUE\n| put nan = inf / inf", 
            "title": "Number constants"
        }, 
        {
            "location": "/modules/number/#numberfromstring", 
            "text": "Convert a value of type String to type Number.  Number.fromString(string)  Example: Parse out HTTP status code from apache access log line, and convert to a number for mathematical comparison  emit -limit 1\n| put message = '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700]  GET /apache_pb.gif HTTP/1.0  404 347'\n| put http_status = Number.fromString(String.split(message, ' ')[8]) \n| filter http_status   200", 
            "title": "Number.fromString()"
        }, 
        {
            "location": "/modules/number/#numbertostring", 
            "text": "Convert a value of type Number to type String.  Number.toString(number)  Example: print out numeric values   Note that multiple ways of printing numbers as part of a string are available. This example shows both Number.toString() conversion, and  string interpolation  without explicit conversion.  emit -from :0: -limit 10\n| put value = count(), even = value%2\n| reduce evens = count(even)\n| put msg1 =  Count of even values:   + Number.toString(evens),\n      msg2 =  There are ${evens} even values", 
            "title": "Number.toString()"
        }, 
        {
            "location": "/modules/object/", 
            "text": "Object\n\n\nObjects can be declared as constants in a flowgraph or variables in\n\nuser defined functions\n.\n\n\nThe Object module exposes a single function, described here.\n\n\n\n\n\n\nObject\n\n\nObject.keys\n\n\n\n\n\n\n\n\n\n\nObject.keys\n\n\nScan an object containing key/value pairs and return a list of its keys.\n\n\nObject.keys(object)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nobject\n\n\nThe object from which to pluck keys\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nemit -from :0: -limit 1\n| put result = Object.keys({'a': 1, 'b': 2})", 
            "title": "____ Object"
        }, 
        {
            "location": "/modules/object/#object", 
            "text": "Objects can be declared as constants in a flowgraph or variables in user defined functions .  The Object module exposes a single function, described here.    Object  Object.keys", 
            "title": "Object"
        }, 
        {
            "location": "/modules/object/#objectkeys", 
            "text": "Scan an object containing key/value pairs and return a list of its keys.  Object.keys(object)     Parameter  Description  Required?      object  The object from which to pluck keys  Yes     Example  emit -from :0: -limit 1\n| put result = Object.keys({'a': 1, 'b': 2})", 
            "title": "Object.keys"
        }, 
        {
            "location": "/modules/string/", 
            "text": "String\n\n\nString module provides Juttle functions to manipulate strings and convert from/to other data types to String. Juttle strings are subject to string coercion and interpolation rules, also described here.\n\n\n\n\n\n\nString\n\n\nString Interpolation\n\n\nString Coercion\n\n\ntoString Conversion\n\n\nDate.toString()\n\n\nDuration.toString()\n\n\nNumber.toString()\n\n\nBoolean.toString()\n\n\nRegExp.toString()\n\n\n\n\n\n\nString.concat\n\n\nString.indexOf\n\n\nString.length\n\n\nString.replace\n\n\nString.search\n\n\nString.slice\n\n\nString.split\n\n\nString.substr\n\n\nString.toLowerCase\n\n\nString.toUpperCase\n\n\n\n\n\n\n\n\n\n\n\n\nString Interpolation\n\n\nIn Juttle, your string literals can include placeholders that contain\nexpressions that generate variable values. Values that aren't originally\nstrings are automatically converted.\n\n\nPlaceholders have this format:\n\n\n${expression}\n\n\n\n\nExample: simple string interpolation\n\n\nconst total = 3;\nemit -limit total\n| put count = count()\n| put message = \nfinished ${count} of ${total}\n\n\n\n\n\nBecause the values of count and total are numbers, Jut automatically\nconverts them to strings.\n\n\nPlaceholders can include more complex expressions, too.\n\n\nExample: complex string interpolation\n\n\nconst total = 3;\nemit -limit total\n| put count = count()\n| put message = \nfinished ${Math.round((count / total) * 100)}%\n\n\n\n\n\n\n\nString Coercion\n\n\nThese are the basic string coercion rules:\n\n\n\n\n\n\nValues for Juttle options are not subject to string coercion.\n\n\nThat is, if you specify an option like \n-option myvalue\n then myvalue\nmust be defined somewhere in your program as a constant or\na variable. If the value of the option should be literally\n\"myvalue\", use \n-option 'myvalue'\n.\n\n\n\n\n\n\nArguments for reducer calls are not subject to string coercion\n\n\nFor example, if your program includes \nreduce count(host)\n then host\nmust be defined elsewhere in your program. If you want to reduce by\ncounting the instances of each value of your data's host field, use\n\nreduce count('host')\n.\n\n\n\n\n\n\n\n\ntoString Conversion\n\n\nWhile not part of the String module, these functions are listed here for convenience.\n\n\nDate.toString()\n\n\nDuration.toString()\n\n\nNumber.toString()\n\n\nBoolean.toString()\n\n\nConvert a value of type Boolean to type String.\n\n\nBoolean.toString(boolean)\n\n\n\n\nExample: print a true/false statement\n\n\nemit -limit 1 | put a=Math.random(), b=Math.random()\n| put c = \na \n b: \n + Boolean.toString(a \n b)\n\n\n\n\nRegExp.toString()\n\n\nConvert a value of type RegExp to type String.\n\n\nRegExp.toString(regular_expression)\n\n\n\n\nThis function replaces the first string it finds. To replace globally,\nappend the \n/g modifier\n to your\nregular expression, like this:\n\n\n/hi/g\n\n\n\n\n\n\nString.concat\n\n\nCombine two or more strings and return a new string.\n\n\nString.concat(string1, string2 [, string3 [, ..] ])\n\n\n\n\n \nTip:\n There are simpler ways to combine\nstrings; see \nString interpolation\n.\nStrings can also be concatenated via the + operator; in the example\nbelow, \nString.concat\n could also be replaced by \nhost + \".\" + domain\n.\n\n\nExample\n\n\nemit -limit 1 \n| put host = \njupiter\n, domain = \nmilkyway.org\n \n| put fqdn = String.concat(host, \n.\n, domain) \n| view text\n\n\n\n\n\n\n\nString.indexOf\n\n\nPerform a substring search and return the index of the first match, or\n-1 if there is no match. See also\n\nString.search\n\nfor regex-based searches.\n\n\nString.indexOf(string, searchstring)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string in which to perform the search\n\n\nYes\n\n\n\n\n\n\nsearchstring\n\n\nThe string to search for\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Find the first instance of \"voodoo\"\n\n\nemit -limit 1 \n| put name=\nThe voodoo that you voodoo\n\n| put newname=String.indexOf(name, \nvoodoo\n)\n| view text\n\n\n\n\n\n\nString.length\n\n\nReturn the length of a string.\n\n\nString.length(string)\n\n\n\n\nExample\n\n\nemit -limit 1 \n| put len = String.length(\njupiter.milkway.org\n) \n| view text\n\n\n\n\n\n\n\nString.replace\n\n\nReplace all or part of a string with another string.\n\n\nString.replace(\noriginal_string\n, \nstring_to_replace\n, \nreplacement_string\n)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\noriginal_string\n\n\nThe original string to manipulate\n\n\nYes\n\n\n\n\n\n\nstring_to_replace\n\n\nThe segment of the string to replace; this can be a regular expression\n\n\nYes\n\n\n\n\n\n\nreplacement_string\n\n\nThe new string\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Transform \"hayneedlehay\" into \"hayFOUNDhay\"\n\n\nemit -from :0: -limit 1 \n| put result = String.replace(\nhayneedlehay\n, \nneedle\n, \nFOUND\n)\n| view text\n\n\n\n\nExample: Transform \"hayneeeeeeedlehay\" (or a similar string with one or more \"e\" characters) into \"hayFOUNDhay\"\n \n\n\nemit -from :0: -limit 1 \n| put result = String.replace(\nhayneeeeeeedlehay\n, /ne+dle/, \nFOUND\n) \n| view text\n\n\n\n\n\n\nString.search\n\n\nSearch for a match between a regular expression and a string. See also\n\nString.indexOf\n\nfor simple substring searches.\n\n\nString.search(string, regexp)\n\n\n\n\nIf successful, this function returns the index of the first match of the\nregular expression inside the string. Otherwise, it returns -1. Regular\nexpression syntax follows common conventions; see for example the\n\nJavaScript documentation\n.\n\n\nExample\n\n\nfunction host_type(host) {\n    if (String.search(host, /^(web|db)?server$/) \n= 0) {\n    return \nserver\n;\n    }\n    if (String.search(host, /switch/) \n= 0) {\n    return \nswitch\n;\n    }\n    return \nunknown\n;\n}\nemit -limit 1 \n| put a = host_type(\nwebserver\n), b = host_type(\ndbserver\n), c=host_type(\nswitch\n) \n| view text\n\n\n\n\n\n\nString.slice\n\n\nExtract a section of a string and return it as a new string. See also \nString.substr\n for length-based extraction.\n\n\nString.slice(string, beginSlice[, endSlice])\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string from which to extract the slice\n\n\nYes\n\n\n\n\n\n\nbeginSlice\n\n\nThe zero-based index at which to begin extraction \nIf this value is negative, it is treated as `(sourceLength+beginSlice)`` where sourceLength is the length of the string. For example, if beginSlice is -3 it is treated as sourceLength-3, or three characters before the end of the string. \n\n\nYes\n\n\n\n\n\n\nendSlice\n\n\nThe zero-based index at which to end extraction \nIf negative, it is treated as (sourceLength-endSlice) where sourceLength is the length of the string.  \n\n\nNo; defaults to the end of the string\n\n\n\n\n\n\n\n\nExample\n\n\nconst name = \nhost-a341-4a29\n;\nemit -limit 1 \n| put id = String.slice(name, -9) \n| view text\n\n\n\n\n\n\n\nString.split\n\n\nSplit a string into an array of strings by separating the string into\nsubstrings.\n\n\nString.split(string, separator)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string to split. String is converted to an array of characters.\n\n\nYes\n\n\n\n\n\n\nseparator\n\n\nOne or more characters to use for separating the string. The separator is treated as if it is an empty string.\n\n\nYes\n\n\n\n\n\n\n\n\nExample\n\n\nemit -limit 1 \n| put host = \nalphaserver-22\n \n| put parts = String.split(host, '-') \n| put id = parts[0] \n| view text\n\n\n\n\n\n\nString.substr\n\n\nExtract a portion of a string beginning at the \nstart\n location through the specified \nlength\n number of characters. Similar to \nString.slice\n, except the optional parameter is \nlength\n rather than ending offset.\n\n\nString.substr(string, start[, length])\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string from which to extract the substring\n\n\nYes\n\n\n\n\n\n\nstart\n\n\nThe character index at which to begin extraction. \nThe \nstart\nlocation of zero (0) maps to the first character of the string; the index of the last character is \nString.length - 1\n. \nIf \nstart\nis positive, it is used as character index counting from the start of the string. If \nstart\nequals or exceeds the length of the string, an empty string will be returned. \nIf \nstart\nvalue is negative, it is used as character index counting from the end of the string. If the absolute value of a negative \nstart\nexceeds the length of the string, zero will be used as the starting index.\n\n\nYes\n\n\n\n\n\n\nlength\n\n\nThe number of characters to extract. \nIf \nlength\nextends beyond the end of the string, fewer characters than \nlength\nwill be returned. If \nlength\nis zero or negative, an empty string will be returned.\n\n\nNo; defaults to extracting until the end of the string is reached.\n\n\n\n\n\n\n\n\nIf \nstart\n is zero and \nlength\n is omitted, the whole string will be returned.\n\n\nExample: substr parameters\n\n\nemit \n| put s = \nHello World!\n \n| put ss0 = String.substr(s, 0),       // Hello World!\n      ss6 = String.substr(s, 6),       // World!\n      ss05 = String.substr(s, 0, 5),   // Hello \n      ss_65 = String.substr(s, -6, 5)  // World\n\n\n\n\nExample: using substr to shorten a name\n \n\n\n// convert a long identifier into a short label\n\nemit -points [\n  {name: \nf79ca0aed8914533269a\n, value: 123},\n  {name: \n15adb9cced74531343b8\n, value: 456}\n]\n| put label = String.substr(name, 0, 6)\n| keep time, label, value \n\n\n\n\n\n\nString.toLowerCase\n\n\nConvert a string to lowercase characters.\n\n\nString.toLowerCase(string)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string to convert to lowercase\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Convert a camel-case string to lowercase\n\n\nemit -limit 1 \n| put name=\nNorthDakota\n\n| put newname=String.toLowerCase(name)\n| view text\n\n\n\n\n\n\nString.toUpperCase\n\n\nConvert a string to uppercase characters.\n\n\nString.toUpperCase(string)\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nstring\n\n\nThe string to convert to uppercase\n\n\nYes\n\n\n\n\n\n\n\n\nExample: Convert a camel-case string to uppercase\n\n\nemit -limit 1 \n| put name=\nNorthDakota\n\n| put newname=String.toUpperCase(name)\n| view text", 
            "title": "____ String"
        }, 
        {
            "location": "/modules/string/#string", 
            "text": "String module provides Juttle functions to manipulate strings and convert from/to other data types to String. Juttle strings are subject to string coercion and interpolation rules, also described here.    String  String Interpolation  String Coercion  toString Conversion  Date.toString()  Duration.toString()  Number.toString()  Boolean.toString()  RegExp.toString()    String.concat  String.indexOf  String.length  String.replace  String.search  String.slice  String.split  String.substr  String.toLowerCase  String.toUpperCase", 
            "title": "String"
        }, 
        {
            "location": "/modules/string/#string-interpolation", 
            "text": "In Juttle, your string literals can include placeholders that contain\nexpressions that generate variable values. Values that aren't originally\nstrings are automatically converted.  Placeholders have this format:  ${expression}  Example: simple string interpolation  const total = 3;\nemit -limit total\n| put count = count()\n| put message =  finished ${count} of ${total}   Because the values of count and total are numbers, Jut automatically\nconverts them to strings.  Placeholders can include more complex expressions, too.  Example: complex string interpolation  const total = 3;\nemit -limit total\n| put count = count()\n| put message =  finished ${Math.round((count / total) * 100)}%", 
            "title": "String Interpolation"
        }, 
        {
            "location": "/modules/string/#string-coercion", 
            "text": "These are the basic string coercion rules:    Values for Juttle options are not subject to string coercion.  That is, if you specify an option like  -option myvalue  then myvalue\nmust be defined somewhere in your program as a constant or\na variable. If the value of the option should be literally\n\"myvalue\", use  -option 'myvalue' .    Arguments for reducer calls are not subject to string coercion  For example, if your program includes  reduce count(host)  then host\nmust be defined elsewhere in your program. If you want to reduce by\ncounting the instances of each value of your data's host field, use reduce count('host') .", 
            "title": "String Coercion"
        }, 
        {
            "location": "/modules/string/#tostring-conversion", 
            "text": "While not part of the String module, these functions are listed here for convenience.", 
            "title": "toString Conversion"
        }, 
        {
            "location": "/modules/string/#datetostring", 
            "text": "", 
            "title": "Date.toString()"
        }, 
        {
            "location": "/modules/string/#durationtostring", 
            "text": "", 
            "title": "Duration.toString()"
        }, 
        {
            "location": "/modules/string/#numbertostring", 
            "text": "", 
            "title": "Number.toString()"
        }, 
        {
            "location": "/modules/string/#booleantostring", 
            "text": "Convert a value of type Boolean to type String.  Boolean.toString(boolean)  Example: print a true/false statement  emit -limit 1 | put a=Math.random(), b=Math.random()\n| put c =  a   b:   + Boolean.toString(a   b)", 
            "title": "Boolean.toString()"
        }, 
        {
            "location": "/modules/string/#regexptostring", 
            "text": "Convert a value of type RegExp to type String.  RegExp.toString(regular_expression)  This function replaces the first string it finds. To replace globally,\nappend the  /g modifier  to your\nregular expression, like this:  /hi/g", 
            "title": "RegExp.toString()"
        }, 
        {
            "location": "/modules/string/#stringconcat", 
            "text": "Combine two or more strings and return a new string.  String.concat(string1, string2 [, string3 [, ..] ])    Tip:  There are simpler ways to combine\nstrings; see  String interpolation .\nStrings can also be concatenated via the + operator; in the example\nbelow,  String.concat  could also be replaced by  host + \".\" + domain .  Example  emit -limit 1 \n| put host =  jupiter , domain =  milkyway.org  \n| put fqdn = String.concat(host,  . , domain) \n| view text", 
            "title": "String.concat"
        }, 
        {
            "location": "/modules/string/#stringindexof", 
            "text": "Perform a substring search and return the index of the first match, or\n-1 if there is no match. See also String.search \nfor regex-based searches.  String.indexOf(string, searchstring)     Parameter  Description  Required?      string  The string in which to perform the search  Yes    searchstring  The string to search for  Yes     Example: Find the first instance of \"voodoo\"  emit -limit 1 \n| put name= The voodoo that you voodoo \n| put newname=String.indexOf(name,  voodoo )\n| view text", 
            "title": "String.indexOf"
        }, 
        {
            "location": "/modules/string/#stringlength", 
            "text": "Return the length of a string.  String.length(string)  Example  emit -limit 1 \n| put len = String.length( jupiter.milkway.org ) \n| view text", 
            "title": "String.length"
        }, 
        {
            "location": "/modules/string/#stringreplace", 
            "text": "Replace all or part of a string with another string.  String.replace( original_string ,  string_to_replace ,  replacement_string )     Parameter  Description  Required?      original_string  The original string to manipulate  Yes    string_to_replace  The segment of the string to replace; this can be a regular expression  Yes    replacement_string  The new string  Yes     Example: Transform \"hayneedlehay\" into \"hayFOUNDhay\"  emit -from :0: -limit 1 \n| put result = String.replace( hayneedlehay ,  needle ,  FOUND )\n| view text  Example: Transform \"hayneeeeeeedlehay\" (or a similar string with one or more \"e\" characters) into \"hayFOUNDhay\"    emit -from :0: -limit 1 \n| put result = String.replace( hayneeeeeeedlehay , /ne+dle/,  FOUND ) \n| view text", 
            "title": "String.replace"
        }, 
        {
            "location": "/modules/string/#stringsearch", 
            "text": "Search for a match between a regular expression and a string. See also String.indexOf \nfor simple substring searches.  String.search(string, regexp)  If successful, this function returns the index of the first match of the\nregular expression inside the string. Otherwise, it returns -1. Regular\nexpression syntax follows common conventions; see for example the JavaScript documentation .  Example  function host_type(host) {\n    if (String.search(host, /^(web|db)?server$/)  = 0) {\n    return  server ;\n    }\n    if (String.search(host, /switch/)  = 0) {\n    return  switch ;\n    }\n    return  unknown ;\n}\nemit -limit 1 \n| put a = host_type( webserver ), b = host_type( dbserver ), c=host_type( switch ) \n| view text", 
            "title": "String.search"
        }, 
        {
            "location": "/modules/string/#stringslice", 
            "text": "Extract a section of a string and return it as a new string. See also  String.substr  for length-based extraction.  String.slice(string, beginSlice[, endSlice])     Parameter  Description  Required?      string  The string from which to extract the slice  Yes    beginSlice  The zero-based index at which to begin extraction  If this value is negative, it is treated as `(sourceLength+beginSlice)`` where sourceLength is the length of the string. For example, if beginSlice is -3 it is treated as sourceLength-3, or three characters before the end of the string.   Yes    endSlice  The zero-based index at which to end extraction  If negative, it is treated as (sourceLength-endSlice) where sourceLength is the length of the string.    No; defaults to the end of the string     Example  const name =  host-a341-4a29 ;\nemit -limit 1 \n| put id = String.slice(name, -9) \n| view text", 
            "title": "String.slice"
        }, 
        {
            "location": "/modules/string/#stringsplit", 
            "text": "Split a string into an array of strings by separating the string into\nsubstrings.  String.split(string, separator)     Parameter  Description  Required?      string  The string to split. String is converted to an array of characters.  Yes    separator  One or more characters to use for separating the string. The separator is treated as if it is an empty string.  Yes     Example  emit -limit 1 \n| put host =  alphaserver-22  \n| put parts = String.split(host, '-') \n| put id = parts[0] \n| view text", 
            "title": "String.split"
        }, 
        {
            "location": "/modules/string/#stringsubstr", 
            "text": "Extract a portion of a string beginning at the  start  location through the specified  length  number of characters. Similar to  String.slice , except the optional parameter is  length  rather than ending offset.  String.substr(string, start[, length])     Parameter  Description  Required?      string  The string from which to extract the substring  Yes    start  The character index at which to begin extraction.  The  start location of zero (0) maps to the first character of the string; the index of the last character is  String.length - 1 .  If  start is positive, it is used as character index counting from the start of the string. If  start equals or exceeds the length of the string, an empty string will be returned.  If  start value is negative, it is used as character index counting from the end of the string. If the absolute value of a negative  start exceeds the length of the string, zero will be used as the starting index.  Yes    length  The number of characters to extract.  If  length extends beyond the end of the string, fewer characters than  length will be returned. If  length is zero or negative, an empty string will be returned.  No; defaults to extracting until the end of the string is reached.     If  start  is zero and  length  is omitted, the whole string will be returned.  Example: substr parameters  emit \n| put s =  Hello World!  \n| put ss0 = String.substr(s, 0),       // Hello World!\n      ss6 = String.substr(s, 6),       // World!\n      ss05 = String.substr(s, 0, 5),   // Hello \n      ss_65 = String.substr(s, -6, 5)  // World  Example: using substr to shorten a name    // convert a long identifier into a short label\n\nemit -points [\n  {name:  f79ca0aed8914533269a , value: 123},\n  {name:  15adb9cced74531343b8 , value: 456}\n]\n| put label = String.substr(name, 0, 6)\n| keep time, label, value", 
            "title": "String.substr"
        }, 
        {
            "location": "/modules/string/#stringtolowercase", 
            "text": "Convert a string to lowercase characters.  String.toLowerCase(string)     Parameter  Description  Required?      string  The string to convert to lowercase  Yes     Example: Convert a camel-case string to lowercase  emit -limit 1 \n| put name= NorthDakota \n| put newname=String.toLowerCase(name)\n| view text", 
            "title": "String.toLowerCase"
        }, 
        {
            "location": "/modules/string/#stringtouppercase", 
            "text": "Convert a string to uppercase characters.  String.toUpperCase(string)     Parameter  Description  Required?      string  The string to convert to uppercase  Yes     Example: Convert a camel-case string to uppercase  emit -limit 1 \n| put name= NorthDakota \n| put newname=String.toUpperCase(name)\n| view text", 
            "title": "String.toUpperCase"
        }, 
        {
            "location": "/reference/time/", 
            "text": "Time\n\n\nTime in Juttle is represented by \"moment\" and \"duration\" types.\n\n\n\n\n\n\nTime\n\n\nMoments and Durations\n\n\nTime Notation\n\n\nAbsolute Moments\n\n\nDurations\n\n\nRelative Moments\n\n\nInfinite Moments and Durations\n\n\n\n\n\n\nTime Arithmetic\n\n\n\n\n\n\n\n\n\n\n\n\nMoments and Durations\n\n\nTime is central to processing in Juttle. Each point that is output by a\nsource node has a time value, stored in the field named\u00a0time, which\nrepresents the date+time\u00a0of the point. We refer to these date+time\nvalues as\u00a0\nmoments\n, and the span of time between two moments\nas\u00a0\ndurations\n.\n\n\nThe time field always holds a moment. You can also assign moments and\ndurations to other fields, do simple arithmetic on moments, and specify\nmoments as parameters to other\n\nFunctions\n,\n\nReducers\n and\n\nProcessors\n.\n\n\nYou can specify moments and durations using moment literals like \n:2 weeks ago:\n, or with date and time functions in \nDate\n and \nDuration\n modules.\u00a0You can also create new moments and durations by doing time arithmetic on existing moments and durations.\n\n\n\n\nTime Notation\n\n\nIn Juttle, you specify time using a string between two colons. Absolute\nmoments are specified as ISO 8601 strings, like \n:1999-12-31:\n.\nDurations and relative moments are specified using a natural-language\nnotation, like \n:this minute:\n.\n\n\n \nTip:\n It's important to know whether your\ndata was imported with a valid time field, and whether the time value\nincluded a time zone. Points without a time zone are imported and\nqueried as UTC, and points without a valid time field receive one that\nuses the import time.\n\n\nA moment or duration is surrounded by colons. Spaces between the colons\nand the time are optional. All times are UTC0 unless a time zone is\nspecified by appending it to an ISO-8601 string.\n\n\nAbsolute Moments\n\n\n\n\nMidnight on a specific date (UTC0):\n\n\n\n\n:2014-09-21:\n\n\n\n\n\n\nAn \nISO-8601\n date and time, UTC0:\n\n\n\n\n: 2014-09-22T11:39:17.993 :\n\n\n\n\n\n\nThe same date and time, as Pacific Standard Time:\n\n\n\n\n: 2014-09-22T03:39:17.993-08:00 :\n\n\n\n\nDurations\n\n\n\n\nOne second, minute, hour, day, week, and so on:\n\n\n\n\n:second:\n:minute:\n:hour:\n:day:\n:week:\n:month:\n:year:\n\n\n\n\nNote that :month: and :year: are special \"calendar\" durations that do\nnot have fixed length but instead advance whole months or years relative\nto a fixed moment.\n\n\n\n\nAbbreviations for 0 seconds, 1 minute, 2 hours, 3 days, 4 weeks, 5\nmonths, 6 years:\n\n\n\n\n:0s:\n:1m:\n:2h:\n:3d:\n:4w:\n:5M:\n:6y:\n\n\n\n\n\n\nTwo ways to write one hour and twenty-three minutes:\n\n\n\n\n: 1 hour and 23 minutes :\n: 01:23:00 :\n\n\n\n\n\n\nAdditional examples:\n\n\n\n\n: 1 second :\n: 20 minutes :\n\n\n\n\nRelative Moments\n\n\n\n\nThe moment at which the program started running:\n\n\n\n\n:now:\n\n\n\n\nThis moment does not change over the life of a program. If you want the\ncontinuously updated wall clock time, use\n\nDate.time\n.\n\n\n\n\nMidnight yesterday, today, or tomorrow:\n\n\n\n\n:yesterday:\n:today:\n:tomorrow:\n\n\n\n\n\n\nSeven hours after midnight on the current day:\n\n\n\n\n: 07:00:00 after today :\n\n\n\n\n\n\nOne minute from the start of the program's execution:\n\n\n\n\n: 1 minute from now :\n\n\n\n\n\n\nShorter way to write \n:2 minutes from :now:\n:\n\n\n\n\n:+2m:\n\n\n\n\n\n\nShorter way to write \n:1 hour and 10 minutes before now:\n:\n\n\n\n\n:-01:10:00:\n\n\n\n\n\n\nNow minus 72 hours:\n\n\n\n\n: 3 days ago :\n\n\n\n\n\n\nNow minus 22 days:\n\n\n\n\n: 3 weeks and 1 day ago :\n\n\n\n\n\n\nMidnight of the first day of the current calendar month:\n\n\n\n\n:this month:\n\n\n\n\n\n\nMidnight of the first day of the previous calendar month:\n\n\n\n\n:last month:\n\n\n\n\n\n\nMidnight of the 20th day of the previous calendar month:\n\n\n\n\n:day 20 of last month:\n\n\n\n\n\n\nThe minute in which the program began executing:\n\n\n\n\n:this minute:\n\n\n\n\n\n\nThe next even hour after the start of the program's execution:\n\n\n\n\n:next hour:\n\n\n\n\n\n\nThe day in which the program began executing:\n\n\n\n\n:this day:\n:today:\n\n\n\n\n\n\nThe day before the day in which the program began executing:\n\n\n\n\n:last day:\n:yesterday:\n\n\n\n\nInfinite Moments and Durations\n\n\n\n\nA historical duration beginning at the first moment for which data is\navailable, and ending at the moment the program was started:\n\n\n\n\n:forever:\n\n\n\n\n(valid in -to and -last)\n\n\n\n\nHistorical data from the first moment for which data exists:\n\n\n\n\n:beginning:\n\n\n\n\n(valid in -from)\n\n\n\n\nLive data until the last moment for which data exists:\n\n\n\n\n:end:\n\n\n\n\n(valid in -to)\n\n\n\n\nTime Arithmetic\n\n\nIn Juttle, you can do arithmetic directly with moments and durations,\nand such arithmetic produces moments, durations, or numbers depending on\nthe operation you perform.\n\n\nTime in Juttle is a distinct type (separate from numbers, Booleans,\netc), and arithmetic on time is possible. For example, a duration may be\nadded to a moment to produce a new moment that is shifted in time. A\nmoment may be subtracted from a moment to produce a duration equal to\nthe span of time between the moments. This means you do not need to\nconvert moments and durations into seconds or milliseconds, then back to\nmoments or durations in order to operate on them. It also means that\nnonsensical time arithmetic (like adding two moments) will be flagged as\nan error.\n\n\nHere are the arithmetic operations you can perform with moments, and\ntheir resulting types:\n\n\n\n\n\n\n\n\nOperation\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n:moment: +/- :duration:\n\n\n:moment:\n\n\n\n\n\n\n:moment: - :moment:\n\n\n:duration:\n\n\n\n\n\n\n:duration: +/- :duration:\n\n\n:duration:\n\n\n\n\n\n\nnumber * :duration:\n\n\n:duration:\n\n\n\n\n\n\n:duration: / :duration:\n\n\nnumber\n\n\n\n\n\n\n:duration: % :duration:\n\n\n:duration: (the remainder of dividing the durations)\n\n\n\n\n\n\n\n\nCalendar durations involving :months: or :years: may also be combined\narithmetically. Note that adding a calendar offset to a moment produces\nanother moment, so \"calendarness\" is no longer relevant (for example,\n\n:now: + :6 months:\n is the moment 6 calendar months from now, but it is\nlike any other moment once computed). Mixed calendar/non-calendar\ndurations (for example, \n:2 months: + :1 day:\n) are possible, but these\nare not generally accepted by Juttle procs wanting durations as their\narguments.\n\n\nExamples\n\n\n:tomorrow: == :yesterday: + :48 hours:\n\n:hour: / :minute: == 60\n\n2 \\* :60 seconds: + :hour: / 60 == :3 minutes:\n\n:now: + :hour: / 2 == :0.5 hours from now:\n\n:this hour: - :today: == (:this hour: - Date.new(0)) % :day:\n\n\n\n\nExample: count frequency of points over time\n\n\nThe following Juttle example counts the number of points over a\nspecified interval, then reports the average number of points per week.\nOne way to understand the moment and duration arithmetic involved is to\nthink of count() / DURATION as being an absolute rate (counts per unit\ntime), which we multiply by :week: to get the expected number of points\nin a week at that rate. Since we cannot write count() / DURATION\ndirectly, we group (DURATION / :w:) to produce a number (durations per\nweek), and count is then divided by this number to produce counts per\nweek.\n\n\nconst START = :2 months ago:;\nconst DURATION = :now: - START; \nemit -every :1 hour: -from START -to :now: | reduce points_per_week = (count() / (DURATION / :w:))", 
            "title": "Time"
        }, 
        {
            "location": "/reference/time/#time", 
            "text": "Time in Juttle is represented by \"moment\" and \"duration\" types.    Time  Moments and Durations  Time Notation  Absolute Moments  Durations  Relative Moments  Infinite Moments and Durations    Time Arithmetic", 
            "title": "Time"
        }, 
        {
            "location": "/reference/time/#moments-and-durations", 
            "text": "Time is central to processing in Juttle. Each point that is output by a\nsource node has a time value, stored in the field named\u00a0time, which\nrepresents the date+time\u00a0of the point. We refer to these date+time\nvalues as\u00a0 moments , and the span of time between two moments\nas\u00a0 durations .  The time field always holds a moment. You can also assign moments and\ndurations to other fields, do simple arithmetic on moments, and specify\nmoments as parameters to other Functions , Reducers  and Processors .  You can specify moments and durations using moment literals like  :2 weeks ago: , or with date and time functions in  Date  and  Duration  modules.\u00a0You can also create new moments and durations by doing time arithmetic on existing moments and durations.", 
            "title": "Moments and Durations"
        }, 
        {
            "location": "/reference/time/#time-notation", 
            "text": "In Juttle, you specify time using a string between two colons. Absolute\nmoments are specified as ISO 8601 strings, like  :1999-12-31: .\nDurations and relative moments are specified using a natural-language\nnotation, like  :this minute: .    Tip:  It's important to know whether your\ndata was imported with a valid time field, and whether the time value\nincluded a time zone. Points without a time zone are imported and\nqueried as UTC, and points without a valid time field receive one that\nuses the import time.  A moment or duration is surrounded by colons. Spaces between the colons\nand the time are optional. All times are UTC0 unless a time zone is\nspecified by appending it to an ISO-8601 string.", 
            "title": "Time Notation"
        }, 
        {
            "location": "/reference/time/#absolute-moments", 
            "text": "Midnight on a specific date (UTC0):   :2014-09-21:   An  ISO-8601  date and time, UTC0:   : 2014-09-22T11:39:17.993 :   The same date and time, as Pacific Standard Time:   : 2014-09-22T03:39:17.993-08:00 :", 
            "title": "Absolute Moments"
        }, 
        {
            "location": "/reference/time/#durations", 
            "text": "One second, minute, hour, day, week, and so on:   :second:\n:minute:\n:hour:\n:day:\n:week:\n:month:\n:year:  Note that :month: and :year: are special \"calendar\" durations that do\nnot have fixed length but instead advance whole months or years relative\nto a fixed moment.   Abbreviations for 0 seconds, 1 minute, 2 hours, 3 days, 4 weeks, 5\nmonths, 6 years:   :0s:\n:1m:\n:2h:\n:3d:\n:4w:\n:5M:\n:6y:   Two ways to write one hour and twenty-three minutes:   : 1 hour and 23 minutes :\n: 01:23:00 :   Additional examples:   : 1 second :\n: 20 minutes :", 
            "title": "Durations"
        }, 
        {
            "location": "/reference/time/#relative-moments", 
            "text": "The moment at which the program started running:   :now:  This moment does not change over the life of a program. If you want the\ncontinuously updated wall clock time, use Date.time .   Midnight yesterday, today, or tomorrow:   :yesterday:\n:today:\n:tomorrow:   Seven hours after midnight on the current day:   : 07:00:00 after today :   One minute from the start of the program's execution:   : 1 minute from now :   Shorter way to write  :2 minutes from :now: :   :+2m:   Shorter way to write  :1 hour and 10 minutes before now: :   :-01:10:00:   Now minus 72 hours:   : 3 days ago :   Now minus 22 days:   : 3 weeks and 1 day ago :   Midnight of the first day of the current calendar month:   :this month:   Midnight of the first day of the previous calendar month:   :last month:   Midnight of the 20th day of the previous calendar month:   :day 20 of last month:   The minute in which the program began executing:   :this minute:   The next even hour after the start of the program's execution:   :next hour:   The day in which the program began executing:   :this day:\n:today:   The day before the day in which the program began executing:   :last day:\n:yesterday:", 
            "title": "Relative Moments"
        }, 
        {
            "location": "/reference/time/#infinite-moments-and-durations", 
            "text": "A historical duration beginning at the first moment for which data is\navailable, and ending at the moment the program was started:   :forever:  (valid in -to and -last)   Historical data from the first moment for which data exists:   :beginning:  (valid in -from)   Live data until the last moment for which data exists:   :end:  (valid in -to)", 
            "title": "Infinite Moments and Durations"
        }, 
        {
            "location": "/reference/time/#time-arithmetic", 
            "text": "In Juttle, you can do arithmetic directly with moments and durations,\nand such arithmetic produces moments, durations, or numbers depending on\nthe operation you perform.  Time in Juttle is a distinct type (separate from numbers, Booleans,\netc), and arithmetic on time is possible. For example, a duration may be\nadded to a moment to produce a new moment that is shifted in time. A\nmoment may be subtracted from a moment to produce a duration equal to\nthe span of time between the moments. This means you do not need to\nconvert moments and durations into seconds or milliseconds, then back to\nmoments or durations in order to operate on them. It also means that\nnonsensical time arithmetic (like adding two moments) will be flagged as\nan error.  Here are the arithmetic operations you can perform with moments, and\ntheir resulting types:     Operation  Result      :moment: +/- :duration:  :moment:    :moment: - :moment:  :duration:    :duration: +/- :duration:  :duration:    number * :duration:  :duration:    :duration: / :duration:  number    :duration: % :duration:  :duration: (the remainder of dividing the durations)     Calendar durations involving :months: or :years: may also be combined\narithmetically. Note that adding a calendar offset to a moment produces\nanother moment, so \"calendarness\" is no longer relevant (for example, :now: + :6 months:  is the moment 6 calendar months from now, but it is\nlike any other moment once computed). Mixed calendar/non-calendar\ndurations (for example,  :2 months: + :1 day: ) are possible, but these\nare not generally accepted by Juttle procs wanting durations as their\narguments.  Examples  :tomorrow: == :yesterday: + :48 hours:\n\n:hour: / :minute: == 60\n\n2 \\* :60 seconds: + :hour: / 60 == :3 minutes:\n\n:now: + :hour: / 2 == :0.5 hours from now:\n\n:this hour: - :today: == (:this hour: - Date.new(0)) % :day:  Example: count frequency of points over time  The following Juttle example counts the number of points over a\nspecified interval, then reports the average number of points per week.\nOne way to understand the moment and duration arithmetic involved is to\nthink of count() / DURATION as being an absolute rate (counts per unit\ntime), which we multiply by :week: to get the expected number of points\nin a week at that rate. Since we cannot write count() / DURATION\ndirectly, we group (DURATION / :w:) to produce a number (durations per\nweek), and count is then divided by this number to produce counts per\nweek.  const START = :2 months ago:;\nconst DURATION = :now: - START; \nemit -every :1 hour: -from START -to :now: | reduce points_per_week = (count() / (DURATION / :w:))", 
            "title": "Time Arithmetic"
        }, 
        {
            "location": "/reference/data_types/", 
            "text": "Data types\n\n\nHere are the data types supported in Juttle data points.\n\n\n\n\n\n\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNull\n\n\nNon-existing or unknown value\n\n\n\n\n\n\nBoolean\n\n\nA logical truth value, either \"true\" or \"false\"\n\n\n\n\n\n\nNumber\n\n\nA number in IEEE 754 64-bit double-precision format\n\n\n\n\n\n\nString\n\n\nZero or more Unicode characters\n\n\n\n\n\n\nRegExp\n\n\nA regular expression, \nas implemented in Javascript\n\n\n\n\n\n\nDate\n\n\nAn exact moment in time, represented by a number\n\n\n\n\n\n\nDuration\n\n\nAn interval between to moments in time, represented by a numerical length\n\n\n\n\n\n\nArray\n\n\nAn ordered sequence of zero or more values\n\n\n\n\n\n\nObject\n\n\nAn unordered collection of zero or more properties, each consisting of a key and a value\n\n\n\n\n\n\n\n\nAll data types supported by Juttle data points are equivalent to their\ncorresponding \nJavaScript types\n,\nexcept Date and Duration which are specific to Juttle.\n\n\nYou can also convert a field to or from type string, using these data\ntyping functions:\n\n\n\n\nBoolean.toString()\n\n\nDate.toString()\n\n\nDuration.toString()\n\n\nNumber.fromString()\n\n\nNumber.toString()\n\n\nRegExp.toString()", 
            "title": "Data Types"
        }, 
        {
            "location": "/reference/data_types/#data-types", 
            "text": "Here are the data types supported in Juttle data points.     Type  Description      Null  Non-existing or unknown value    Boolean  A logical truth value, either \"true\" or \"false\"    Number  A number in IEEE 754 64-bit double-precision format    String  Zero or more Unicode characters    RegExp  A regular expression,  as implemented in Javascript    Date  An exact moment in time, represented by a number    Duration  An interval between to moments in time, represented by a numerical length    Array  An ordered sequence of zero or more values    Object  An unordered collection of zero or more properties, each consisting of a key and a value     All data types supported by Juttle data points are equivalent to their\ncorresponding  JavaScript types ,\nexcept Date and Duration which are specific to Juttle.  You can also convert a field to or from type string, using these data\ntyping functions:   Boolean.toString()  Date.toString()  Duration.toString()  Number.fromString()  Number.toString()  RegExp.toString()", 
            "title": "Data types"
        }, 
        {
            "location": "/reference/operators/", 
            "text": "Operators\n\n\nJuttle supports these logical operators.\n\n\n\n\n\n\n\n\nOperator\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAND,and,\n\n\nConjunctive operators \n is \nshort-circuiting\n while the others are not.\n\n\n\n\n\n\nOR,or,\\\n\n\n\\\n\n\n\n\n\n\nNOT,not,!\n\n\nNegation operators\n\n\n\n\n\n\n==\n\n\nEquality operator \nNote \n=\nis also valid when used in filter expressions, but not in \nif\nconditionals.\n\n\n\n\n\n\n!=\n\n\nInequality operator\n\n\n\n\n\n\n, \n=, \n, \n=\n\n\nIs less than, is less than or equal to, is greater than, is greater than or equal to\n\n\n\n\n\n\n\\~, =\\~\n\n\nMatching operator for matching with \"glob\" expressions or regular expressions. See \nmatching\n below\n\n\n\n\n\n\n!~\n\n\nMatching negation operator\n\n\n\n\n\n\nin\n\n\nInclusion in an array\n\n\n\n\n\n\n?:\n\n\nTernary operator\n\n\n\n\n\n\n??\n\n\nNull coalescing operator\n, a shortcut for a ternary operator with a null check\n\n\n\n\n\n\n\n\nMatching\n\n\nThe \n~\n or \n~=\n operators implement \"glob\" style wildcard matching or regular expression matching on a field. The matching used depends on the type of the literal being compared.\n\n\nGlob matching is used when the operand is a string and supports wildcard characters \n*\n which matches any number of characters and \n?\n which matches a single character: For example:\n\n\nemit\n| put message=\nThe quick brown fox\n\n| filter message~\n*qu?ck*\n\n\n\n\n\nRegular expression matching is performed when the operand is a literal regular expression. For example:\n\n\nemit\n| put message=\nThe quick brown fox\n\n| filter message~/.*[quick]+.*/\n\n\n\n\nShort circuiting\n\n\nBelow are examples that illustrate the difference between\nshort-circuiting and non\u2013short-circuiting behavior.\n\n\nExample: Short-circuiting\n\n\nThis works because if s is null, the second condition won\u2019t be evaluated.\n\n\nfunction isEmpty(s) {\n  return s == null || String.length(s) == 0;\n}\n\nemit | put e = isEmpty(invalidField)\n\n\n\n\nExample: Non-short-circuiting\n\n\nThis produces a type error because if s is null, the second condition will still be evaluated.\n\n\nfunction isEmpty(s) {\n  return s == null OR String.length(s) == 0;\n}\n\nemit | put e = isEmpty(invalidField)", 
            "title": "Operators"
        }, 
        {
            "location": "/reference/operators/#operators", 
            "text": "Juttle supports these logical operators.     Operator  Description      AND,and,  Conjunctive operators   is  short-circuiting  while the others are not.    OR,or,\\  \\    NOT,not,!  Negation operators    ==  Equality operator  Note  = is also valid when used in filter expressions, but not in  if conditionals.    !=  Inequality operator    ,  =,  ,  =  Is less than, is less than or equal to, is greater than, is greater than or equal to    \\~, =\\~  Matching operator for matching with \"glob\" expressions or regular expressions. See  matching  below    !~  Matching negation operator    in  Inclusion in an array    ?:  Ternary operator    ??  Null coalescing operator , a shortcut for a ternary operator with a null check", 
            "title": "Operators"
        }, 
        {
            "location": "/reference/operators/#matching", 
            "text": "The  ~  or  ~=  operators implement \"glob\" style wildcard matching or regular expression matching on a field. The matching used depends on the type of the literal being compared.  Glob matching is used when the operand is a string and supports wildcard characters  *  which matches any number of characters and  ?  which matches a single character: For example:  emit\n| put message= The quick brown fox \n| filter message~ *qu?ck*   Regular expression matching is performed when the operand is a literal regular expression. For example:  emit\n| put message= The quick brown fox \n| filter message~/.*[quick]+.*/", 
            "title": "Matching"
        }, 
        {
            "location": "/reference/operators/#short-circuiting", 
            "text": "Below are examples that illustrate the difference between\nshort-circuiting and non\u2013short-circuiting behavior.  Example: Short-circuiting  This works because if s is null, the second condition won\u2019t be evaluated.  function isEmpty(s) {\n  return s == null || String.length(s) == 0;\n}\n\nemit | put e = isEmpty(invalidField)  Example: Non-short-circuiting  This produces a type error because if s is null, the second condition will still be evaluated.  function isEmpty(s) {\n  return s == null OR String.length(s) == 0;\n}\n\nemit | put e = isEmpty(invalidField)", 
            "title": "Short circuiting"
        }, 
        {
            "location": "/reference/conditionals/", 
            "text": "Conditionals\n\n\nConditionals are expressed in two different ways in Juttle, depending on whether they occur in function context, or in flowgraph context. \n\n\nFunctions and reducers support standard \nif\n/\nelse\n syntax for branching on condition.\n\n\nFlowgraphs and subs do not support \nif\n/\nelse\n and instead can be forked using the \nfilter\n proc to create conditional branch execution. \n\n\nBoth functions/reducers and flowgraphs/subs support the ternary \n?\n operator for conditional assignments, and its shortcut \n??\n version, see \noperators\n.\n\n\nNote that multiple conditions inside functions/reducers can be joined with either \nAND\n/\nOR\n or \n/\n||\n, but different logic applies, as the latter are short-circuiting, see examples for \noperators\n.\n\n\nIn filter expressions, only \nAND\n/\nOR\n/\nNOT\n syntax can be used to join multiple conditions, see examples for \nfilter expressions\n.\n\n\nExample 1: direct conditional forking of the flowgraph\n\n\nemit -from :-1m: -limit 10 | put value=count() \n| put highs = value - 5\n| (\n  filter highs \n 0 | reduce count_highs = count();\n  filter highs \n= 0 | reduce count_lows = count()\n)\n\n\n\n\nExample 2: conditional forking of the flowgraph with if/else logic in a function\n\n\nfunction is_high(v) {\n  if (v \n 5) {\n    return true;\n  }\n  else {\n    return false;\n  }\n}\n\nemit -from :-1m: -limit 10 | put value=count() \n| put high = is_high(value)\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)\n\n\n\n\nExample 3: use of a ternary operator inside the function\n\n\nfunction is_high(v) {\n  return (v \n 5) ? true : false;\n}\n\nemit -from :-1m: -limit 10 \n| put value=count() \n| put high = is_high(value)\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)\n\n\n\n\nExample 4: use of a ternary operator for conditional assignment in a flowgraph\n\n\nemit -from :-1m: -limit 10 \n| put value=count() \n| put high = (value \n 5) ? true : false\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)\n\n\n\n\nExample 5: use of a reducer for conditional computation\n\n\nreducer count_highs(v) {\n  var count = 0;\n  function update() {\n    // field dereferencing to get value *v from field name v\n    if (*v \n 5) {\n      count = count + 1;\n    }\n  }\n  function result() {\n    return count;\n  }\n}\n\nreducer count_lows(v) {\n  var count = 0;\n  function update() {\n    if (*v \n= 5) {\n      count = count + 1;\n    }\n  }\n  function result() {\n    return count;\n  }\n}\n\nemit -from :-1m: -limit 10 | put value=count() \n| reduce high = count_highs(value), low = count_lows(value);", 
            "title": "Conditionals"
        }, 
        {
            "location": "/reference/conditionals/#conditionals", 
            "text": "Conditionals are expressed in two different ways in Juttle, depending on whether they occur in function context, or in flowgraph context.   Functions and reducers support standard  if / else  syntax for branching on condition.  Flowgraphs and subs do not support  if / else  and instead can be forked using the  filter  proc to create conditional branch execution.   Both functions/reducers and flowgraphs/subs support the ternary  ?  operator for conditional assignments, and its shortcut  ??  version, see  operators .  Note that multiple conditions inside functions/reducers can be joined with either  AND / OR  or  / || , but different logic applies, as the latter are short-circuiting, see examples for  operators .  In filter expressions, only  AND / OR / NOT  syntax can be used to join multiple conditions, see examples for  filter expressions .  Example 1: direct conditional forking of the flowgraph  emit -from :-1m: -limit 10 | put value=count() \n| put highs = value - 5\n| (\n  filter highs   0 | reduce count_highs = count();\n  filter highs  = 0 | reduce count_lows = count()\n)  Example 2: conditional forking of the flowgraph with if/else logic in a function  function is_high(v) {\n  if (v   5) {\n    return true;\n  }\n  else {\n    return false;\n  }\n}\n\nemit -from :-1m: -limit 10 | put value=count() \n| put high = is_high(value)\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)  Example 3: use of a ternary operator inside the function  function is_high(v) {\n  return (v   5) ? true : false;\n}\n\nemit -from :-1m: -limit 10 \n| put value=count() \n| put high = is_high(value)\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)  Example 4: use of a ternary operator for conditional assignment in a flowgraph  emit -from :-1m: -limit 10 \n| put value=count() \n| put high = (value   5) ? true : false\n| (\n  filter high = true | reduce count_highs = count();\n  filter high = false | reduce count_lows = count()\n)  Example 5: use of a reducer for conditional computation  reducer count_highs(v) {\n  var count = 0;\n  function update() {\n    // field dereferencing to get value *v from field name v\n    if (*v   5) {\n      count = count + 1;\n    }\n  }\n  function result() {\n    return count;\n  }\n}\n\nreducer count_lows(v) {\n  var count = 0;\n  function update() {\n    if (*v  = 5) {\n      count = count + 1;\n    }\n  }\n  function result() {\n    return count;\n  }\n}\n\nemit -from :-1m: -limit 10 | put value=count() \n| reduce high = count_highs(value), low = count_lows(value);", 
            "title": "Conditionals"
        }, 
        {
            "location": "/reference/style_guide/", 
            "text": "Juttle Style Guide\n\n\nJuttle programs in this repository follow conventions of this style guide, as we believe it improves readability.\n\n\nThis guide uses pseudocode in some places. For working Juttle code examples, look at \ndocs/examples\n and \nsimple examples\n on GitHub, and the full program example at the end of this document.\n\n\nComments\n\n\nBoth \n//\n and \n/* .. */\n style comments are supported. Prefix lines of a long comment block with \n*\n. If commenting \n//\n to the right of multiple lines of code, strive to align the comments:\n\n\n...\nemit -points data\n| put *name = value   // transpose data points\n| join                // combine data points with matching timestamps\n...\n\n\n\n\nWhitespace\n\n\n\n\nThe indentation step is 2 spaces. Don\u2019t use tabs.\n\n\nIndent each continuation of a given step in a Juttle pipeline (see Flowgraphs).\n\n\nAvoid trailing whitespace or extra whitespace on blank lines.\n\n\nNo fixed right-hand limit on line length, but be reasonable.\n\n\nOne space before and after \n=\n in assignments:\n\n\n\n\nemit | put good = '1 space', just = 'like that'\n\n\n\n\nFunctions, Reducers, Subgraphs\n\n\nNo space between a function, subgraph or reducer name and opening \n(\n in both declarations and calls. Note that sub invocation has a different parameter syntax than functions or reducers, but follows the same style guidelines.\n\n\nOne space between closing \n)\n and opening \n{\n (do not drop the opening \n{\n down to the next line in declarations).\n\n\nSingle space separation between parameters in function declarations and calls.\n\n\nfunction is_good(a, b) {\n  // function body\n}\n\nreducer get_better(x, when = :now:) {\n  // reducer body\n}\n\nsub stamper(mark, force) {\n  // subgraph body\n}\n\nemit\n| put life = is_good(1, 0)\n| reduce things = get_better(life, :tomorrow:)\n| stamper -mark 'awesome' -force 'light'\n\n\n\n\nArrays and Objects\n\n\nIn both arrays and objects, leave a single space on the inside of \n[ brackets ]\n and \n{ curly: braces }\n. \n\n\nSingle space after (but not before) the separators \n,\n and \n:\n.\n\n\nconst nice_array = [ we, favor, spaces ];\n\nconst nice_object = { here: 'also', spaces: 'rule' };\n\n\n\n\nFor large arrays and objects, leave the brackets or curlies on their own lines. In Juttle this tends to happen with arrays of data points, and options objects for sinks.\n\n\nconst input_points = [\n  { time: '2015-12-01T10:51:04', name: 'style guide', status: 'progress' },\n  { time: '2015-12-01T16:45:28', name: 'style guide', status: 'done' }\n];\n\nemit -points input_points\n| view text -o {\n    style: 'csv',\n    limit: 10,\n    title: 'TPS Report'\n  }\n\n\n\n\nCapitalization\n\n\nConstants should be capitalized.\n\n\nJuttle's built-in modules use camelCase for naming functions, such as \nDate.startOf()\n.\n\n\nThe user-defined variable, function, reducer and subgraph names commonly use underscore_case in provided Juttle examples.\n\n\n\n\nvariable_names_like_this: \nvar max_delta\n\n\nfunction_names_like_this: \nfunction is_good(a)\n\n\nreducers_as_well: \nreducer get_better(x)\n\n\nsubgraphs_too: \nsub does_it_all()\n\n\nCONSTANT_VALUES_LIKE_THIS: \nconst CUSTOM_COLOR\n\n\n\n\nWhen working with an existing body of Juttle code, adhere to the naming style chosen by the authors, be that camelCase, underscore_case or otherwise.\n\n\nConditionals and Equality\n\n\nIn Juttle, if/else conditionals appear only inside functions and reducers; in flowgraph context, filter expressions are used to fork a flowgraph based on true/false evaluation.\n\n\nIn single-line conditionals, place the condition on the first line, without spaces inside the \n(parentheses)\n but with a space before opening \n{\n.\n\n\nPlace the statements on the next line (not on the same line as the condition), and align the closing brace \n}\n to the opening statement:\n\n\nfunction is_null(value) {\n  if (value == null) {\n    return true;\n  } else {\n    return false;\n  }\n}\n\n\n\n\nIn multi-line conditionals, keep the and/or/not operators on the line with the preceding condition, and drop the \n{\n onto its own line aligned with \nif\n:\n\n\nfunction is_expected(one, two, three) {\n  if ((one == expected_one) \n\n      (two \n= min_two) \n\n      (three != 'bad_third_value'))\n  {\n    return true;\n  } else {\n    return false;\n  }\n}\n\n\n\n\nIn flowgraph context, when the \nfilter\n processor is used to conditionally branch the flowgraph, the equality test can be done with either \n==\n or \n=\n operator. The idiomatic Juttle uses \n=\n in that case:\n\n\nemit\n| (\n  filter time = null\n  | put qed = \nimpossible\n;\n\n  filter time != null\n  | put qed = \nemit always sets time\n;\n)\n\n\n\n\nWhen using ternary or null coalescing operators, leave a single empty space on both sides of the operator:\n\n\nemit\n| put foo = (foo == null) ? 42 : 53\n| put bar = bar ?? 53\n\n\n\n\nString Literals\n\n\nUse single quotes for all string literals: \n'good'\n not \n\"bad\"\n.\n\n\nNote that when running Juttle programs via CLI \n-e\n option, this allows to enclose the program body in double quotes, avoiding any need for escapes:\n\n\n$ juttle -e \nemit | put quotes = 'single', problems = 'none'\n\n\n\n\n\nMultiline string literals continued with the \n\\\n character are not supported in Juttle, use string concatenation with \n+\n instead:\n\n\nemit\n| put the_tyger = 'Tyger Tyger, burning bright, '\n    + 'In the forests of the night; '\n    + 'What immortal hand or eye, '\n    + 'Could frame thy fearful symmetry?'\n\n\n\n\nNote that multiline strings containing newline \n\\n\n will get rendered with or without true newlines depending on the sink.\n\n\nFlowgraphs\n\n\nFor pipelines, generally put each proc at a separate line and format them like this:\n\n\nsource \nsome gist\n\n| keep \nthing\n\n| view table\n\n\n\n\nException: if a short program contains only one pipe, it is acceptable to keep it on a single line:\n\n\nemit | put name = \nexample\n\n\n\n\n\nAlways put a single whitespace after the pipe symbol, like this: \nemit | put ...\n not like this: \nemit |put ...\n\n\nWhen the pipeline contains a parallel graph, format it like this, indenting each branch:\n\n\nsource -option -option\n| put this = that, biff = boff\n| keep \non\n, \nkeeping\n, \non\n\n| (\n  flowgraph;\n  flowgraph;\n) | join -first -outer -inner -last\n| view donuthole\n\n\n\n\nNote that each parallel branch ends with \n;\n (including the last one) and the final join (or merge) goes at the same line as the closing \n)\n.\n\n\nIf one or more parallel branches are multi-line, put a blank line between the branches:\n\n\nread -from :a long time ago: -to :sometime yesterday:\n| (\n  reduce\n  | filter\n  | view treechart;\n\n  batch :fortnight:\n  | sort ear_shape\n  | write webmd;\n)\n\n\n\n\nThe trailing semicolon at the end of a single-branch flowgraph is optional; Juttle treats semicolons as separators, not terminators, and this coding style generally omits the trailing semi. It is idiomatic to write \nemit | view table\n but it is also acceptable to write \nemit | view table;\n if desired.\n\n\nProcessors\n\n\nIn general, each proc (short for \"processor\") invocation should be on its own line. For short proc invocations, include all the parameters on the same line. In case of multiple assignments (e.g. in \nput\n and \nreduce\n procs), if they are short enough, keep them all on the same line:\n\n\nemit\n| put when = \ni fall down\n, you = \nput me\n, back = \ntogether\n\n\n\n\n\nIf multiple assignments are too long for one line, invoke the proc several times rather than splitting up assignments into several lines:\n\n\nemit\n| put when = \ni wake up\n\n| put well = \ni know i'm gonna be, i'm gonna be\n\n| put the_man = \nwho wakes up next to you\n\n\n\n\n\nSources\n\n\nFor source procs which contain both options and filter expressions (e.g. \nreadx\n), put all the options on the same line, and each filter expression on a separate line below the proc invocation, indented by one step relative to the proc name:\n\n\nread -from :a long time ago: -to :sometime yesterday:\n  name ~ \n*ears\n\n  AND normal = false\n  AND symptoms in ['leaking', 'deaf', 'mossy']\n\n\n\n\nSinks\n\n\nFor long invocations of sink procs, put the parameters on separate lines below the proc invocation indented by one step relative to the proc name:\n\n\n...\n| view treechart\n  -boundaryField \near_shape\n\n  -title \nRelative distribution of ear shape\n\n\n\n\n\nInputs\n\n\nFormat long input definitions similarly to long proc invocations:\n\n\ninput a: combobox\n  -items [ 'a', 'b', 'c' ]\n  -multi true;\n\n\n\n\nName the input on the line where it is declared, indent each option, and end with a semicolon \n;\n on the same line as the last option. Array values follow array formatting.\n\n\nFull Program Example\n\n\nThis example is intentionally long and complex, to showcase style of each element; it is styled right, while not necessarily well designed.\n\n\ninput resolution: text\n  -description 'Resolution such as: approved, denied, on ice'\n  -default 'denied';\n\nconst input_points = [\n  { time: '2015-12-01T10:51:04', name: 'style guide', status: 'progress' },\n  { time: '2015-12-01T16:45:28', name: 'style guide', status: 'done' }\n];\n\nconst THE_FORCE = 'heavy';\n\nfunction level_it(value) {\n  if (value \n= 50) {\n    return 42;\n  } else {\n    return Math.pow(value, Math.round(value/10)); \n  }\n}\n\nfunction rand10() {\n  return Math.floor(Math.random()*10);\n}\n\nsub stamper(mark, force = 'light') {\n  put stamp = mark, power = force\n}\n\nemit -points input_points\n| put position = count(), dice = rand10()\n| put note = 'Reviewed on: ' + Date.format(:now:, 'MM/DD/YYYY')\n    + ' by He-Who-Must-Not-Be-Named aka You-Know-Who'\n| stamper -mark resolution -force THE_FORCE\n| (\n  filter dice \n 5\n  | put level = dice * 2;\n\n  filter dice \n= 5\n  | put level = level_it(dice)\n) \n| view text -o {\n    style: 'csv',\n    limit: 10,\n    title: 'TPS Report'\n  };\n\nemit | put message = \nDone!", 
            "title": "Style Guide"
        }, 
        {
            "location": "/reference/style_guide/#juttle-style-guide", 
            "text": "Juttle programs in this repository follow conventions of this style guide, as we believe it improves readability.  This guide uses pseudocode in some places. For working Juttle code examples, look at  docs/examples  and  simple examples  on GitHub, and the full program example at the end of this document.", 
            "title": "Juttle Style Guide"
        }, 
        {
            "location": "/reference/style_guide/#comments", 
            "text": "Both  //  and  /* .. */  style comments are supported. Prefix lines of a long comment block with  * . If commenting  //  to the right of multiple lines of code, strive to align the comments:  ...\nemit -points data\n| put *name = value   // transpose data points\n| join                // combine data points with matching timestamps\n...", 
            "title": "Comments"
        }, 
        {
            "location": "/reference/style_guide/#whitespace", 
            "text": "The indentation step is 2 spaces. Don\u2019t use tabs.  Indent each continuation of a given step in a Juttle pipeline (see Flowgraphs).  Avoid trailing whitespace or extra whitespace on blank lines.  No fixed right-hand limit on line length, but be reasonable.  One space before and after  =  in assignments:   emit | put good = '1 space', just = 'like that'", 
            "title": "Whitespace"
        }, 
        {
            "location": "/reference/style_guide/#functions-reducers-subgraphs", 
            "text": "No space between a function, subgraph or reducer name and opening  (  in both declarations and calls. Note that sub invocation has a different parameter syntax than functions or reducers, but follows the same style guidelines.  One space between closing  )  and opening  {  (do not drop the opening  {  down to the next line in declarations).  Single space separation between parameters in function declarations and calls.  function is_good(a, b) {\n  // function body\n}\n\nreducer get_better(x, when = :now:) {\n  // reducer body\n}\n\nsub stamper(mark, force) {\n  // subgraph body\n}\n\nemit\n| put life = is_good(1, 0)\n| reduce things = get_better(life, :tomorrow:)\n| stamper -mark 'awesome' -force 'light'", 
            "title": "Functions, Reducers, Subgraphs"
        }, 
        {
            "location": "/reference/style_guide/#arrays-and-objects", 
            "text": "In both arrays and objects, leave a single space on the inside of  [ brackets ]  and  { curly: braces } .   Single space after (but not before) the separators  ,  and  : .  const nice_array = [ we, favor, spaces ];\n\nconst nice_object = { here: 'also', spaces: 'rule' };  For large arrays and objects, leave the brackets or curlies on their own lines. In Juttle this tends to happen with arrays of data points, and options objects for sinks.  const input_points = [\n  { time: '2015-12-01T10:51:04', name: 'style guide', status: 'progress' },\n  { time: '2015-12-01T16:45:28', name: 'style guide', status: 'done' }\n];\n\nemit -points input_points\n| view text -o {\n    style: 'csv',\n    limit: 10,\n    title: 'TPS Report'\n  }", 
            "title": "Arrays and Objects"
        }, 
        {
            "location": "/reference/style_guide/#capitalization", 
            "text": "Constants should be capitalized.  Juttle's built-in modules use camelCase for naming functions, such as  Date.startOf() .  The user-defined variable, function, reducer and subgraph names commonly use underscore_case in provided Juttle examples.   variable_names_like_this:  var max_delta  function_names_like_this:  function is_good(a)  reducers_as_well:  reducer get_better(x)  subgraphs_too:  sub does_it_all()  CONSTANT_VALUES_LIKE_THIS:  const CUSTOM_COLOR   When working with an existing body of Juttle code, adhere to the naming style chosen by the authors, be that camelCase, underscore_case or otherwise.", 
            "title": "Capitalization"
        }, 
        {
            "location": "/reference/style_guide/#conditionals-and-equality", 
            "text": "In Juttle, if/else conditionals appear only inside functions and reducers; in flowgraph context, filter expressions are used to fork a flowgraph based on true/false evaluation.  In single-line conditionals, place the condition on the first line, without spaces inside the  (parentheses)  but with a space before opening  { .  Place the statements on the next line (not on the same line as the condition), and align the closing brace  }  to the opening statement:  function is_null(value) {\n  if (value == null) {\n    return true;\n  } else {\n    return false;\n  }\n}  In multi-line conditionals, keep the and/or/not operators on the line with the preceding condition, and drop the  {  onto its own line aligned with  if :  function is_expected(one, two, three) {\n  if ((one == expected_one)  \n      (two  = min_two)  \n      (three != 'bad_third_value'))\n  {\n    return true;\n  } else {\n    return false;\n  }\n}  In flowgraph context, when the  filter  processor is used to conditionally branch the flowgraph, the equality test can be done with either  ==  or  =  operator. The idiomatic Juttle uses  =  in that case:  emit\n| (\n  filter time = null\n  | put qed =  impossible ;\n\n  filter time != null\n  | put qed =  emit always sets time ;\n)  When using ternary or null coalescing operators, leave a single empty space on both sides of the operator:  emit\n| put foo = (foo == null) ? 42 : 53\n| put bar = bar ?? 53", 
            "title": "Conditionals and Equality"
        }, 
        {
            "location": "/reference/style_guide/#string-literals", 
            "text": "Use single quotes for all string literals:  'good'  not  \"bad\" .  Note that when running Juttle programs via CLI  -e  option, this allows to enclose the program body in double quotes, avoiding any need for escapes:  $ juttle -e  emit | put quotes = 'single', problems = 'none'   Multiline string literals continued with the  \\  character are not supported in Juttle, use string concatenation with  +  instead:  emit\n| put the_tyger = 'Tyger Tyger, burning bright, '\n    + 'In the forests of the night; '\n    + 'What immortal hand or eye, '\n    + 'Could frame thy fearful symmetry?'  Note that multiline strings containing newline  \\n  will get rendered with or without true newlines depending on the sink.", 
            "title": "String Literals"
        }, 
        {
            "location": "/reference/style_guide/#flowgraphs", 
            "text": "For pipelines, generally put each proc at a separate line and format them like this:  source  some gist \n| keep  thing \n| view table  Exception: if a short program contains only one pipe, it is acceptable to keep it on a single line:  emit | put name =  example   Always put a single whitespace after the pipe symbol, like this:  emit | put ...  not like this:  emit |put ...  When the pipeline contains a parallel graph, format it like this, indenting each branch:  source -option -option\n| put this = that, biff = boff\n| keep  on ,  keeping ,  on \n| (\n  flowgraph;\n  flowgraph;\n) | join -first -outer -inner -last\n| view donuthole  Note that each parallel branch ends with  ;  (including the last one) and the final join (or merge) goes at the same line as the closing  ) .  If one or more parallel branches are multi-line, put a blank line between the branches:  read -from :a long time ago: -to :sometime yesterday:\n| (\n  reduce\n  | filter\n  | view treechart;\n\n  batch :fortnight:\n  | sort ear_shape\n  | write webmd;\n)  The trailing semicolon at the end of a single-branch flowgraph is optional; Juttle treats semicolons as separators, not terminators, and this coding style generally omits the trailing semi. It is idiomatic to write  emit | view table  but it is also acceptable to write  emit | view table;  if desired.", 
            "title": "Flowgraphs"
        }, 
        {
            "location": "/reference/style_guide/#processors", 
            "text": "In general, each proc (short for \"processor\") invocation should be on its own line. For short proc invocations, include all the parameters on the same line. In case of multiple assignments (e.g. in  put  and  reduce  procs), if they are short enough, keep them all on the same line:  emit\n| put when =  i fall down , you =  put me , back =  together   If multiple assignments are too long for one line, invoke the proc several times rather than splitting up assignments into several lines:  emit\n| put when =  i wake up \n| put well =  i know i'm gonna be, i'm gonna be \n| put the_man =  who wakes up next to you", 
            "title": "Processors"
        }, 
        {
            "location": "/reference/style_guide/#sources", 
            "text": "For source procs which contain both options and filter expressions (e.g.  readx ), put all the options on the same line, and each filter expression on a separate line below the proc invocation, indented by one step relative to the proc name:  read -from :a long time ago: -to :sometime yesterday:\n  name ~  *ears \n  AND normal = false\n  AND symptoms in ['leaking', 'deaf', 'mossy']", 
            "title": "Sources"
        }, 
        {
            "location": "/reference/style_guide/#sinks", 
            "text": "For long invocations of sink procs, put the parameters on separate lines below the proc invocation indented by one step relative to the proc name:  ...\n| view treechart\n  -boundaryField  ear_shape \n  -title  Relative distribution of ear shape", 
            "title": "Sinks"
        }, 
        {
            "location": "/reference/style_guide/#inputs", 
            "text": "Format long input definitions similarly to long proc invocations:  input a: combobox\n  -items [ 'a', 'b', 'c' ]\n  -multi true;  Name the input on the line where it is declared, indent each option, and end with a semicolon  ;  on the same line as the last option. Array values follow array formatting.", 
            "title": "Inputs"
        }, 
        {
            "location": "/reference/style_guide/#full-program-example", 
            "text": "This example is intentionally long and complex, to showcase style of each element; it is styled right, while not necessarily well designed.  input resolution: text\n  -description 'Resolution such as: approved, denied, on ice'\n  -default 'denied';\n\nconst input_points = [\n  { time: '2015-12-01T10:51:04', name: 'style guide', status: 'progress' },\n  { time: '2015-12-01T16:45:28', name: 'style guide', status: 'done' }\n];\n\nconst THE_FORCE = 'heavy';\n\nfunction level_it(value) {\n  if (value  = 50) {\n    return 42;\n  } else {\n    return Math.pow(value, Math.round(value/10)); \n  }\n}\n\nfunction rand10() {\n  return Math.floor(Math.random()*10);\n}\n\nsub stamper(mark, force = 'light') {\n  put stamp = mark, power = force\n}\n\nemit -points input_points\n| put position = count(), dice = rand10()\n| put note = 'Reviewed on: ' + Date.format(:now:, 'MM/DD/YYYY')\n    + ' by He-Who-Must-Not-Be-Named aka You-Know-Who'\n| stamper -mark resolution -force THE_FORCE\n| (\n  filter dice   5\n  | put level = dice * 2;\n\n  filter dice  = 5\n  | put level = level_it(dice)\n) \n| view text -o {\n    style: 'csv',\n    limit: 10,\n    title: 'TPS Report'\n  };\n\nemit | put message =  Done!", 
            "title": "Full Program Example"
        }
    ]
}